{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "IWarcgUu77mn",
        "colab_type": "text"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objetivo-e-Instrucciones:\" data-toc-modified-id=\"Objetivo-e-Instrucciones:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objetivo e Instrucciones:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objetivo\" data-toc-modified-id=\"Objetivo-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objetivo</a></span></li><li><span><a href=\"#Fecha-de-Entrega:\" data-toc-modified-id=\"Fecha-de-Entrega:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Fecha de Entrega:</a></span></li><li><span><a href=\"#Detalles-e-instrucciones-de-la-competencia:\" data-toc-modified-id=\"Detalles-e-instrucciones-de-la-competencia:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Detalles e instrucciones de la competencia:</a></span></li><li><span><a href=\"#Reporte\" data-toc-modified-id=\"Reporte-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Reporte</a></span></li><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Baseline</a></span></li></ul></li><li><span><a href=\"#1.-Introducción\" data-toc-modified-id=\"1.-Introducción-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1. Introducción</a></span></li><li><span><a href=\"#2.-Representaciones\" data-toc-modified-id=\"2.-Representaciones-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>2. Representaciones</a></span></li><li><span><a href=\"#3.-Algoritmos\" data-toc-modified-id=\"3.-Algoritmos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3. Algoritmos</a></span></li><li><span><a href=\"#4.-Métricas-de-Evaluación\" data-toc-modified-id=\"4.-Métricas-de-Evaluación-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>4. Métricas de Evaluación</a></span></li><li><span><a href=\"#5.-Experimentos\" data-toc-modified-id=\"5.-Experimentos-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>5. Experimentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importar-librerías-y-utiles\" data-toc-modified-id=\"Importar-librerías-y-utiles-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Importar librerías y utiles</a></span></li><li><span><a href=\"#Definir-métodos-de-evaluación\" data-toc-modified-id=\"Definir-métodos-de-evaluación-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Definir métodos de evaluación</a></span></li><li><span><a href=\"#Datos\" data-toc-modified-id=\"Datos-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Datos</a></span></li><li><span><a href=\"#Analizar-los-datos\" data-toc-modified-id=\"Analizar-los-datos-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Analizar los datos</a></span></li><li><span><a href=\"#Custom-Features\" data-toc-modified-id=\"Custom-Features-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Custom Features</a></span></li><li><span><a href=\"#Definir-la-representación-y-el-clasificador\" data-toc-modified-id=\"Definir-la-representación-y-el-clasificador-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Definir la representación y el clasificador</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-para-algún-dataset\" data-toc-modified-id=\"Ejecutar-el-pipeline-para-algún-dataset-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Ejecutar el pipeline para algún dataset</a></span></li><li><span><a href=\"#Ejecutar-el-sistema-creado-por-cada-train-set\" data-toc-modified-id=\"Ejecutar-el-sistema-creado-por-cada-train-set-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Ejecutar el sistema creado por cada train set</a></span></li><li><span><a href=\"#Predecir-los-target-set-y-crear-la-submission\" data-toc-modified-id=\"Predecir-los-target-set-y-crear-la-submission-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Predecir los target set y crear la submission</a></span></li></ul></li><li><span><a href=\"#6.-Conclusiones\" data-toc-modified-id=\"6.-Conclusiones-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>6. Conclusiones</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "id": "D5AsygLs77mo",
        "colab_type": "text"
      },
      "source": [
        "# Tarea 1 NLP : Competencia de Clasificación de Texto\n",
        "-------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDwqjwrP77mp",
        "colab_type": "text"
      },
      "source": [
        "- **Nombre:** Juan Pablo Cáceres y Javier Urrutia\n",
        "\n",
        "- **Usuario o nombre de equipo en Codalab:** PanteraNegra\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8194wtbW77ms",
        "colab_type": "text"
      },
      "source": [
        "## Objetivo e Instrucciones:\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Esta tarea consiste en participar en una competencia cuyo objetivo es la clasificación de tweets según su intensidad de emoción. Específicamente: \n",
        "\n",
        "Tendrán 4 datasets de tweets de distintas emociones: `anger`, `fear`, `sadness` y `joy`. Para cada uno de estos datasets, deberán crear un clasificador que indique la intensidad de dicha emoción en sus tweets (`low`, `medium`, `high`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltMmBB2t77mt",
        "colab_type": "text"
      },
      "source": [
        "###  Fecha de Entrega: \n",
        "\n",
        "Por ser anunciada una vez termine el paro. Se publicará la fecha en ucursos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T14:34:38.796217Z",
          "start_time": "2020-04-07T14:34:38.782255Z"
        },
        "id": "n9IMd7tZ77mu",
        "colab_type": "text"
      },
      "source": [
        "### Detalles e instrucciones de la competencia:\n",
        "\n",
        "- La competencia consiste en resolver 4 problemas de clasificación distintos, cada uno de tres clases. Por cada problema deberán crear un clasificador distinto. La evaluación de la competencia se realiza en base a 4 métricas: AUC, Kappa y Accuracy. Los mejores puntajes en cada ítem serán los que ganen.\n",
        "\n",
        "- Para comenzar se les entregará en este notebook el baseline y la estructura del reporte. El baseline es el código que realiza creación de features y clasificación básica. Los puntajes de este serán ocupados como base para la competencia: deben superar sus resultados para ser bien evaluados.  \n",
        "\n",
        "- Para participar, deben registrarse en Codalab y luego ingresar a la competencia usando el siguiente [link]( https://competitions.codalab.org/competitions/24121?secret_key=f5eb2d95-b36e-4aad-8fc5-4d9d77f4e4dc). \n",
        "\n",
        "- **Es requisito entregar el reporte con el código y haber participado en la competencia para ser evaluado.**\n",
        "\n",
        "- Pueden hacer grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo (En codalab, ir a settings y después cambiar Team Name). Solo una persona debe administrar la cuenta del grupo.\n",
        "\n",
        "- En total pueden hacer un **máximo de 4 envíos/submissions** (tanto para equipos como para envíos indivuales).\n",
        "\n",
        "- Hagan varios experimentos haciendo cross-validation o evaluación sobre una sub-partición antes de enviar sus predicciones a Codalab. Asegúrense que la distribución de las clases sea balanceada en las particiones de training y testing. Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les será evaluado incorrectamente.\n",
        "\n",
        "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n",
        "\n",
        "- No se limiten a los contenidos vistos ni a scikit ni a este baseline. ¡Usen todo su conocimiento e ingenio en mejorar sus sistemas! \n",
        "\n",
        "- Todas las dudas escríbanlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xNHxDiE77mv",
        "colab_type": "text"
      },
      "source": [
        "### Reporte\n",
        "\n",
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**Introducción**: Presentar brevemente el problema a resolver, los métodos y representaciones utilizadas en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
        "2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores. Si bien, con Bag of Words (baseline) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano. Mas abajo encontrarán una lista útil de estos que les podrá ser de utilidad. (1.5 puntos)\n",
        "3.\t**Algoritmos**: Describir brevemente los algoritmos de clasificación usados. (0.5 puntos)\n",
        "4.\t**Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación indicando que miden y su interpretación. (0.5 puntos)\n",
        "5.\t**Experimentos**: Reportar todos sus experimentos. Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones. Estos experimentos los hacen sobre la sub-partición de evaluación que deben crear (o pueden usar cross-validation). Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (2 puntos)\n",
        "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1 punto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:18:43.301002Z",
          "start_time": "2019-08-21T19:18:43.298037Z"
        },
        "id": "wZGZtiKI77mz",
        "colab_type": "text"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Por último, el baseline contiene un código básico que:\n",
        "\n",
        "- Obtiene los dataset.\n",
        "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
        "- Crea un Pipeline que: \n",
        "    - Crea features personalizadas.\n",
        "    - Transforma los dataset a bag of words (BoW).  \n",
        "    - Entrena un clasificador usando cada train set.\n",
        "- Clasifica y evalua el sistema creado usando el test set.\n",
        "- Clasifica el target set.\n",
        "- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook. \n",
        "\n",
        "\n",
        "Algunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendrá mas sentido cuando vean el código)\n",
        "\n",
        "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del el enunciado. Investigar también el vectorizador tf-idf.\n",
        "\n",
        "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`).\n",
        "\n",
        "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aquí les adjuntamos algunos ejemplos:\n",
        "    -\tWord n-grams.\n",
        "    -\tCharacter n-grams. \n",
        "    -\tPart-of-speech tags.\n",
        "    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n",
        "        - Count the number of positive and negative words within a sentence.\n",
        "        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n",
        "        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n",
        "        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n",
        "    -\tThe number of elongated words (words with one character repeated more than two times).\n",
        "    -\tThe number of words with all characters in uppercase.\n",
        "    -\tThe presence and the number of positive or negative emoticons.\n",
        "    -\tThe number of individual negations.\n",
        "    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n",
        "    -\tWord Embeddings: Here are some good ideas on how to use them.\n",
        "    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n",
        "\n",
        "- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
        "\n",
        "- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:25:19.677190Z",
          "start_time": "2020-04-07T15:25:19.671206Z"
        },
        "id": "rfZ4uxrD77m1",
        "colab_type": "text"
      },
      "source": [
        "(Pueden eliminar cualquier celda con instrucciones...)\n",
        "\n",
        "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpqdglwn77m2",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "id": "UGSXA_OV77m3",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3IsGt_77m4",
        "colab_type": "text"
      },
      "source": [
        "En este trabajo se buscará diseñar modelos que permitan determinar la intensidad de la emoción que presentan distintos tweets. Para ello se utilizarán técnicas de NLP basadas en Machine Learning, extrayendo características de los textos y probando distintos tipos de clasificadores.\n",
        "\n",
        "Los datos que se utilizararán son cuatro dataset que corresponden a tweets de cuatro emociones distintas (Enojo, miedo, alegría y tristeza) que están clasificados en tres niveles de intensidad (bajo, medio y alto).\n",
        "\n",
        "Para resolver este problema se utilizararáán distintos tipos de representaciones como BoW, TF-IDF y Words Embeddings preentrenados. Para realizar la clasificación se probó con Naive Bayes multinomial, SVM clasification, MLP y regresión logística.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "id": "Ml7qlHyO77m4",
        "colab_type": "text"
      },
      "source": [
        "## 2. Representaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:17.719268Z",
          "start_time": "2020-03-31T13:47:17.709207Z"
        },
        "id": "WgDH_IsD77m6",
        "colab_type": "text"
      },
      "source": [
        "Se probaron distintos tipos de representaciones para los textos. \n",
        "\n",
        "En primer lugar están las que representan los tokens contenidos en un texto que son Bag of Words(BoW) y Term-frequency-inverse document frequency(TF-IDF). En la primera se crea un arreglo con la cuenta de los tokens presentes en el texto mientras que la segunda muestra la relevancia de cada token presente en el texto que está dada por su frecuencia en éste descontado por su frecuencia en todo el corpus de entrenamiento.\n",
        "\n",
        "Tambien se extrae como característica la frecuencia de los carácteres '?','!' y '#' que pueden definir en parte la intensidad de lo que se está diciendo. Además de una representación de la frecuencia de los emojis que presentan los tweets.\n",
        "\n",
        "Otra forma de representacion utilizada es el word2vec, que lleva cada palabra a un espacio vectorial que se crea al entrenar un modelo en un corpus grande para predecir palabras contexto. Así este espacio representa de cierta forma el significado de la palabra y al utilizar una funcion de agregacion, como la suma, se puede obtener un vector que represente un documento completo. Sin embargo, entrenar este tipo de modelos requiere un corpus bastante grande, por lo cual se decidio a utilizar los pesos entrenados GloVe Twitter 200d. \n",
        "\n",
        "Con el fin que las representaciones obtener representaciones densas más informativas, se evaluo en varias representaciones la utilización de análisis de componentes principales (PCA), lo cual realiza una selección de las dimensiones que presentan mayor información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bpbt95777m7",
        "colab_type": "text"
      },
      "source": [
        "## 3. Algoritmos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXXldcuO77m8",
        "colab_type": "text"
      },
      "source": [
        "Se probaron distintos tipos de clasificador para elegir el que tuviera mejor rendimiento.\n",
        "\n",
        "El primero es un Naive Bayes multinomial que se utiliza directamente sobre la representación BoW y calcula la probabilidad de que un documento pertenezca a cada clase.\n",
        "\n",
        "El segundo es un *support vector clasification*(SVC) que utiliza SVM para clasificar cada clase en un esquema uno versus uno. Este se probó von un kernel lineal y *rbf*. En la implementación de scikit se soporta además el entrenamiento sobre un set desbalanceado al ajustar automáticamente un peso para cada clase.\n",
        "\n",
        "El tercer clasificador utilizado es una red neuronal *Multi Layer Perceptron*(MLP). Para elegir la cantintad de neuronas y la función de activación se realizó una búsqueda con la función *gridsearch* de scikit learn, sobre uno de los conjuntos.\n",
        "\n",
        "El cuarto clasificador es un aregresión logistica que itiliza un esquema uno versus el resto para el caso multiclase. En este tambíen se realizo un ajuste de peso para cada clase por el desbalance de los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "id": "0JT7sm5q77m9",
        "colab_type": "text"
      },
      "source": [
        "## 4. Métricas de Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7m_wsq77m-",
        "colab_type": "text"
      },
      "source": [
        "Se utilizan las siguiente métricas para evaluar los modelos entrenados\n",
        "•\tAUC: Corresponde al área bajo la curva ROC(Receiver Operating Characteristics) que se obtiene al graficar el ratio de verdaderos positivos. Como este caso se trabaja con 3 clases, se calcula como el promedio ponderado del AUC de cada clase versus el resto.\n",
        "•\tKappa: Corresponde a un estadístico que compara la exactitud del clasificador (Accuracy) con la exactitud esperada por un clasificador aleatorio y dependerá del balance de las clases clasificadas.\n",
        "•\tAccuracy: Corresponde a la exactitud del clasificador y es la proporción entre las clasificaciones correctas(verdaderos positivos) y el total de ejemplos clasificados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNvP-GQ177m-",
        "colab_type": "text"
      },
      "source": [
        "## 5. Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA-_T-PW77m_",
        "colab_type": "text"
      },
      "source": [
        "A continuación se muestran los experimentos realizados.\n",
        "\n",
        "En primer lugar se probaron la representaciones BoW junto con *CharsCountTransformer* con los distintos clasificadores propuestos para elegir el más adecuado. Luego se implemento la representación word embedding con GloVe Twitter 200d junto con *CharsCountTransformer* con distintos tipos tipos de clasificadores. PCA se utilizo unicamente en los experimentos que provocaba mejoras en las metricas definidas en la sección anterior. Con respecto a la representación *EmojiCountTranformer* esta no presento mejoras en ninguno de los experimentos.\n",
        "\n",
        "Para evaluar los modelos se sigue una estrategia de validación cruzada dividiendo el set de entrenamiento con la estrategia *Stratified K-Fold* en 5 sets. Se utiliza ésta pues mantiene el balance de datos que existe originalmente en el set en todas las particiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:31:40.023344Z",
          "start_time": "2020-03-31T13:31:40.003541Z"
        },
        "id": "r_mRdkUY77nB",
        "colab_type": "text"
      },
      "source": [
        "### Importar librerías y utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.587160Z",
          "start_time": "2020-04-07T15:44:19.319386Z"
        },
        "id": "O6UZFd6177nE",
        "colab_type": "code",
        "outputId": "20a603c6-9718-4ee8-96f8-13ecbb57c274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import shutil\n",
        "import string \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "import imblearn.pipeline\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')  \n",
        "from nltk.corpus import stopwords \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.tokenize import TweetTokenizer "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM5wsO-r77nJ",
        "colab_type": "text"
      },
      "source": [
        "### Definir métodos de evaluación\n",
        "\n",
        "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "id": "AZb117Hu77nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set])\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
        "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
        "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(high_test, high_predicted)\n",
        "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(low_test, low_predicted)\n",
        "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
        "             high_test.sum() * auc_high) / (\n",
        "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "\n",
        "def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    print(\n",
        "        confusion_matrix(y_test,\n",
        "                         predicted_labels,\n",
        "                         labels=['low', 'medium', 'high']))\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(\n",
        "        classification_report(y_test,\n",
        "                              predicted_labels,\n",
        "                              labels=['low', 'medium', 'high']))\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])\n",
        "\n",
        "def score_fun(predicted_probabilities, y_test, labels):\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    return np.array([auc, kappa, accuracy])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZXkcFNC77nP",
        "colab_type": "text"
      },
      "source": [
        "### Datos\n",
        "\n",
        "Obtener los datasets desde el github del curso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "id": "-dUTmCKT77nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datasets de entrenamiento.\n",
        "train = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
        "}\n",
        "# Datasets que deberán predecir para la competencia.\n",
        "target = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "id": "yJlYCM2777nU",
        "colab_type": "code",
        "outputId": "162b68cb-8e25-44eb-8525-aacb8d09eafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias:\n",
        "train['anger'].sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>10282</td>\n",
              "      <td>and apparently he's supposed to have a Scottis...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>10780</td>\n",
              "      <td>@leener00 @libbyfloyd1 @G_Eazy my snap is andr...</td>\n",
              "      <td>anger</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>10810</td>\n",
              "      <td>and naoto nod she also like tha bands. but she...</td>\n",
              "      <td>anger</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>10119</td>\n",
              "      <td>@XemitSellsMagic add tracking but resent them</td>\n",
              "      <td>anger</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>10387</td>\n",
              "      <td>#EpiPen: when public outrage occurs, expand #P...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... sentiment_intensity\n",
              "282  10282  ...              medium\n",
              "780  10780  ...                 low\n",
              "810  10810  ...                 low\n",
              "119  10119  ...                high\n",
              "387  10387  ...              medium\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d89bCTPI77nZ",
        "colab_type": "text"
      },
      "source": [
        "### Analizar los datos \n",
        "\n",
        "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento. Noten que las clases están desbalanceadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.117633Z",
          "start_time": "2020-04-07T15:44:21.090703Z"
        },
        "id": "yCk9zDXm77na",
        "colab_type": "code",
        "outputId": "ca124db1-025a-49c7-93f7-682e435b9787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "def get_group_dist(group_name, train):\n",
        "    print(group_name, \"\\n\",\n",
        "          train[group_name].groupby('sentiment_intensity').count(),\n",
        "          '\\n---------------------------------------\\n')\n",
        "for dataset_name in train:\n",
        "    get_group_dist(dataset_name, train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 163    163    163\n",
            "low                  161    161    161\n",
            "medium               617    617    617 \n",
            "---------------------------------------\n",
            "\n",
            "fear \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 270    270    270\n",
            "low                  288    288    288\n",
            "medium               699    699    699 \n",
            "---------------------------------------\n",
            "\n",
            "joy \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 195    195    195\n",
            "low                  219    219    219\n",
            "medium               488    488    488 \n",
            "---------------------------------------\n",
            "\n",
            "sadness \n",
            "                       id  tweet  class\n",
            "sentiment_intensity                   \n",
            "high                 197    197    197\n",
            "low                  210    210    210\n",
            "medium               453    453    453 \n",
            "---------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKzHqMcm77ng",
        "colab_type": "text"
      },
      "source": [
        "### Custom Features \n",
        "\n",
        "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
        "\n",
        "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta carácteres relevantes ('!', '?', '#') en los tweets.\n",
        "2. Definios una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
        "3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
        "\n",
        "Esto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.128600Z",
          "start_time": "2020-04-07T15:44:21.119624Z"
        },
        "id": "B9qYv4ji77nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count('#')\n",
        "        num_exclamations = tweet.count('!')\n",
        "        num_interrogations = tweet.count('?')\n",
        "        return [num_hashtags, num_exclamations, num_interrogations]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.145564Z",
          "start_time": "2020-04-07T15:44:21.131593Z"
        },
        "id": "YqPfhfpn77nl",
        "colab_type": "code",
        "outputId": "3473a1c9-3dda-426f-c275-881b566fc72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['anger'].sample(5).tweet\n",
        "pd.DataFrame(zip(sample, CharsCountTransformer().transform(sample)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That way ur so angry you can literally feel ur...</td>\n",
              "      <td>[0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@SapientSoldier timid but determined version o...</td>\n",
              "      <td>[0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Controlling #anger is a sign of those #righteo...</td>\n",
              "      <td>[6, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nahhhhhh @konanplaydirty snap story has got me...</td>\n",
              "      <td>[0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Forever angry that gh ruined Molly and morgan'...</td>\n",
              "      <td>[0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0          1\n",
              "0  That way ur so angry you can literally feel ur...  [0, 0, 0]\n",
              "1  @SapientSoldier timid but determined version o...  [0, 0, 0]\n",
              "2  Controlling #anger is a sign of those #righteo...  [6, 0, 0]\n",
              "3  Nahhhhhh @konanplaydirty snap story has got me...  [0, 0, 0]\n",
              "4  Forever angry that gh ruined Molly and morgan'...  [0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbTmZ_7zdxqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmojiCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "      emoticons = set(range(int('1f600',16), int('1f650', 16)))\n",
        "      self.emoji = [c for c in emoticons]\n",
        "      self.hot_emoji = [0 for emo in emoticons]\n",
        "\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        for w in tweet:\n",
        "          for char in w:\n",
        "            if ord(char) in self.emoji:\n",
        "              i = self.emoji.index(ord(char))\n",
        "              self.hot_emoji[i]+=1\n",
        "        #print(self.hot_emoji)\n",
        "        return self.hot_emoji\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSF51r58dx0i",
        "colab_type": "code",
        "outputId": "2502a0a9-ea50-4783-8828-793a9921c088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['joy'].sample(5).tweet\n",
        "pd.DataFrame(zip(sample, EmojiCountTransformer().transform(sample)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@MerenthaProphet - the Hunter in this way, con...</td>\n",
              "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@Nikhilv95 @LydiajaneF I wish, I really, truly...</td>\n",
              "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#LMFAO @MSNBC 's #racepimp Tamron Hall used th...</td>\n",
              "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@BertGatti not had much to cheer about you see...</td>\n",
              "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@elena_yip My random guesses for you! A potent...</td>\n",
              "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                  1\n",
              "0  @MerenthaProphet - the Hunter in this way, con...  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  @Nikhilv95 @LydiajaneF I wish, I really, truly...  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2  #LMFAO @MSNBC 's #racepimp Tamron Hall used th...  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3  @BertGatti not had much to cheer about you see...  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4  @elena_yip My random guesses for you! A potent...  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkb4LkFVAV8P",
        "colab_type": "text"
      },
      "source": [
        "### Doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD8gTtU7_-7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import string \n",
        "string.punctuation\n",
        "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
        "\n",
        "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando algún modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, aggregation_func):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.model = model \n",
        "        self.tokenizer = TweetTokenizer()\n",
        "        self.table = str.maketrans('', '', string.punctuation)\n",
        "        # indicamos la función de agregación (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuación, lleva las letras a minúscula(opcional) y \n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"  \n",
        "        doc = doc.lower()\n",
        "        doc = self.tokenizer.tokenize(doc)\n",
        "        doc = [token for token in doc if not token.startswith('@')] # eliminar usuarios\n",
        "        doc = [w.translate(self.table) for w in doc]\n",
        "        doc = [w for w in doc if w != ''] # elimina words ''\n",
        "        doc = [w for w in doc if w.isalpha()]\n",
        "        doc = [w for w in doc if w in self.model.vocab]\n",
        "        return doc\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        doc_embeddings = []\n",
        "        \n",
        "        for doc in X:\n",
        "            # tokenizamos el documento. Se llevan todos los tokens a minúscula. \n",
        "            # ojo con esto, ya que puede que tokens con minúscula y mayúscula tengan\n",
        "            # distintas representaciones\n",
        "            tokens = self.simple_tokenizer(doc, lower = True) \n",
        "            #print(tokens)\n",
        "            selected_wv = []\n",
        "            for token in tokens:\n",
        "                if token in self.model.vocab:\n",
        "                    selected_wv.append(self.model[token])\n",
        "                else:\n",
        "                  #pass\n",
        "                  print(token)\n",
        "                    \n",
        "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo añadimos.\n",
        "            if len(selected_wv) > 0:\n",
        "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "                doc_embeddings.append(doc_embedding)\n",
        "            # si no, añadimos un vector de ceros que represente a ese documento.\n",
        "            else: \n",
        "                print('No pude encontrar ningún embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
        "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
        "        #print(np.array(doc_embeddings).shape)\n",
        "        return np.array(doc_embeddings)   \n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaTlLz9YJY_E",
        "colab_type": "code",
        "outputId": "098059d3-6999-409f-9e40-2d42d77d82cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-twitter-200\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 97.3% 737.9/758.5MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_CJGunBvobu",
        "colab_type": "code",
        "outputId": "c876bc82-c102-47c6-b01d-e0a45e92d65f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('size vocab w2v:',len(word_vectors.vocab))\n",
        "word_vectors.vector_size\n",
        "#word_vectors.most_similar(\"cant\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size vocab w2v: 1193514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXMnYwhhOCdd",
        "colab_type": "code",
        "outputId": "26311bac-a7fd-4793-9f40-06bf1f067d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "import sys\n",
        "sample = train['anger'].sample(5).tweet\n",
        "pd.DataFrame(zip(sample, Doc2VecTransformer(word_vectors, np.sum).transform(sample)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#Taurus will react angrily when she can't take...</td>\n",
              "      <td>[0.43582892, 4.9305, -0.44725996, 2.711693, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@notquitefrodo @dominictarason I remember TAs ...</td>\n",
              "      <td>[4.7843857, 2.916362, 3.7790437, 0.924433, 1.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Petition for non-Maghrebi PoC to stop buying '...</td>\n",
              "      <td>[-0.55734116, 2.375447, 1.0665569, 4.1605077, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@evanareteos :)) im now writing abt the changi...</td>\n",
              "      <td>[0.6360725, 0.78656703, 0.073327, 0.8076659, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@SusanHensThe shameful display I watched today...</td>\n",
              "      <td>[5.535181, 7.109447, -0.40932912, 1.170087, -3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                  1\n",
              "0  #Taurus will react angrily when she can't take...  [0.43582892, 4.9305, -0.44725996, 2.711693, -0...\n",
              "1  @notquitefrodo @dominictarason I remember TAs ...  [4.7843857, 2.916362, 3.7790437, 0.924433, 1.0...\n",
              "2  Petition for non-Maghrebi PoC to stop buying '...  [-0.55734116, 2.375447, 1.0665569, 4.1605077, ...\n",
              "3  @evanareteos :)) im now writing abt the changi...  [0.6360725, 0.78656703, 0.073327, 0.8076659, -...\n",
              "4  @SusanHensThe shameful display I watched today...  [5.535181, 7.109447, -0.40932912, 1.170087, -3..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUkV--qrIExz",
        "colab_type": "text"
      },
      "source": [
        "### Negation Tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhIwt_j9Z6N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetTok = TweetTokenizer(reduce_len=True, strip_handles=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYvi6-EErzAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def negationTweetTokenizer(doc):\n",
        "  global tweetTok\n",
        "  return mark_negation(tweetTok.tokenize(doc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwiG8fZRsD5n",
        "colab_type": "code",
        "outputId": "ad4f67da-245f-4172-c97b-838f6c8fea76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "for tweet in sample:\n",
        "  print(negationTweetTokenizer(tweet))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#Taurus', 'will', 'react', 'angrily', 'when', 'she', \"can't\", 'take_NEG', 'being_NEG', 'provoked_NEG', 'any_NEG', 'longer_NEG', '.']\n",
            "['I', 'remember', 'TAs', 'in', 'the', 'gamedev', 'program', 'said', 'he', 'had', 'temper', 'tantrums', 'in', 'class', 'all', 'the', 'time', 'lol']\n",
            "['Petition', 'for', 'non-Maghrebi', 'PoC', 'to', 'stop', 'buying', \"'\", 'couscous', \"'\", 'from', 'white', 'grocery', 'stores', 'and', 'boiling', 'it', '.', 'That', 'is', 'not', 'couscous_NEG', ',_NEG', 'that_NEG', 'is_NEG', 'trash_NEG', '.']\n",
            "[':)', ')', 'im', 'now', 'writing', 'abt', 'the', 'changing', 'face', 'of', 'sex', 'industry', 'in', 'tr', ':)', ')', ')', 'now', 'men', 'will', 'talk', 'to', 'me', 'then', '!']\n",
            "['shameful', 'display', 'I', 'watched', 'today', 'has', 'left', 'me', 'reeling', 'with', 'so', 'much', 'anger', 'dt', 'I', 'feel', 'like', 'exploding', ',', 'those', 'clowns', 'should', 'watch', 'it']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfBH2E377np",
        "colab_type": "text"
      },
      "source": [
        "### Definir la representación y el clasificador\n",
        "\n",
        "Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n",
        "\n",
        "El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n",
        "\n",
        "```python\n",
        "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
        "```\n",
        "\n",
        "\n",
        "Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n",
        "\n",
        "```python\n",
        "    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
        "                                        ('chars_count',CharsCountTransformer())])),\n",
        "              ('clf', MultinomialNB())])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVolgxJT77nq",
        "colab_type": "text"
      },
      "source": [
        "Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krcy3rFNKlUM",
        "colab_type": "text"
      },
      "source": [
        "### Features: CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.155528Z",
          "start_time": "2020-04-07T15:44:21.149545Z"
        },
        "id": "L3EIIAxD77nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_experiment_0_pipeline():\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])), ('clf', MultinomialNB())])\n",
        "\n",
        "############################ SVM ##############################################\n",
        "\n",
        "def get_experiment_search_0_pipeline():\n",
        "    parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                  {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),('chars_count', CharsCountTransformer())])), \n",
        "       ('clf', GridSearchCV(SVC(probability=True,class_weight='balanced'), parameters, n_jobs=cores,verbose=2, scoring='roc_auc_ovr_weighted'))              ])\n",
        "\n",
        "    \n",
        "def get_experiment_1_pipeline():\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])), ('clf', SVC(kernel='linear',C=1, probability=True, class_weight='balanced'))])\n",
        "    \n",
        "def get_experiment_2_pipeline():\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])), ('clf', SVC(kernel='rbf',C=1, probability=True, class_weight='balanced'))])\n",
        "    \n",
        "def get_experiment_3_pipeline():\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])), ('clf', SVC(kernel='rbf',C=1000, gamma=0.001, probability=True, class_weight='balanced'))])\n",
        "\n",
        "######################## MLP ###################################################\n",
        "\n",
        "def get_experiment_search_1_pipeline():\n",
        "    # MLP gridsearch\n",
        "    parameters = {'hidden_layer_sizes':[(100,),(200,),(100,50),(50,),(50,50)],\n",
        "                  'activation':('relu', 'logistic','tanh')}\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                     ('clf', GridSearchCV(MLPClassifier(), parameters, n_jobs=cores,verbose=2, scoring='roc_auc_ovr_weighted'))])\n",
        "\n",
        "\n",
        "def get_experiment_4_pipeline():\n",
        "    return imblearn.pipeline.Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                                    ('sampler', RandomOverSampler()),\n",
        "                                    ('clf', MLPClassifier(hidden_layer_sizes=(100,100), activation='logistic'))])\n",
        "\n",
        "def get_experiment_5_pipeline():\n",
        "    return imblearn.pipeline.Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                                    ('sampler', RandomOverSampler()),\n",
        "                                    ('clf', MLPClassifier(hidden_layer_sizes=(200,), activation='tanh', max_iter=300))])\n",
        "    \n",
        "######################## Logistic ###################################################    \n",
        "def get_experiment_6_pipeline():\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('bow', CountVectorizer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                                    ('clf', LogisticRegression(class_weight='balanced', max_iter=300))])\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9tJvuuIK1zN",
        "colab_type": "text"
      },
      "source": [
        "### Features: Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z94vADr7Kzeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## tf-idf ###################################################\n",
        "def get_experiment_7_pipeline():\n",
        "    return Pipeline([('features',TfidfVectorizer(max_features=2000, min_df=5, max_df=0.8, stop_words=stopwords.words('english'))  ),\n",
        "                     ('clf', SVC(kernel='linear',C=1, probability=True, class_weight='balanced'))])\n",
        "    \n",
        "def get_experiment_8_pipeline():\n",
        "    global tweetTok\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tf-idf', TfidfVectorizer(max_features=2000, min_df=5, max_df=0.8, tokenizer=tweetTok.tokenize)),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                     ('clf', SVC(kernel='linear',C=1, probability=True, class_weight='balanced'))])\n",
        "    \n",
        "def get_experiment_9_pipeline():\n",
        "    global tweetTok\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tf-idf', TfidfVectorizer(max_features=2000, min_df=5, max_df=0.8, tokenizer=negationTweetTokenizer)),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                     ('clf', SVC(kernel='linear',C=1, probability=True, class_weight='balanced'))])\n",
        "\n",
        "def get_experiment_10_pipeline():\n",
        "    global tweetTok\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('tf-idf', TfidfVectorizer(max_features=4000, min_df=5, max_df=0.8, tokenizer=tweetTok.tokenize)),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                     ('clf', SVC(kernel='linear',C=1, probability=True, class_weight='balanced'))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIQvYUEkK-4J",
        "colab_type": "text"
      },
      "source": [
        "### Features: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avT5VuY6LFhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "def get_experiment_11_pipeline():\n",
        "    pca = PCA()\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('features1',Doc2VecTransformer(word_vectors, np.mean)), \n",
        "                                    ('chars_count', CharsCountTransformer())])),('pca', pca),\n",
        "                                    ('clf', SVC(kernel='rbf',C=1000,gamma=0.0001, probability=True, class_weight='balanced'))])\n",
        "    \n",
        "def get_experiment_12_pipeline(): # Ganando\n",
        "    pca = PCA()\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('features1',Doc2VecTransformer(word_vectors, np.mean)),('emoji_count',EmojiCountTransformer()),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),('pca', pca),\n",
        "                                    ('clf', LogisticRegression(class_weight='balanced'))])\n",
        "    \n",
        "def get_experiment_13_pipeline():\n",
        "    pca = PCA()\n",
        "    return imblearn.pipeline.Pipeline([('features',\n",
        "                      FeatureUnion([('features1',Doc2VecTransformer(word_vectors, np.mean)),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                                    ('clf', MLPClassifier(hidden_layer_sizes=(50,50),solver='adam', activation='logistic'))])\n",
        "\n",
        "def get_experiment_search_2_pipeline():\n",
        "    # MLP gridsearch\n",
        "    pca = PCA()\n",
        "    parameters = {'hidden_layer_sizes':[(100,),(200,),(100,50),(50,),(50,50),(100,100)],\n",
        "                  'activation':('relu', 'logistic','tanh'),'solver' : ['lbfgs', 'sgd', 'adam'],}\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    return Pipeline([('features',FeatureUnion([('features1',Doc2VecTransformer(word_vectors, np.mean)),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                     ('clf', GridSearchCV(MLPClassifier(), parameters, n_jobs=cores,verbose=2, scoring='roc_auc_ovr_weighted'))])\n",
        "    \n",
        "def get_experiment_search_3_pipeline():\n",
        "    # SVM gridsearch\n",
        "    pca = PCA()\n",
        "    parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                  {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    return Pipeline([('features',\n",
        "                      FeatureUnion([('features',Doc2VecTransformer(word_vectors, np.mean)),\n",
        "                                    ('chars_count', CharsCountTransformer())\n",
        "                                    ])),\n",
        "                     ('clf', GridSearchCV(SVC(probability=True,class_weight='balanced'), parameters, n_jobs=cores,verbose=2, scoring='roc_auc_ovr_weighted'))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOfFBOr477nv",
        "colab_type": "text"
      },
      "source": [
        "### Ejecutar el pipeline para algún dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.167498Z",
          "start_time": "2020-04-07T15:44:21.157540Z"
        },
        "scrolled": true,
        "id": "cPlTtbBN77nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import Counter\n",
        "\n",
        "def run_cv(dataset, pipeline_generator):\n",
        "    # Dividir por stratified k-fold\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "    scores = np.zeros((5,3))\n",
        "    i=0\n",
        "    X = dataset.tweet\n",
        "    y = dataset.sentiment_intensity\n",
        "    count_t = Counter(y)\n",
        "    print(f\"total:{len(dataset)} L:{count_t['low']} M:{count_t['medium']} H:{count_t['high']}\")\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        count_train = Counter(y_train)\n",
        "        count_test = Counter(y_test)\n",
        "        print(\"split_{}  total:{}/{}  L:{}/{}  M:{}/{}  H:{}/{}\"\n",
        "              .format(i, len(train_index), len(test_index),\n",
        "                      count_train['low'],count_test['low'],\n",
        "                      count_train['medium'],count_test['medium'],\n",
        "                      count_train['high'],count_test['high']))\n",
        "\n",
        "        # Generar pipeline\n",
        "        pipeline = pipeline_generator()\n",
        "        \n",
        "        # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        \n",
        "        # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "        predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "    \n",
        "        # Obtenemos el orden de las clases aprendidas.\n",
        "        learned_labels = pipeline.classes_\n",
        "\n",
        "        # Evaluamos:\n",
        "        scores[i] = score_fun(predicted_probabilities, y_test, learned_labels)\n",
        "        i+=1\n",
        "    return scores.mean(axis=0)\n",
        "\n",
        "def run_eval(dataset, dataset_name, pipeline, seed=None):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        test_size=0.33,\n",
        "        random_state=seed)\n",
        "    \n",
        "    class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train)\n",
        "    \n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    # Evaluamos:\n",
        "    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores\n",
        "\n",
        "def run(dataset, pipeline):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas\"\"\"\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
        "    pipeline.fit(dataset.tweet, dataset.sentiment_intensity)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "\n",
        "    return pipeline, learned_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsGdGBdtH978",
        "colab_type": "text"
      },
      "source": [
        "### SVC gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQBiOYh-H48r",
        "colab_type": "code",
        "outputId": "cd2a088e-54a1-492b-f7b6-3c02c103cb23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "svc = get_experiment_search_3_pipeline()\n",
        "svc,_ = run(train['sadness'], svc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   24.5s\n",
            "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fzPYUwjMLKT",
        "colab_type": "code",
        "outputId": "0e596d6f-b63c-42f2-bfc5-b677c44dcde8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(svc[-1].best_score_)\n",
        "print(svc[-1].best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7191189707678747\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISD7ECAQEGEP",
        "colab_type": "text"
      },
      "source": [
        "### MLP gridsearch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3D5bcVUESy4",
        "colab_type": "code",
        "outputId": "a6bfda6e-4f0c-4c2e-9950-ee9ff20e8830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "mlp = get_experiment_search_2_pipeline()\n",
        "mlp,_ = run(train['sadness'], mlp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   59.7s\n",
            "[Parallel(n_jobs=2)]: Done 158 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=2)]: Done 270 out of 270 | elapsed:  6.1min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFTgAonEkdw",
        "colab_type": "code",
        "outputId": "228cc1c8-2a54-4361-b5eb-353e6980e2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(mlp[-1].best_score_)\n",
        "print(mlp[-1].best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7144067310712086\n",
            "{'activation': 'logistic', 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9uufEg77n0",
        "colab_type": "text"
      },
      "source": [
        "### Evaluar el sistema creado por cada train set\n",
        "\n",
        "Este código crea y entrena los 4 sistemas de clasificación y luego los evalua. Para los experimentos, pueden copiar este código variando el pipeline cuantas veces estimen conveniente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac3bih8q2Ilg",
        "colab_type": "code",
        "outputId": "7d68d2d1-13c0-49dd-b1cf-55571fe377d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "for dataset_name, dataset in train.items():\n",
        "    print(dataset_name)\n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    scores = run_cv(dataset, get_experiment_13_pipeline)\n",
        "\n",
        "    # guardamos los scores obtenidos\n",
        "    scores_array.append(scores)\n",
        "    \n",
        "    print(\"{} average scores:\\n\".format(dataset_name))\n",
        "    print(\"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\\n\\n\"\n",
        "          .format(*scores))\n",
        "          \n",
        "# print avg scores\n",
        "print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger\n",
            "total:941 L:161 M:617 H:163\n",
            "split_0  total:752/189  L:128/33  M:494/123  H:130/33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_1  total:753/188  L:129/32  M:494/123  H:130/33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_2  total:753/188  L:129/32  M:494/123  H:130/33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_3  total:753/188  L:129/32  M:493/124  H:131/32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_4  total:753/188  L:129/32  M:493/124  H:131/32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "anger average scores:\n",
            "\n",
            "Average AUC: 0.705\t Average Kappa: 0.281\t Average Accuracy: 0.648\n",
            "\n",
            "\n",
            "fear\n",
            "total:1257 L:288 M:699 H:270\n",
            "split_0  total:1005/252  L:230/58  M:559/140  H:216/54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_1  total:1005/252  L:230/58  M:559/140  H:216/54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_2  total:1006/251  L:231/57  M:559/140  H:216/54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_3  total:1006/251  L:231/57  M:559/140  H:216/54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_4  total:1006/251  L:230/58  M:560/139  H:216/54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fear average scores:\n",
            "\n",
            "Average AUC: 0.736\t Average Kappa: 0.301\t Average Accuracy: 0.608\n",
            "\n",
            "\n",
            "joy\n",
            "total:902 L:219 M:488 H:195\n",
            "split_0  total:721/181  L:175/44  M:390/98  H:156/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_1  total:721/181  L:175/44  M:390/98  H:156/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_2  total:722/180  L:176/43  M:390/98  H:156/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_3  total:722/180  L:175/44  M:391/97  H:156/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_4  total:722/180  L:175/44  M:391/97  H:156/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "joy average scores:\n",
            "\n",
            "Average AUC: 0.734\t Average Kappa: 0.339\t Average Accuracy: 0.607\n",
            "\n",
            "\n",
            "sadness\n",
            "total:860 L:210 M:453 H:197\n",
            "split_0  total:688/172  L:168/42  M:363/90  H:157/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_1  total:688/172  L:168/42  M:363/90  H:157/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_2  total:688/172  L:168/42  M:362/91  H:158/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_3  total:688/172  L:168/42  M:362/91  H:158/39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "split_4  total:688/172  L:168/42  M:362/91  H:158/39\n",
            "sadness average scores:\n",
            "\n",
            "Average AUC: 0.714\t Average Kappa: 0.329\t Average Accuracy: 0.601\n",
            "\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.722\t Average Kappa: 0.313\t Average Accuracy: 0.616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.384119Z",
          "start_time": "2020-04-07T15:44:21.170488Z"
        },
        "scrolled": false,
        "id": "MQDGmh2V77n0",
        "colab_type": "code",
        "outputId": "c4c7a56e-de7e-4abd-84de-ff81e8093169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "scores_array = []\n",
        "\n",
        "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "for dataset_name, dataset in train.items():\n",
        "    \n",
        "    # creamos el pipeline\n",
        "    pipeline = get_experiment_13_pipeline()\n",
        "    \n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    classifier, learned_labels, scores = run_eval(dataset, dataset_name, pipeline, seed=100)\n",
        "\n",
        "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # guardamos las labels aprendidas por el clasificador\n",
        "    learned_labels_array.append(learned_labels)\n",
        "\n",
        "    # guardamos los scores obtenidos\n",
        "    scores_array.append(scores)\n",
        "\n",
        "# print avg scores\n",
        "print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 16  30   1]\n",
            " [ 32 156  23]\n",
            " [  0  33  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.34      0.34        47\n",
            "      medium       0.71      0.74      0.73       211\n",
            "        high       0.45      0.38      0.41        53\n",
            "\n",
            "    accuracy                           0.62       311\n",
            "   macro avg       0.50      0.49      0.49       311\n",
            "weighted avg       0.61      0.62      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.661\tKappa: 0.194\tAccuracy: 0.617\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 41  55   5]\n",
            " [ 32 166  26]\n",
            " [  5  54  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.53      0.41      0.46       101\n",
            "      medium       0.60      0.74      0.67       224\n",
            "        high       0.50      0.34      0.41        90\n",
            "\n",
            "    accuracy                           0.57       415\n",
            "   macro avg       0.54      0.50      0.51       415\n",
            "weighted avg       0.56      0.57      0.56       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.708\tKappa: 0.244\tAccuracy: 0.573\n",
            "------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 35  33   1]\n",
            " [ 21 120  18]\n",
            " [  0  37  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.62      0.51      0.56        69\n",
            "      medium       0.63      0.75      0.69       159\n",
            "        high       0.63      0.47      0.54        70\n",
            "\n",
            "    accuracy                           0.63       298\n",
            "   macro avg       0.63      0.58      0.60       298\n",
            "weighted avg       0.63      0.63      0.62       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.757\tKappa: 0.358\tAccuracy: 0.631\n",
            "------------------------------------------------------\n",
            "\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 33  25   1]\n",
            " [ 31 102  23]\n",
            " [  3  27  39]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.49      0.56      0.52        59\n",
            "      medium       0.66      0.65      0.66       156\n",
            "        high       0.62      0.57      0.59        69\n",
            "\n",
            "    accuracy                           0.61       284\n",
            "   macro avg       0.59      0.59      0.59       284\n",
            "weighted avg       0.62      0.61      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.712\tKappa: 0.354\tAccuracy: 0.613\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.71\t Average Kappa: 0.287\t Average Accuracy: 0.609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6NbkFSYydv1",
        "colab_type": "text"
      },
      "source": [
        "Average AUC: 0.71\t Average Kappa: 0.287\t Average Accuracy: 0.609"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQWfbgrTBdKu",
        "colab_type": "text"
      },
      "source": [
        "### Entrenar el sistema con cada train set completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xst67WiyBj7J",
        "colab_type": "code",
        "outputId": "83a0c504-1350-41d4-9985-1736717cf522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "classifiers = []\n",
        "learned_labels_array = []\n",
        "\n",
        "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "for dataset_name, dataset in train.items():\n",
        "    \n",
        "    # creamos el pipeline\n",
        "    pipeline = get_experiment_13_pipeline()\n",
        "    \n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    classifier, learned_labels = run(dataset, pipeline)\n",
        "\n",
        "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # guardamos las labels aprendidas por el clasificador\n",
        "    learned_labels_array.append(learned_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:37:43.169737Z",
          "start_time": "2019-08-21T19:37:43.166744Z"
        },
        "id": "EIlObXmh77oA",
        "colab_type": "text"
      },
      "source": [
        "### Predecir los target set y crear la submission\n",
        "\n",
        "Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.392097Z",
          "start_time": "2020-04-07T15:44:21.386114Z"
        },
        "id": "XHFAaZ2v77oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
        "    # Agregar ids\n",
        "    predicted['id'] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.588573Z",
          "start_time": "2020-04-07T15:44:21.394094Z"
        },
        "scrolled": true,
        "id": "boOrnQAY77oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if (not os.path.exists('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "# por cada target set:\n",
        "for idx, key in enumerate(target):\n",
        "    # Predecirlo\n",
        "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
        "                                           learned_labels_array[idx])\n",
        "    # Guardar predicciones en archivos separados. \n",
        "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
        "                                 sep='\\t',\n",
        "                                 header=False,\n",
        "                                 index=False)\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odTQ9cHsAJlt",
        "colab_type": "text"
      },
      "source": [
        "### Resultados de experimentos\n",
        "\n",
        "> Con Validación cruzada\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcoqE6Wku3io",
        "colab_type": "text"
      },
      "source": [
        "| Feature                                               | Clasificador                | Average AUC      | anger AUC      | fear AUC      | joy AUC      | sadness AUC      |\n",
        "|-------------------------------------------------------|-----------------------------|------------------|----------------|---------------|--------------|------------------|\n",
        "| bow+CharsCountTransformer                             | MultinomialNB               |            0.654 |          0.624 |         0.645 |        0.703 |            0.642 |\n",
        "| bow+CharsCountTransformer                             | Linear SVC C=1              |            0.683 |          0.687 |         0.707 |        0.677 |            0.662 |\n",
        "| tf-idf(max 2000)+stopwords                            | Linear SVC C=1              |            0.667 |          0.646 |         0.681 |        0.677 |            0.665 |\n",
        "| tf-idf(max 2000)+tweetTokenizer+CharsCountTransformer | Linear SVC C=1              |            0.681 |          0.653 |         0.670 |         0.71 |             0.69 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca    | Linear SVC C=1              |            0.711 |          0.705 |         0.714 |        0.732 |            0.695 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca    | Rbf SVC C=1000 gamma=0.0001 |            0.719 |          0.704 |         0.716 |        0.737 |            0.719 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca    | LogisticRegression          |            0.719 |          0.703 |         0.729 |        0.743 |              0.7 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer        | MLP                         |            0.720 |          0.702 |         0.730 |        0.731 |            0.717 |\n",
        "\n",
        "|\n",
        "\n",
        "| Feature | Clasificador | Average Kappa | anger Kappa | fear Kappa | joy Kappa | sadness Kappa |\n",
        "|-|-|-|-|-|-|-|\n",
        "| bow+CharsCountTransformer | MultinomialNB | 0.177 | 0.165 | 0.151 | 0.258 | 0.134 |\n",
        "| bow+CharsCountTransformer | Linear SVC C=1 | 0.183 | 0.168 | 0.240 | 0.222 | 0.1 |\n",
        "| tf-idf(max 2000)+stopwords | Linear SVC C=1 | 0.161 | 0.105 | 0.172 | 0.197 | 0.171 |\n",
        "| tf-idf(max 2000)+tweetTokenizer+CharsCountTransformer | Linear SVC C=1 | 0.197 | 0.160 | 0.176 | 0.258 | 0.194 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca | Linear SVC C=1 | 0.219 | 0.150 | 0.236 | 0.26 | 0.23 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca | Rbf SVC C=1000 gamma=0.0001 | 0.242 | 0.183 | 0.262 | 0.267 | 0.255 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca | LogisticRegression | 0.302 | 0.301 | 0.303 | 0.338 | 0.268 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer | MLP | 0.315 | 0.291 | 0.285 | 0.336 | 0.348 |\n",
        "\n",
        "|\n",
        "\n",
        "| Feature | Clasificador | Average Accuracy | anger Accuracy | fear Accuracy | joy Accuracy | sadness Accuracy |\n",
        "|-|-|-|-|-|-|-|\n",
        "| bow+CharsCountTransformer | MultinomialNB | 0.600 | 0.666 | 0.572 | 0.606 | 0.554 |\n",
        "| bow+CharsCountTransformer | Linear SVC C=1 | 0.603 | 0.672 | 0.611 | 0.593 | 0.538 |\n",
        "| tf-idf(max 2000)+stopwords | Linear SVC C=1 | 0.589 | 0.664 | 0.572 | 0.567 | 0.55 |\n",
        "| tf-idf(max 2000)+tweetTokenizer+CharsCountTransformer | Linear SVC C=1 | 0.612 | 0.683 | 0.588 | 0.602 | 0.576 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca | Linear SVC C=1 | 0.618 | 0.669 | 0.608 | 0.607 | 0.589 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca | Rbf SVC C=1000 gamma=0.0001 | 0.623 | 0.675 | 0.616 | 0.604 | 0.599 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer+pca | LogisticRegression | 0.558 | 0.577 | 0.551 | 0.574 | 0.532 |\n",
        "| word2vect+tweetTokenizer+CharsCountTransformer | MLP | 0.618 | 0.65 | 0.602 | 0.605 | 0.614 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQCZxmjN77oI",
        "colab_type": "text"
      },
      "source": [
        "## 6. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aycooN4E77oJ",
        "colab_type": "text"
      },
      "source": [
        "Observando los resultados de los modelos entrenados con Cross Validation, se observar que los mejores modelos corresponden al feature: *word2vect+tweetTokenizer+CharCountsTransformer+PCA*, con clasificadores SVM con kernel RBF y MLP. Al evaluar estos modelos cor la función *eval* se obtiene que el mejor clasificador corresponde a MLP, con métricas de Average AUC: 0.71\t Average Kappa: 0.287\t Average Accuracy: 0.609. \n",
        "\n",
        "Que el mejor modelo corresponda a MLP, se debe a que las características con la cual este algortimo discrimina corresponden a vectores densos, los cuales son ideales para MLP. \n",
        "\n",
        "Se pudo observar que en general representaciones con word embeddings, entregan mejor accuracy que representaciones basadas en conteo, las cuales pueden contener varios 0s. Cabe mencionar, que el conteo de carácteres corresponde a una buena representación que permite agregar buena información a las características bases probadas. Por otro lado, el conteo de emojis implementado no fue una técnica para entregar información, debido a que no aporto a mejorar las métricas, ya que era un vector demasiado sparse. \n",
        "\n",
        "Finalmente, se menciona que la actividad pudo ser realizada de forma exitosa, pudiendo experimentar con varios métodos de extracción de caractererísticas y algoritmos de clasificación, permitiendo corroborar la teoría. "
      ]
    }
  ]
}
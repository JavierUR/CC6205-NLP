{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tarea_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jBcL7vz_Uqcu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# Tarea 2 - Named Entity Recognition\n",
        "\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T15:21:15.416464Z",
          "start_time": "2020-06-22T15:21:15.411478Z"
        },
        "colab_type": "text",
        "id": "X3QUWWoWaSE3"
      },
      "source": [
        "- **Nombre:** Juan Pablo Cáceres y Javier Urrutia\n",
        "\n",
        "- **Usuario o nombre de equipo en Codalab:** PanteraNegra\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W_dxGEs3iiau"
      },
      "source": [
        "\n",
        "## Introducción a la tarea\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "\n",
        "El objetivo de esta tarea es resolver una de las tasks mas importantes de Sequence Labelling: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf). \n",
        "\n",
        "En particular, deberán participar, al igual que en la tarea anterior, en una competencia en donde deberán crear distintos modelos que apunten a resolver NER en español. Para esto, les proveeremos un dataset de NER de noticias etiquetadas en español mas este baseline en donde podrán comenzar a trabajar. \n",
        "\n",
        "Esperamos que (por lo menos) utilizen Redes Neuronales Recurrentes (RNN) para resolverla. Nuevamente, hay total libertad para utilizar software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados (como el caso de spacy).\n",
        "\n",
        "\n",
        "**¿Qué es Sequence Labelling?** \n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (frase u oración) sequence labelling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "Esta tarea consiste en localizar y clasificar los tokens de una oración que representen entidades nombradas. Es decir, tokens que simbolicen (1) **personas**, (2) **organizaciones**, (3) **lugares** y (4) **adjetivos, eventos y otras entidades que no entren en las categorías anteriores** deberán ser taggeados como (1) **PER**, (2) **ORG**, (3) **LOC** y (4) **MISC** respectivamente. Adicionalmente, dado que existen entidades representadas en más de un token (como La Serena), se utiliza la notación BIO como prefijo al tag: Beginning, Inside, Outside. Es decir, si encuentro una entidad, el primer token etiquetado será precedido por B, el segundo por I y los n restantes por I. Por otra parte, si el token no representa ninguna entidad nombrada, se representa por O. Un ejemplo de esto es:\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "```\n",
        "Felipe B-PER\n",
        "Bravo I-PER\n",
        "es O\n",
        "el O\n",
        "profesor O\n",
        "de O\n",
        "PLN B-MISC\n",
        "de O\n",
        "la O\n",
        "Universidad B-ORG\n",
        "de I-ORG\n",
        "Chile I-ORG\n",
        ". O\n",
        "```\n",
        "\n",
        "Estos links son los más indicados para comenzar:\n",
        "\n",
        "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWlfabmkaSE7"
      },
      "source": [
        "### Reglas de la tarea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T15:33:49.808401Z",
          "start_time": "2020-06-22T15:33:49.798428Z"
        },
        "colab_type": "text",
        "id": "3w9Dw4CSaSE8"
      },
      "source": [
        "Algunos detalles de la competencia:\n",
        "\n",
        "- Para que su tarea sea evaluada, deben participar en la competencia como también, enviar este notebook con su informe.\n",
        "- Para participar, deben registrarse en la competencia en Codalab en grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo. (¡Y deben reportarlo en su informe!)\n",
        "- Las métricas usadas serán Precisión, Recall y F1.\n",
        "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar correrlo en su computador. en este caso, deberá ser compatible con cuda y deberán instalar todo por su cuenta.\n",
        "- En total pueden hacer un **máximo de 4 envíos**.\n",
        "- Por favor, todas sus dudas haganlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio. Recuerden el ánimo colaborativo del curso!!\n",
        "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5BBxJWQaSE-"
      },
      "source": [
        "**Link a la competencia:  https://competitions.codalab.org/competitions/25302?secret_key=690406c7-b3b0-4092-8694-d08d7991ca94**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9spX-Hkh8YJg"
      },
      "source": [
        "### Modelos\n",
        "\n",
        "La RNN del baseline adjunto a este notebook está programado en [`pytorch`](https://pytorch.org/) y contiene:\n",
        "\n",
        "- La carga los datasets, creación de batches de texto y padding. En resumen, carga los datos y los deja listo para entrenar la red.\n",
        "- La implementación básica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad. \n",
        "- La construcción un output para que lo puedan probar en la tarea en codelab.\n",
        "\n",
        "\n",
        "\n",
        "roponer algunos experimentos a hacer:\n",
        "(cambiar el batch size, dimensiones de las capas, cambiar el tipo de\n",
        "RNN, cambiar el optimizer, usar una CRF loss, usar embeddings\n",
        "pre-entrenados, usar BERT??). Quizás podemos sugerir usar algo como\n",
        "https://github.com/flairNLP/flair\n",
        "\n",
        "Se espera que ustedes experimenten con el baseline utilizando (pero no limitándose) estas sugerencias:\n",
        "\n",
        "*   Probar Early stopping\n",
        "*   Variar la cantidad de parámetros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de parámetros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...).[Guía breve aquí](https://github.com/dccuchile/spanish-word-embeddings), [Embeddings en español aquí](https://github.com/dccuchile/spanish-word-embeddings).\n",
        "*   Variar la cantidad de épocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc...\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Probar teacher forcing.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en español usando [Huggingface](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4HfZqQ-_aSFE"
      },
      "source": [
        "### Reporte\n",
        "\n",
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**Introducción**: Presentar brevemente el problema a resolver, los modelos utilizados en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
        "\n",
        "2.\t**Modelos**: Describir brevemente los modelos, métodos y hiperparámetros utilizados. (1.0 puntos)\n",
        "\n",
        "4.\t**Métricas de evaluación**: Describir las métricas utilizadas en la evaluación indicando que miden y cuál es su interpretación en este problema en particular. (0.5 puntos)\n",
        "\n",
        "5.\t**Experimentos**: Reportar todos sus experimentos y código en esta sección. Comparar los resultados obtenidos utilizando diferentes modelos. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (3.0 puntos)\n",
        "\n",
        "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1.0 punto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGM-hn87aSFF"
      },
      "source": [
        "(Pueden eliminar cualquier celda con instrucciones...)\n",
        "\n",
        "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f7X2FruyaSFG"
      },
      "source": [
        "\n",
        "-----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "\n",
        "El siguiente reporte tratá uno de los problemas más importantes de Sequence Labeling, llamado NER. Sequence Labeling o Tagging Problem corresponde a la tarea de dada una secuencia de entrada de dimensión finita, se tiene que entregar una salida de la misma dimensión de salida, es decir, a cada word de la secuencia de entrada se le asigna una etiqueta. En particular, el problema de Reconocimiento de Entidades Nombradas (NER), corresponde a localizar y clasificar entidades dentro de un texto en categorias predefinidas, como personas, organizaciones, lugares y adjetivos.  \n",
        "\n",
        "Para este tipo de problemas, unos de los modelos que mejores resultados han tenido corresponden a las redes neuronales recursivas (RNN), en particular, las redes recursivas transducer. Teniendo en cuenta lo anterior, los modelos escogidos a experimentar corresponden:  \n",
        "\n",
        "*   LSTM de 2 capas\n",
        "*   BiLSTM de 1 y 2 capas.\n",
        "*   BiGRU de 1 y 2 capas.\n",
        "*   MLP\n",
        "\n",
        "Para mejorar el rendimiento de las RNN, se suele acompañarlas con un modelo de word embeddings, en donde las palabras a clasificar se pasan un espacio en donde exiten metricas explicitas de comparación entre palabras. Los word embeddings utilizados en esta tarea serán:\n",
        "\n",
        "*  Embeddings entrenados desde 0.\n",
        "*  Embeddings pre-entrenados con Fasttext.\n",
        "*  Embeddings contextual de Flair\n",
        "\n",
        "Finalmente, las conclusiones obtenidas en la realización de esta tarea corresponden a, primero la utilización de embeddings pre-entrenados es mejor que la utilización entrenar un embedding desde cero, y más si estos embeddings son contextuales. Segundo, la utilización de LSTM y GRU en este tipo de problema no posee grandes diferencias, a menos que se este trabajando con un modelo muy profundo, donde la LSTM obtiene mejor rendimiento.    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## Modelos \n",
        "\n",
        "\n",
        "En este trabajo se utilizaron principalmente arquitecturas de tipo recurrente para resolver el problema NER, que se plantea como un problema de clasificación. La función de pérdida utilizada es entropía cruzada y se utilizó el optimizador ADAM.  El baseline correspondia a una red que contiene una capa de embeddings, seguida de una red recurrente LSTM y luego una capa *fully connected* para clasificar la palabra según el tag NER que le correspondería.\n",
        "\n",
        "Para obtener los resultados, se realizaron tres entrenamientos por cada modelo probado y se promediaron los scores dado que el dropout introducía variaciones. \n",
        "\n",
        "### Red recurrente y embedding entrenado del corpus\n",
        "\n",
        "Respecto al baseline se evaluaron variaciones, cambiando el número de capas recurrentes, el tamaño del estado oculto o el tamaño del embedding.\n",
        "\n",
        "También se utilizaron redes de tipo GRU, variando hiperparámetros similares. Para estos casos, la entrada a la red es un mini batch de frases donde cada palabra es representada por un índice y se realiza padding con un caracter especíal para pasar el conjunto por la red. \n",
        "\n",
        "### Embeddings pre-entrenados con Flair\n",
        "\n",
        "Además, se experimentó con la libreria Flair de NLP. Si bien esta contiene métodos para entrenar directamente una red para etiquetar palabras, resultaba dificil la evaluación utilizando el resto del código presente, por lo que se optó por sólo utilizar las funciones de embeddings pre-entrenados que posee. Así, para evaluar este sistema se cargan los datos con un método especifico de la librería y se utiliza un wrapper para que la red reciba directamente el embedding de cada palabra.\n",
        "\n",
        "Utilizando embeddings pre-entrenados sobre un corpus más grande, se esperan mejores resultados o una mayor facilidad para llegar a un buen resultado. El primer embedding utilizado es fasttext entrenado sobre wikipedia en español. Este embedding no es contextual, así que se utilizó en conjunto con una red recurrente.\n",
        "\n",
        "También se utilizó el embedding de Flair que es contextualizado. Este modelo del lenguaje analiza oraciones a nivel de los caracteres para producir un embedding para cada palabra que es modificado por su contexto. Como este embedding ya utiliza una red recurrente (LSTM), se probó sólo con una red simple MLP para ver cómo se comportaba en esta tarea. Como comparación se utilizó tambien una MLP directamente con el embedding de fasttext.\n",
        "\n",
        "También se intentó utilizar un embedding BERT, pero la librería dio problemas en algunos ejemplos del set de entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## Métricas de evaluación\n",
        "\n",
        "- **Precision:** Mide la razón de objetos clasificados como clase i que realmente corresponden a la clase i. Su interpretación en este problema corresponde, a la razón de entidades o palabras clasificadas como cierta categoría {(1),(2),(3),(4)} que realmente su tag corresponde a esa categoría. \n",
        "\n",
        "- **Recall:** Mide la razón de objetos clasificados como clase i del total de objetos que les corresponde la clase i. Su interpretación en este problema corresponde, a la razón de entidades o palabras correctamente clasificadas como cierta categoría {(1),(2),(3),(4)} con respecto al número total de entidades que realmente les corresponde a esa categoría.\n",
        "- **F1 score:** Corresponde a una medida de acierto combinada entre la Precisión y el Recall. Esta medida tiende a acercarse más al mínimo entre ambas métricas. La interpretación de esta métrica en este problema es servir de trade-off entre el Recall y la Precisión, ya que lo que se busca es que se clasifiquen la mayor cantidad de palabras con cada categoria (Recall) y que de esas palabras categorizadas la mayor cantidad sea categorizada correctamente en su categoria real (Precisión)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T15:44:52.175773Z",
          "start_time": "2020-06-22T15:44:52.172782Z"
        },
        "colab_type": "text",
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## Experimentos\n",
        "\n",
        "\n",
        "El código que les entregaremos servirá de baseline para luego implementar mejores modelos. \n",
        "En general, el código asociado a la carga de los datos, las funciones de entrenamiento, de evaluación y la predicción de los datos de la competencia no deberían cambiar. \n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperparámetros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*.\n",
        "\n",
        "\n",
        "El resumen de los resultados obtenidos se puede ver en la siguiente tabla, donde se destacan los mejores puntajes y la red utilizada en la competencia:\n",
        "\n",
        "| Arquitectura | F1 | Precision | Recall |\n",
        "|-|-|-|-|\n",
        "| Baseline (embedding + LSTM 2 capas) | 0.56 | 0.63 | 0.56 |\n",
        "|  |  |  |  |\n",
        "| Embedding + BiLSTM 1 capas + 0.5 dropout + 128 hidden + 100 emb + 0.001 lr | 0.62 | 0.68 | 0.62 |\n",
        "| Embedding + BiLSTM 1 capas + 0.5 dropout + 256 hidden + 100 emb + 0.001 lr | 0.62 | 0.68 | 0.61 |\n",
        "| Embedding + BiLSTM 2 capas + 0.5 dropout + 128 hidden + 100 emb + 0.001 lr | 0.58 | 0.65 | 0.58 |\n",
        "| Embedding + BiLSTM 2 capas + 0.5 dropout + 256 hidden + 100 emb + 0.001 lr | 0.61 | 0.67 | 0.62 |\n",
        "|  |  |  |  |\n",
        "| Embedding + BiGRU 1 capas + 0.5 dropout + 128 hidden + 100 emb + 0.001 lr | 0.60 | 0.67 | 0.58 |\n",
        "| Embedding + BiGRU 1 capas + 0.25 dropout + 128 hidden + 100 emb + 0.001 lr | 0.61 | 0.67 | 0.60 |\n",
        "| Embedding + BiGRU 1 capas + 0.5 dropout + 256 hidden + 100 emb + 0.001 lr | 0.60 | 0.68 | 0.59 |\n",
        "| Embedding + BiGRU 2 capas + 0.25 dropout + 128 hidden + 100 emb + 0.001 lr | 0.58 | 0.66 | 0.57 |\n",
        "| Embedding + BiGRU 2 capas + 0.5 dropout + 128 hidden + 100 emb + 0.001 lr | 0.62 | 0.68 | 0.61 |\n",
        "| Embedding + BiGRU 2 capas + 0.5 dropout + 256 hidden + 100 emb + 0.001 lr | 0.61 | 0.68 | 0.61 |\n",
        "| Embedding + BiGRU 2 capas + 0.5 dropout + 128 hidden + 150 emb | 0.59 | 0.67 | 0.59 |\n",
        "|  |  |  |  |\n",
        "| **Embedding-Flair-fasttext + BILSTM 2 capas + 0.25 dropout + 128 hidden + 0.0005 lr** | **0.64** | **0.70** | 0.62 |\n",
        "| Embedding-Flair-fasttext  + BiGRU 2 capas + 0.5 dropout + 256 hidden + 0.001 lr | 0.62 | 0.68 | 0.62 |\n",
        "| Embedding-Flair-Contextual-bi + MLP 256 hidden + 0.5 dropout | 0.62 | 0.66 | **0.64** |\n",
        "| Embedding-Flair-Fasttext + MLP 256 hidden + 0.5 dropout | 0.34 | 0.41 | 0.34 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  Carga de datos y Preprocesamiento\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text).\n",
        "En particular usaremos su módulo `data`, el cual según su documentación original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso será el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:24:52.392908Z",
          "start_time": "2020-06-23T22:24:50.641641Z"
        },
        "colab_type": "code",
        "id": "27csY87GaSFO",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e79f487a-63d1-48e7-ed67-a5b25c5c501a"
      },
      "source": [
        "# Instalar torchtext (en codalab) - Descomentar.\n",
        "!pip3 install --upgrade torchtext\n",
        "!pip3 install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.91)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.10)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.2)\n",
            "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (6.0.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.8)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.13.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.0.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.4.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (1.14.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.30 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (1.17.30)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.30->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:24:53.086354Z",
          "start_time": "2020-06-23T22:24:52.394902Z"
        },
        "colab_type": "code",
        "id": "ng7wRGEyawjM",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad \n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### Obtener datos\n",
        "\n",
        "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:49.789218Z",
          "start_time": "2020-06-23T22:24:53.088871Z"
        },
        "colab_type": "code",
        "id": "lbT0g_kC18Jb",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/Data/train_NER_esp.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/Data/val_NER_esp.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/Data/test_NER_esp.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  Fields\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros parámetros relacionados con la forma en que se debe numericalizar un tipo de datos, como un método de tokenización y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "Abogado B-PER\n",
        "General I-PER\n",
        "del I-PER\n",
        "Estado I-PER\n",
        ", O\n",
        "Daryl B-PER\n",
        "Williams I-PER\n",
        "```\n",
        "\n",
        "Cada linea contiene una palabra y su clase. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y los NER_TAGS (`clase`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:49.795126Z",
          "start_time": "2020-06-23T22:25:49.791108Z"
        },
        "colab_type": "code",
        "id": "3DcM_IjgCdzz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6f84bcd1-969f-42a8-babb-fd41d76f774d"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = data.Field(unk_token=None)\n",
        "\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  SequenceTaggingDataset\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext diseñada para contener datasets de sequence labelling. \n",
        "Los ejemplos que se guarden en una instancia de estos serán arreglos de palabras pareados con sus respectivos tags.\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estará pareado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.294370Z",
          "start_time": "2020-06-23T22:25:49.797092Z"
        },
        "colab_type": "code",
        "id": "HsHdGml62J21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "74e6af81-ca78-4d82-e64e-e98362df6985"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train_NER_esp.txt\",\n",
        "    validation=\"val_NER_esp.txt\",\n",
        "    test=\"test_NER_esp.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"iso-8859-1\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.301354Z",
          "start_time": "2020-06-23T22:25:50.296368Z"
        },
        "colab_type": "code",
        "id": "Hu7q3HCliia5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5a19225e-6294-42ba-b5fd-cd3f70847f80"
      },
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
        "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de ejemplos de entrenamiento: 8323\n",
            "Número de ejemplos de validación: 1915\n",
            "Número de ejemplos de test (competencia): 1517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.317313Z",
          "start_time": "2020-06-23T22:25:50.303361Z"
        },
        "colab_type": "code",
        "id": "T023Ld4RaSF4",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "outputId": "8385b5ec-da8f-4b57-fb42-1d9d5f387f92"
      },
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Con', 'O'),\n",
              " ('ese', 'O'),\n",
              " ('objetivo', 'O'),\n",
              " (',', 'O'),\n",
              " ('el', 'O'),\n",
              " ('viceministro', 'O'),\n",
              " ('panameño', 'O'),\n",
              " ('de', 'O'),\n",
              " ('Comercio', 'B-MISC'),\n",
              " ('Exterior', 'I-MISC'),\n",
              " (',', 'O'),\n",
              " ('Roberto', 'B-PER'),\n",
              " ('Henríquez', 'I-PER'),\n",
              " (',', 'O'),\n",
              " ('y', 'O'),\n",
              " ('el', 'O'),\n",
              " ('embajador', 'O'),\n",
              " ('estadounidense', 'O'),\n",
              " ('en', 'O'),\n",
              " ('Panamá', 'B-LOC'),\n",
              " (',', 'O'),\n",
              " ('Simón', 'B-PER'),\n",
              " ('Ferro', 'I-PER'),\n",
              " (',', 'O'),\n",
              " ('se', 'O'),\n",
              " ('reunieron', 'O'),\n",
              " ('el', 'O'),\n",
              " ('lunes', 'O'),\n",
              " ('en', 'O'),\n",
              " ('la', 'O'),\n",
              " ('capital', 'O'),\n",
              " ('panameña', 'O'),\n",
              " (',', 'O'),\n",
              " ('informaron', 'O'),\n",
              " ('hoy', 'O'),\n",
              " ('a', 'O'),\n",
              " ('EFE', 'B-ORG'),\n",
              " ('una', 'O'),\n",
              " ('fuente', 'O'),\n",
              " ('de', 'O'),\n",
              " ('ese', 'O'),\n",
              " ('ministerio', 'O'),\n",
              " ('y', 'O'),\n",
              " ('una', 'O'),\n",
              " ('funcionaria', 'O'),\n",
              " ('de', 'O'),\n",
              " ('la', 'O'),\n",
              " ('Embajada', 'B-ORG'),\n",
              " ('de', 'I-ORG'),\n",
              " ('EEUU', 'I-ORG'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### Construir los vocabularios para el texto y las etiquetas\n",
        "\n",
        "Los vocabularios son los obbjetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields.\n",
        "El siguiente paso consiste en construirlos. Para esto, hacemos uso del método `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.445968Z",
          "start_time": "2020-06-23T22:25:50.320305Z"
        },
        "colab_type": "code",
        "id": "PBhp7WICiibL",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.453960Z",
          "start_time": "2020-06-23T22:25:50.448987Z"
        },
        "colab_type": "code",
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e5d174f6-3b30-42d3-e08d-e1ac5992188f"
      },
      "source": [
        "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens únicos en TEXT: 26101\n",
            "Tokens únicos en NER_TAGS: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.460965Z",
          "start_time": "2020-06-23T22:25:50.455942Z"
        },
        "colab_type": "code",
        "id": "d4FeyL9nFnId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "7246c8d7-179f-45ea-d100-67af3e62b2b8"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'B-ORG',\n",
              " 'I-ORG',\n",
              " 'B-LOC',\n",
              " 'B-PER',\n",
              " 'I-PER',\n",
              " 'I-MISC',\n",
              " 'B-MISC',\n",
              " 'I-LOC']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.473893Z",
          "start_time": "2020-06-23T22:25:50.462923Z"
        },
        "colab_type": "code",
        "id": "m5eSLm4diibR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "8319a540-5e36-47d5-b81d-e82bbd14f0a2"
      },
      "source": [
        "# Tokens mas frecuentes\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 17657),\n",
              " (',', 14716),\n",
              " ('la', 9571),\n",
              " ('que', 7516),\n",
              " ('.', 7263),\n",
              " ('el', 6905),\n",
              " ('en', 6484),\n",
              " ('\"', 5691),\n",
              " ('y', 5336),\n",
              " ('a', 4304)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.479897Z",
          "start_time": "2020-06-23T22:25:50.475889Z"
        },
        "id": "lNZKFLNbdI-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### Frecuencia de los Tags\n",
        "\n",
        "Visualizemos rápidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:50.490885Z",
          "start_time": "2020-06-23T22:25:50.481873Z"
        },
        "colab_type": "code",
        "id": "tuXOsbJUiibh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "1a1000de-2915-402a-a299-6d1bfad3f1ca"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t231920\t87.6%\n",
            "B-ORG\t7390\t 2.8%\n",
            "I-ORG\t4992\t 1.9%\n",
            "B-LOC\t4913\t 1.9%\n",
            "B-PER\t4321\t 1.6%\n",
            "I-PER\t3903\t 1.5%\n",
            "I-MISC\t3212\t 1.2%\n",
            "B-MISC\t2173\t 0.8%\n",
            "I-LOC\t1891\t 0.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T21:44:17.730460Z",
          "start_time": "2020-06-22T21:44:17.724482Z"
        },
        "colab_type": "text",
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### Configuramos pytorch y dividimos los datos.\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaño de los batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.101455Z",
          "start_time": "2020-06-23T22:25:50.492843Z"
        },
        "colab_type": "code",
        "id": "uB7cwLWpaSGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0bb0358d-a3b2-454f-c2f8-9a008a5b6361"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que está disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IWvUw47LaTn",
        "colab_type": "text"
      },
      "source": [
        "#### Dataset para Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTk2wD2ZLZWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "daf4414e-52c1-46d8-e90a-d4517b165f32"
      },
      "source": [
        "import flair\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.training_utils import store_embeddings\n",
        "#flair.device = torch.device('cpu')\n",
        "#flair.embeddings_storage_mode = 'cpu' \n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'nertags'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = './'\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='train_NER_esp.txt',\n",
        "                              dev_file='val_NER_esp.txt',\n",
        "                              test_file='test_NER_esp.txt',\n",
        "                              #column_delimiter=' ',\n",
        "                              encoding=\"iso-8859-1\"\n",
        "                              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 21:41:11,390 Reading data from .\n",
            "2020-08-02 21:41:11,390 Train: train_NER_esp.txt\n",
            "2020-08-02 21:41:11,396 Dev: val_NER_esp.txt\n",
            "2020-08-02 21:41:11,397 Test: test_NER_esp.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RirfTBKKLeLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7a550df-c909-4309-8de1-a7757bfc4e80"
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus: 8323 train + 1915 dev + 1517 test sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tic1pttQLfxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0c31716c-cdc2-4471-b477-b85b6e0836af"
      },
      "source": [
        "corpus.train[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentence: \"Por su parte , el Abogado General de Victoria , Rob Hulls , indicó que no hay nadie que controle que las informaciones contenidas en CrimeNet son veraces .\"   [− Tokens: 29  − Token-Labels: \"Por su parte , el Abogado <B-PER> General <I-PER> de Victoria <B-LOC> , Rob <B-PER> Hulls <I-PER> , indicó que no hay nadie que controle que las informaciones contenidas en CrimeNet <B-MISC> son veraces .\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rVXB_noi001",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "Batch_col = collections.namedtuple('Batch_col', 'text nertags')\n",
        "\n",
        "# Constructor de función para usar embeddings de Flair en batches \n",
        "class Flair_embedding_collate:\n",
        "    def __init__(self, embeddings):\n",
        "        self.embeddings = embeddings\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        global NER_TAGS, PAD_TAG_IDX\n",
        "        with torch.no_grad():\n",
        "            self.embeddings.embed(batch)\n",
        "            lengths = [len(sentence.tokens) for sentence in batch]\n",
        "            longest_token_sequence_in_batch = max(lengths)\n",
        "\n",
        "            pre_allocated_zero_tensor = torch.zeros(\n",
        "              self.embeddings.embedding_length * longest_token_sequence_in_batch,\n",
        "              dtype=torch.float,\n",
        "              device=flair.device,\n",
        "            )\n",
        "            names = self.embeddings.get_names()\n",
        "            all_embs = list()\n",
        "            tags = list()\n",
        "            for sentence in batch:\n",
        "                all_embs += [\n",
        "                      emb for token in sentence for emb in token.get_each_embedding(names)\n",
        "                ]\n",
        "                tags += [NER_TAGS.vocab.stoi[token.get_tag('nertags').value] for token in sentence]\n",
        "                nb_padding_tokens = longest_token_sequence_in_batch - len(sentence)\n",
        "\n",
        "                if nb_padding_tokens > 0:\n",
        "                    t = pre_allocated_zero_tensor[\n",
        "                        : self.embeddings.embedding_length * nb_padding_tokens\n",
        "                    ]\n",
        "                    all_embs.append(t)\n",
        "                    tags += [PAD_TAG_IDX]*nb_padding_tokens\n",
        "\n",
        "            sentence_tensor = torch.cat(all_embs).view(\n",
        "                [\n",
        "                    len(batch),\n",
        "                    longest_token_sequence_in_batch,\n",
        "                    self.embeddings.embedding_length,\n",
        "                ]\n",
        "            ).permute(1,0,2).to(device)\n",
        "            tags_tensor = torch.tensor(tags).view((len(batch),longest_token_sequence_in_batch)).permute(1,0).contiguous().to(device)\n",
        "            store_embeddings(batch, 'none') # Para limpiar embeddings de GPU\n",
        "            return Batch_col(text=sentence_tensor, nertags=tags_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### Métricas de evaluación\n",
        "\n",
        "Además, definiremos las métricas que serán usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `f1`.\n",
        "**Importante**: Noten que la evaluación solo se hace para las Named Entities (sin contar 'O')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.654826Z",
          "start_time": "2020-06-23T22:25:51.103450Z"
        },
        "colab_type": "code",
        "id": "9mUOOLEWiicU",
        "colab": {}
      },
      "source": [
        "# Definimos las métricas\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "import sklearn.exceptions\n",
        "warnings.filterwarnings(\"ignore\",\n",
        "                        category=sklearn.exceptions.UndefinedMetricWarning)\n",
        "\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "    # Obtenemos los indices distintos de 0.\n",
        "\n",
        "    # filtramos <pad> y O para calcular los scores.\n",
        "    mask = [(y_true != o_idx) & (y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu')\n",
        "    y_true = y_true.to('cpu')\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "def calculate_metrics_complete(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "    # Obtenemos los indices distintos de 0.\n",
        "\n",
        "    # filtramos <pad> y O para calcular los scores.\n",
        "    mask = [(y_true != o_idx) & (y_true != pad_idx)]\n",
        "    #y_pred = y_pred[mask]\n",
        "    #y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu')\n",
        "    y_true = y_true.to('cpu')\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### Modelo Baseline\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n",
        "\n",
        "Este constará de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendrá la red.\n",
        "2. Definir los hiperparámetros e inicializar la red. \n",
        "3. Definir la época de entrenamiento\n",
        "3. Definir la función de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.666751Z",
          "start_time": "2020-06-23T22:25:51.656778Z"
        },
        "colab_type": "code",
        "id": "rMPL08XqaSG3",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "              \n",
        "          # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "          self.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "          self.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-22T21:43:02.333880Z",
          "start_time": "2020-06-22T21:43:02.329861Z"
        },
        "colab_type": "text",
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### Hiperparámetros de la red\n",
        "\n",
        "Definimos los hiperparámetros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.705684Z",
          "start_time": "2020-06-23T22:25:51.668746Z"
        },
        "colab_type": "code",
        "id": "EHdi3QdOaSG8",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.25\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.710633Z",
          "start_time": "2020-06-23T22:25:51.706649Z"
        },
        "colab_type": "code",
        "id": "jlF1DhJeaSHA",
        "colab": {}
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la función de loss\n",
        "\n",
        "1.   Elemento de la lista\n",
        "2.   Elemento de la lista\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.715637Z",
          "start_time": "2020-06-23T22:25:51.712628Z"
        },
        "colab_type": "code",
        "id": "6G_4k99_aSHG",
        "colab": {}
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpLK6lqpa2mf",
        "colab_type": "text"
      },
      "source": [
        "#### Optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aedTNFNaa1dW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_opt_params = {\"params\": baseline_model.parameters(),\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": False}\n",
        "baseline_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelos a probar\n",
        "\n",
        "En estas secciones pueden implementar nuevas redes al modificar los hiperparámetros, la cantidad de épocas de entrenamiento, el tamaño de los batches, loss, optimizador, etc... como también definir nuevas arquitecturas de red (mediante la creación de clases nuevas)\n",
        "\n",
        "\n",
        "Al final de estas, hay 4 variables, las cuales deben setear con los modelos, épocas de entrenamiento, loss y optimizador que deseen probar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC1fsDQ2DAJA",
        "colab_type": "text"
      },
      "source": [
        "#### Modelo 1\n",
        "\n",
        "*   LSTM bidireccional de 2 capas  \n",
        "*   Embedding de dimensión: 100 \n",
        "*   LSTM hidden de dimensión: 256 \n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam\n",
        "*   Custom Embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.722604Z",
          "start_time": "2020-06-23T22:25:51.717615Z"
        },
        "colab_type": "code",
        "id": "c81f8ki5aSHL",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "bilstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "bilstm_model_name = 'bilstm-2capas'  # nombre que tendrá el modelo guardado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XYjOO8WsCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstm_n_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUSp29FWWurn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "bisltm_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHO98RPBaWkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstm_opt_params = {\"params\": bilstm_model.parameters(),\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": False}\n",
        "bilstm_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKJky6UlMpMw"
      },
      "source": [
        "#### Modelo 2\n",
        "\n",
        "*   LSTM bidireccional de 1 capa  \n",
        "*   Embedding de dimensión: 100 \n",
        "*   LSTM hidden de dimensión: 128\n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam\n",
        "*   Custom Embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.722604Z",
          "start_time": "2020-06-23T22:25:51.717615Z"
        },
        "colab_type": "code",
        "id": "XcvH7nGAMpM9",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 1  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "bilstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "bilstm_model_name = 'bilstm-1capa'  # nombre que tendrá el modelo guardado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKokRMPwMpNV",
        "colab": {}
      },
      "source": [
        "bilstm_n_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bi02EP3uMpNf",
        "colab": {}
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "bisltm_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0z8gvaTMpNm",
        "colab": {}
      },
      "source": [
        "bilstm_opt_params = {\"params\": bilstm_model.parameters(),\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": False}\n",
        "bilstm_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "#### Modelo 3\n",
        "*   GRU bidireccional de 1 capa  \n",
        "*   Embedding de dimensión: 100 \n",
        "*   LSTM hidden de dimensión: 128 \n",
        "*   Dropout p: 0.25\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam\n",
        "*   Custom Embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrfFcIbemoWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN_2(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "              \n",
        "          # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "          self.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "          self.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.728587Z",
          "start_time": "2020-06-23T22:25:51.724596Z"
        },
        "colab_type": "code",
        "id": "KWPzETaNaSHP",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 1  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "bigru_model = NER_RNN_2(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "bigru_model_name = 'bigru-1'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r7ilTAWkpa_F",
        "colab": {}
      },
      "source": [
        "bigru_n_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rRbVI5Igpa_m",
        "colab": {}
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "bigru_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pnDwFDsXpa_z",
        "colab": {}
      },
      "source": [
        "bigru_opt_params = {\"params\": bigru_model.parameters(),\n",
        "              \"lr\": 0.001}\n",
        "bigru_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MhHX5jdcWhMt"
      },
      "source": [
        "---------------\n",
        "\n",
        "#### Modelo 4\n",
        "*   GRU bidireccional de 2 capas \n",
        "*   Embedding de dimensión: 100 \n",
        "*   LSTM hidden de dimensión: 128 \n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam\n",
        "*   Custom Embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.728587Z",
          "start_time": "2020-06-23T22:25:51.724596Z"
        },
        "colab_type": "code",
        "id": "iVmj3-ENWhNS",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "bigru_model = NER_RNN_2(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "bigru_model_name = 'bigru-2'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CHTDJ37oWhNc",
        "colab": {}
      },
      "source": [
        "bigru_n_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uRSqr0pLWhNi",
        "colab": {}
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "bigru_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uvvpgGdtWhNp",
        "colab": {}
      },
      "source": [
        "bigru_opt_params = {\"params\": bigru_model.parameters(),\n",
        "              \"lr\": 0.001}\n",
        "bigru_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foxHl4JoMqsF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "#### Modelo 5\n",
        "*   LSTM bidireccional de 2 capas \n",
        "*   Embedding de dimensión: 300 \n",
        "*   LSTM hidden de dimensión: 128 \n",
        "*   Dropout p: 0.25\n",
        "*   Learning rate: 0.0005\n",
        "*   Optimizador: Adam\n",
        "*   Embedding Fasttext wikipedia español de Flair\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Oaht7fMpx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN_FLAIR_EMB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "\n",
        "\n",
        "    def forward(self, embedded):\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P74D8zqCMx4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "112cbe4f-ee30-4f13-a88a-bd25b1f9c86a"
      },
      "source": [
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ...\n",
        "\n",
        "embedding_types = [\n",
        "\n",
        "    WordEmbeddings('es'),\n",
        "    #FlairEmbeddings('es-forward'),\n",
        "    #FlairEmbeddings('es-backward')\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "generate_flair_batch = Flair_embedding_collate(embeddings)\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.train,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )\n",
        "valid_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.dev,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 23:12:28,431 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/es-wiki-fasttext-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmpkalrjmcu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1182799328/1182799328 [01:06<00:00, 17849118.60B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 23:13:35,468 copying /tmp/tmpkalrjmcu to cache at /root/.flair/embeddings/es-wiki-fasttext-300d-1M.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 23:13:40,181 removing temp file /tmp/tmpkalrjmcu\n",
            "2020-08-02 23:13:40,919 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/es-wiki-fasttext-300d-1M not found in cache, downloading to /tmp/tmpktnb0oyw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39655801/39655801 [00:03<00:00, 11552560.37B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 23:13:45,028 copying /tmp/tmpktnb0oyw to cache at /root/.flair/embeddings/es-wiki-fasttext-300d-1M\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-02 23:13:45,066 removing temp file /tmp/tmpktnb0oyw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OD1Lu_dNNil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "EMBEDDING_DIM = embeddings.embedding_length  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.25\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "embedded_model = NER_RNN_FLAIR_EMB(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "\n",
        "embedded_model_name = 'flair-fasttext+BILSTM2'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "embedded_n_epochs = 15\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "embedded_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "embedded_opt_params = {\"params\": embedded_model.parameters(),\n",
        "              \"lr\": 0.0005,\n",
        "              \"amsgrad\": True}\n",
        "embedded_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZBYNKtolSMp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "#### Modelo 6\n",
        "*   GRU bidireccional de 2 capas \n",
        "*   Embedding de dimensión: 300 \n",
        "*   GRU hidden de dimensión: 256 \n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam\n",
        "*   Embedding Fasttext wikipedia español de Flair\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-riV0yH2lSMv",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN_GRU_FLAIR_EMB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "\n",
        "\n",
        "    def forward(self, embedded):\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-og_W2DlSNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7556beef-1322-4c4b-f745-01f1c9caf3c9"
      },
      "source": [
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ...\n",
        "\n",
        "embedding_types = [\n",
        "\n",
        "    WordEmbeddings('es'),\n",
        "    #FlairEmbeddings('es-forward'),\n",
        "    #FlairEmbeddings('es-backward')\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "generate_flair_batch = Flair_embedding_collate(embeddings)\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.train,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )\n",
        "valid_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.dev,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sxIIpn4hlSNT",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "EMBEDDING_DIM = embeddings.embedding_length  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "embedded2_model = NER_RNN_GRU_FLAIR_EMB(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "\n",
        "embedded2_model_name = 'flair-fasttext+BIGRU2'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "embedded2_n_epochs = 20\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "embedded2_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "embedded2_opt_params = {\"params\": embedded_model.parameters(),\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": True}\n",
        "embedded2_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eKMUtP3ktlee"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "#### Modelo 7\n",
        "\n",
        "*   Embedding contextual bidireccional de Flair\n",
        "*   Embedding de dimensión: 4096\n",
        "*   MLP\n",
        "*   Capa oculta: 256 \n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wcol496Wtlet",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_FLAIR_EMB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa oculta\n",
        "        self.hidden = nn.Linear(embedding_dim,\n",
        "                            hidden_dim)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "\n",
        "\n",
        "    def forward(self, embedded):\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs = self.hidden(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m3mBp7lQtle3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "854ffd3f-dbc0-4bc1-cda8-d3f6d664e00f"
      },
      "source": [
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ...\n",
        "\n",
        "embedding_types = [\n",
        "\n",
        "    #WordEmbeddings('es'),\n",
        "    FlairEmbeddings('es-forward'),\n",
        "    FlairEmbeddings('es-backward')\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "generate_flair_batch = Flair_embedding_collate(embeddings)\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.train,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )\n",
        "valid_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.dev,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:125: UserWarning: \n",
            "Tesla T4 with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
            "If you want to use the Tesla T4 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tAzUJ53ztlfA",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "EMBEDDING_DIM = embeddings.embedding_length  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "embedded3_model = NER_FLAIR_EMB(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         DROPOUT)\n",
        "\n",
        "embedded3_model_name = 'flairEmbeddings-Bi'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "embedded3_n_epochs = 20\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "embedded3_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "embedded3_opt_params = {\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": True}\n",
        "embedded3_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uX9-FUrzxHP5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "#### Modelo 8\n",
        "\n",
        "*   Embedding contextual Transformer Bert\n",
        "*   Embedding de dimensión: 3072\n",
        "*   MLP\n",
        "*   Capa oculta: 256 \n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9YFCHxExHP7",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_FLAIR_EMB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa oculta\n",
        "        self.hidden = nn.Linear(embedding_dim,\n",
        "                            hidden_dim)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "\n",
        "\n",
        "    def forward(self, embedded):\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs = self.hidden(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBS590WMxHQG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "e68f1a8f-3726-4476-e2d0-e4e4b84c08a9"
      },
      "source": [
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ...\n",
        "\n",
        "embedding_types = [\n",
        "    TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "    #TransformerWordEmbeddings('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "generate_flair_batch = Flair_embedding_collate(embeddings)\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.train,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )\n",
        "valid_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.dev,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-596e235378e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStackedEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# modelo_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model_name_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# n_epochs_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# loss_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xsm3ir3RxHQP",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "EMBEDDING_DIM = embeddings.embedding_length  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "embedded4_model = NER_FLAIR_EMB(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         DROPOUT)\n",
        "\n",
        "embedded4_model_name = 'flairEmbeddings-BERT'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "embedded4_n_epochs = 20\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "embedded4_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "embedded4_opt_params = {\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": True}\n",
        "embedded4_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jBcL7vz_Uqcu"
      },
      "source": [
        "\n",
        "---\n",
        "#### Modelo 9\n",
        "\n",
        "*   Embedding Fasttext wikipedia español de Flair\n",
        "*   Embedding de dimensión: 300\n",
        "*   *MLP*\n",
        "*   Capa oculta: 256 \n",
        "*   Dropout p: 0.5\n",
        "*   Learning rate: 0.001\n",
        "*   Optimizador: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vv9x64uxUqcy",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_FLAIR_EMB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa oculta\n",
        "        self.hidden = nn.Linear(embedding_dim,\n",
        "                            hidden_dim)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "          # Inicializamos los pesos como aleatorios\n",
        "          for name, param in m.named_parameters():\n",
        "              nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "\n",
        "\n",
        "    def forward(self, embedded):\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "        outputs = self.hidden(embedded)\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBbPsz0MUqc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "e68f1a8f-3726-4476-e2d0-e4e4b84c08a9"
      },
      "source": [
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ...\n",
        "\n",
        "embedding_types = [\n",
        "\n",
        "    WordEmbeddings('es'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "generate_flair_batch = Flair_embedding_collate(embeddings)\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.train,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )\n",
        "valid_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.dev,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=generate_flair_batch,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-596e235378e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStackedEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# modelo_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model_name_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# n_epochs_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# loss_3 = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tGvxFFH6UqdG",
        "colab": {}
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "EMBEDDING_DIM = embeddings.embedding_length  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "embedded5_model = NER_FLAIR_EMB(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         DROPOUT)\n",
        "\n",
        "embedded5_model_name = 'flair-fasttext'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "embedded5_n_epochs = 20\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "embedded5_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "embedded5_opt_params = {\n",
        "              \"lr\": 0.001,\n",
        "              \"amsgrad\": True}\n",
        "embedded5_opt_class = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### Entrenamos y evaluamos\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el número de épocas de entrenamiento, la loss y el optimizador que usarán para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LxxpgNGo9j1",
        "colab_type": "text"
      },
      "source": [
        "Modelo 1 BiLSTM 2 capas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7CUYi57o96Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = bilstm_model\n",
        "model_name = bilstm_model_name\n",
        "criterion = bisltm_criterion\n",
        "n_epochs = bilstm_n_epochs\n",
        "opt_params = bilstm_opt_params\n",
        "opt_class = bilstm_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7cjqDY_yOfSM"
      },
      "source": [
        "Modelo 2 BiLSTM 1 capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u6SvhO_ROfSZ",
        "colab": {}
      },
      "source": [
        "model = bilstm_model\n",
        "model_name = bilstm_model_name\n",
        "criterion = bisltm_criterion\n",
        "n_epochs = bilstm_n_epochs\n",
        "opt_params = bilstm_opt_params\n",
        "opt_class = bilstm_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHyx0ouDqFY0",
        "colab_type": "text"
      },
      "source": [
        "Modelo 3 BiGRU 1 capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igXiQDCzqGxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = bigru_model\n",
        "model_name = bigru_model_name\n",
        "criterion = bigru_criterion\n",
        "n_epochs = bigru_n_epochs\n",
        "opt_params = bigru_opt_params\n",
        "opt_class = bigru_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAArv0F3IdrI",
        "colab_type": "text"
      },
      "source": [
        "Modelo 4 BiGRU 2 capas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGXWE64pIe1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = bigru_model\n",
        "model_name = bigru_model_name\n",
        "criterion = bigru_criterion\n",
        "n_epochs = bigru_n_epochs\n",
        "opt_params = bigru_opt_params\n",
        "opt_class = bigru_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXM-dMsQjf8Q",
        "colab_type": "text"
      },
      "source": [
        "Modelo 5 embedding fasttext + BiLSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbIMEdC6jjVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = embedded_model\n",
        "model_name = embedded_model_name\n",
        "criterion = embedded_criterion\n",
        "n_epochs = embedded_n_epochs\n",
        "opt_params = embedded_opt_params\n",
        "opt_class = embedded_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P1mGLL1KmFp7"
      },
      "source": [
        "Modelo 6 embedding fasttext + BiGRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2G13VgUmGnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = embedded2_model\n",
        "model_name = embedded2_model_name\n",
        "criterion = embedded2_criterion\n",
        "n_epochs = embedded2_n_epochs\n",
        "opt_params = embedded2_opt_params\n",
        "opt_class = embedded2_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbaofcveu-l_"
      },
      "source": [
        "Modelo 7 Embedding Flair contextual bidireccional + MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OwnHK7neu_0Z",
        "colab": {}
      },
      "source": [
        "model = embedded3_model\n",
        "model_name = embedded3_model_name\n",
        "criterion = embedded3_criterion\n",
        "n_epochs = embedded3_n_epochs\n",
        "opt_params = embedded3_opt_params\n",
        "opt_class = embedded3_opt_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### Inicializamos la red\n",
        "\n",
        "iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.805380Z",
          "start_time": "2020-06-23T22:25:51.751524Z"
        },
        "colab_type": "code",
        "id": "Q-G_NWFcaSHe",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "#model.apply(init_weights)\n",
        "model.reset_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhbWzvnwqeow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ffe3a4fb-c291-44c7-cd0b-d8074ae6400d"
      },
      "source": [
        "model.modules"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of NER_RNN_2(\n",
              "  (embedding): Embedding(26101, 150, padding_idx=1)\n",
              "  (gru): GRU(150, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.812389Z",
          "start_time": "2020-06-23T22:25:51.806377Z"
        },
        "colab_type": "code",
        "id": "mjWDX2CJaSHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8db65feb-84cb-4a35-fe23-71fa4939dc45"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El modelo actual tiene 5,729,688 parámetros entrenables.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### Definimos el optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:51.819370Z",
          "start_time": "2020-06-23T22:25:51.814357Z"
        },
        "colab_type": "code",
        "id": "AH6o8_cTaSHq",
        "colab": {}
      },
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF2zset_Y68p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_params[\"params\"] = model.parameters()\n",
        "optimizer = opt_class(**opt_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "#### Enviamos el modelo a cuda\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.503226Z",
          "start_time": "2020-06-23T22:25:51.821338Z"
        },
        "colab_type": "code",
        "id": "dqr0AJ6_iicR",
        "colab": {}
      },
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### Definimos el entrenamiento de la red\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n",
        "\n",
        "Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\"\n",
        "\n",
        "Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.515194Z",
          "start_time": "2020-06-23T22:25:54.505221Z"
        },
        "colab_type": "code",
        "id": "DV6YLt0oiicW",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la época:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parámetros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las métricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### `Definimos la función de evaluación`\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validación. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación. \n",
        "Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.527162Z",
          "start_time": "2020-06-23T22:25:54.518186Z"
        },
        "colab_type": "code",
        "id": "WsRuiUuHiicY",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
        "\n",
        "def evaluate_complete(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics_complete(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:54.535141Z",
          "start_time": "2020-06-23T22:25:54.529158Z"
        },
        "colab_type": "code",
        "id": "Xs-n9Y5yiica",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### Entrenamiento de la red\n",
        "\n",
        "En este cuadro de código ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el número de épocas y luego por cada época, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la función `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T21:49:02.524817Z",
          "start_time": "2020-06-23T21:47:09.863026Z"
        },
        "colab_type": "code",
        "id": "iK5lQqpviicf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "63f7c562-d54c-4cac-e3d9-caa13f53f252"
      },
      "source": [
        "# Early stopping\n",
        "bad_ep_count = 0\n",
        "stop_cond = 2 # Number of bad epochs to stop\n",
        "last_valid_loss = float('inf')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validación)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "    if valid_loss > last_valid_loss:\n",
        "        bad_ep_count += 1\n",
        "    else:\n",
        "      bad_ep_count = 0\n",
        "    \n",
        "    last_valid_loss = valid_loss\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )\n",
        "\n",
        "    if bad_ep_count == stop_cond:\n",
        "      print(f\"Early Stopping at {epoch+1} epochs\")\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-189-4598d2d8e6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Entrenar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     train_loss, train_precision, train_recall, train_f1 = train(\n\u001b[0;32m---> 16\u001b[0;31m         model, train_iterator, optimizer, criterion)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Evaluar (valid = validación)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-186-cdad43c8acbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Calculamos el accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Calculamos los gradientes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-bbccad647be6>\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(preds, y_true, pad_idx, o_idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# calcular scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m         \u001b[0mf_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# return no support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iaNe3YzdI_u",
        "colab_type": "text"
      },
      "source": [
        "**Importante**: Recuerden que el último modelo entrenado no es el mejor (probablemente esté *overfitteado*), si no el que guardamos con la menor loss del conjunto de validación.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n",
        "Este problema lo pueden solucionar con *early stopping*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:25:58.680706Z",
          "start_time": "2020-06-23T22:25:58.663725Z"
        },
        "colab_type": "code",
        "id": "y27CNYfrjtQ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a794360f-4f60-466a-dd1f-185468efd520"
      },
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T21:52:04.077979Z",
          "start_time": "2020-06-23T21:52:04.072991Z"
        },
        "id": "BMpiR01ndI_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### Evaluamos el set de validación con el modelo final\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:26:01.788742Z",
          "start_time": "2020-06-23T22:26:00.558829Z"
        },
        "colab_type": "code",
        "id": "s0gVbP8yiicj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0b6978f3-8740-44d1-c9e3-a1ce835a09ee"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val. Loss: 0.201 |  Val. f1: 0.56 | Val. precision: 0.62 | Val. recall: 0.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj765N24dapM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "2edf61d1-4b12-4e62-a351-215eae8ff38e"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate_complete(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val. Loss: 0.201 |  Val. f1: 0.54 | Val. precision: 0.57 | Val. recall: 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhl1aYa5eeJW",
        "colab_type": "text"
      },
      "source": [
        "#### Entrenamiento promediado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbxPf_dwedKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def prom_train(model, model_name, criterion, opt_params, opt_class, n_epochs, train_iterator, valid_iterator, num_train, stop_cond):\n",
        "  \"\"\"\n",
        "  num_train: Number of complete train to calculate mean \n",
        "  stop_cond: Number of bad epochs to stop\n",
        "  \"\"\"\n",
        "  print(f\"Training {model_name}\")\n",
        "  scores = np.zeros((num_train, 3))\n",
        "  for i in range(num_train):\n",
        "    # Configure model and optimizer\n",
        "    model.reset_parameters()\n",
        "    opt_params[\"params\"] = model.parameters()\n",
        "    optimizer = opt_class(**opt_params)\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "    # Early stopping\n",
        "    bad_ep_count = 0\n",
        "    last_valid_loss = float('inf')\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    best_scores = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "        # Entrenar\n",
        "        train_loss, train_precision, train_recall, train_f1 = train(\n",
        "            model, train_iterator, optimizer, criterion)\n",
        "\n",
        "        # Evaluar (valid = validación)\n",
        "        valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "            model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "        # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_scores = [valid_f1, valid_precision, valid_recall]\n",
        "            torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "        # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "        if valid_loss > last_valid_loss:\n",
        "            bad_ep_count += 1\n",
        "        else:\n",
        "          bad_ep_count = 0\n",
        "        \n",
        "        last_valid_loss = valid_loss\n",
        "\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(\n",
        "            f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "        )\n",
        "        print(\n",
        "            f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "        )\n",
        "\n",
        "        if bad_ep_count == stop_cond:\n",
        "          print(f\"Early Stopping at {epoch+1} epochs\")\n",
        "          break\n",
        "    # Save scores\n",
        "    scores[i] = best_scores\n",
        "    mean_scores = scores.mean(axis = 0)\n",
        "    print( \"Mean scores:\")\n",
        "    print(\n",
        "            '\\t Val. f1: {:.2f} |  Val. precision: {:.2f} | Val. recall: {:.2f}'.format(*mean_scores)\n",
        "        )\n",
        "\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_cRrgQugV3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07da164b-d1b4-49e9-861a-2b9fb7490e60"
      },
      "source": [
        "scores = prom_train(model, model_name,\n",
        "           criterion, \n",
        "           opt_params, \n",
        "           opt_class, \n",
        "           n_epochs, \n",
        "           train_iterator, \n",
        "           valid_iterator,\n",
        "           num_train = 3, stop_cond=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training bigru-2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.430 | Train f1: 0.23 | Train precision: 0.34 | Train recall: 0.20\n",
            "\t Val. Loss: 0.265 |  Val. f1: 0.44 |  Val. precision: 0.54 | Val. recall: 0.43\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.172 | Train f1: 0.55 | Train precision: 0.62 | Train recall: 0.53\n",
            "\t Val. Loss: 0.204 |  Val. f1: 0.55 |  Val. precision: 0.65 | Val. recall: 0.53\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.100 | Train f1: 0.68 | Train precision: 0.73 | Train recall: 0.67\n",
            "\t Val. Loss: 0.201 |  Val. f1: 0.59 |  Val. precision: 0.68 | Val. recall: 0.58\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.068 | Train f1: 0.75 | Train precision: 0.79 | Train recall: 0.75\n",
            "\t Val. Loss: 0.194 |  Val. f1: 0.61 |  Val. precision: 0.69 | Val. recall: 0.61\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.049 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.80\n",
            "\t Val. Loss: 0.205 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.038 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.83\n",
            "\t Val. Loss: 0.199 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.030 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
            "\t Val. Loss: 0.219 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.63\n",
            "Epoch: 08 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.026 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.235 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.61\n",
            "Epoch: 09 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.021 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
            "\t Val. Loss: 0.227 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.63\n",
            "Epoch: 10 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.90\n",
            "\t Val. Loss: 0.265 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.60\n",
            "Epoch: 11 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
            "\t Val. Loss: 0.251 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.64\n",
            "Epoch: 12 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
            "\t Val. Loss: 0.259 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
            "Epoch: 13 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
            "\t Val. Loss: 0.305 |  Val. f1: 0.61 |  Val. precision: 0.69 | Val. recall: 0.59\n",
            "Epoch: 14 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.268 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.62\n",
            "Epoch: 15 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.301 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.60\n",
            "Epoch: 16 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.296 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
            "Epoch: 17 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.290 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
            "Epoch: 18 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.312 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
            "Epoch: 19 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
            "\t Val. Loss: 0.347 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.60\n",
            "Epoch: 20 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
            "\t Val. Loss: 0.327 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
            "Mean scores:\n",
            "\t Val. f1: 0.20 |  Val. precision: 0.23 | Val. recall: 0.20\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.426 | Train f1: 0.23 | Train precision: 0.34 | Train recall: 0.20\n",
            "\t Val. Loss: 0.274 |  Val. f1: 0.44 |  Val. precision: 0.55 | Val. recall: 0.42\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.172 | Train f1: 0.55 | Train precision: 0.62 | Train recall: 0.53\n",
            "\t Val. Loss: 0.206 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.55\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.098 | Train f1: nan | Train precision: nan | Train recall: nan\n",
            "\t Val. Loss: 0.198 |  Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.58\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.067 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.75\n",
            "\t Val. Loss: 0.193 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.050 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
            "\t Val. Loss: 0.205 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.038 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
            "\t Val. Loss: 0.216 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.031 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
            "\t Val. Loss: 0.225 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
            "Early Stopping at 7 epochs\n",
            "Mean scores:\n",
            "\t Val. f1: 0.41 |  Val. precision: 0.46 | Val. recall: 0.40\n",
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.424 | Train f1: 0.23 | Train precision: 0.35 | Train recall: 0.20\n",
            "\t Val. Loss: 0.268 |  Val. f1: 0.44 |  Val. precision: 0.55 | Val. recall: 0.41\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.171 | Train f1: 0.56 | Train precision: 0.64 | Train recall: 0.54\n",
            "\t Val. Loss: 0.202 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.55\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.099 | Train f1: 0.68 | Train precision: 0.73 | Train recall: 0.67\n",
            "\t Val. Loss: 0.206 |  Val. f1: 0.58 |  Val. precision: 0.67 | Val. recall: 0.58\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.065 | Train f1: 0.76 | Train precision: 0.80 | Train recall: 0.76\n",
            "\t Val. Loss: 0.222 |  Val. f1: 0.60 |  Val. precision: 0.68 | Val. recall: 0.58\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.80\n",
            "\t Val. Loss: 0.231 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.59\n",
            "Early Stopping at 5 epochs\n",
            "Mean scores:\n",
            "\t Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "\n",
        "### Predecir datos para la competencia\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, predeciremos las etiquetas que serán evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:31:56.776563Z",
          "start_time": "2020-06-23T22:31:39.654525Z"
        },
        "colab_type": "code",
        "id": "1RBs3UU4wLk3",
        "colab": {}
      },
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oración predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jItY7SIlwE_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predecir en modelo con flair\n",
        "def get_words(batch):\n",
        "    sentences = []\n",
        "    for sentence in batch:\n",
        "        sentences.append([token.text for token in sentence])\n",
        "    return sentences\n",
        "    \n",
        "test_iterator = torch.utils.data.DataLoader(\n",
        "        corpus.test,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: (get_words(x),generate_flair_batch(x)),\n",
        "    )\n",
        "\n",
        "tags_vocab = NER_TAGS.vocab.itos\n",
        "model.eval()\n",
        "predictions = []\n",
        "for sentences, batch in test_iterator:\n",
        "    # Predecir los tags de las sentences del batch\n",
        "    predictions_batch = model(batch.text)\n",
        "    predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "    # por cada oración predicha:\n",
        "    for sentence, sentence_prediction in zip(sentences,\n",
        "                                             predictions_batch):\n",
        "        for ix, word in enumerate(sentence):\n",
        "            argmax_index = sentence_prediction[ix,:].topk(1)[1]\n",
        "\n",
        "            current_tag = tags_vocab[argmax_index]\n",
        "            predictions.append([word, current_tag])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### Generar el archivo para la submission\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T22:33:41.845955Z",
          "start_time": "2020-06-23T22:33:41.731717Z"
        },
        "colab_type": "code",
        "id": "RPfZkjJGkWyq",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for word, tag in predictions:\n",
        "    f.write(word + ' ' + tag + '\\n')\n",
        "f.write('\\n')\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-23T21:49:19.575711Z",
          "start_time": "2020-06-23T21:49:19.100486Z"
        },
        "colab_type": "code",
        "id": "k2PqvJAmTFWR",
        "colab": {}
      },
      "source": [
        "# A veces no funciona a la primera. Ejecutar mas de una vez para obtener el archivo...\n",
        "from google.colab import files\n",
        "files.download('predictions.zip')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "\n",
        "Respecto del uso de embeddings pre-entrenados, se ve claramente que funcionan mejor que entrenar uno sobre el corpus que se tiene, y es lo esperado al poseer una representación más rica de las palabras cuando están entrenados sobre corpus grandes. Primero se ve que sólo cambiando el embedding por fasttext se mejoran los puntajes. También, se ve que usando el embedding contextualizado de Flair se logra superar el baseline incluso si se usa un clasificador simple como un MLP, en contraste con fasttext que tiene un mal desempeño por si sólo con el mismo clasificador.\n",
        "\n",
        "Como RNN se utilizo LSTM y GRU, ambas redes conocidas por su uso de compuertas, que permiten controlar la información a actualizar u olvidar. A pesar de tener rendimiento en general similares en este problema, se puede obsevar que cuando se trabajo con un mayor número de capas y/o un mayor número de caracteristicas de capa oculta, la LSTM obtuvo superioridad. Lo anterior es debido, a que la LSTM utiliza 2 estados, lo cual le permite reducir el problema del gradiente descendiente, provocando que se pueda entrenar de mejor forma redes más profundas.\n",
        "\n",
        "Finalmente, como trabajo futuro se propone la utilización de Transformers debido a su mejor eficiencia con respecto a las redes recursivas, ya que estos pueden paralelizar su trabajo en la GPU, hecho que no puede realizar las redes recurrentes. Consecuencias de lo anterior, seria poder entrenar con corpus más grandes y variados, que permitiria una mejor generalización de los modelos. "
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:30:18.109327Z",
     "start_time": "2020-03-19T18:30:18.103344Z"
    },
    "colab_type": "text",
    "id": "q5CSRY4oNCHK"
   },
   "source": [
    "\n",
    "# Minitarea 3\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "Nombre: Javier Urrutia\n",
    "\n",
    "Fecha de Entrega: Domingo 17 de Mayo\n",
    "\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "- El ejercicio consiste en:\n",
    "\n",
    "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
    "    \n",
    "    - Entrenar Word2Vec y FastText sobre un pequeño corpus.\n",
    "    \n",
    "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
    "\n",
    "- La minitarea es INDIVIDUAL.\n",
    "\n",
    "- Está demás decir que no se admiten copias, ni de código, ni de respuestas escritas. \n",
    "\n",
    "- La entrega debe ser por u-cursos.\n",
    "\n",
    "- Atrasos: se descontará un punto por día hábil de atraso tanto para las mini-tareas como para las competencias.\n",
    "\n",
    "- En el horario de auxiliar se abrirán horarios de consulta en donde podrán preguntar acerca del ejercicio y en general, de todo el curso. \n",
    "\n",
    "- Cada sección tiene un punto base y se evalúa sobre 6 puntos.\n",
    "\n",
    "- Al revisar, tu código será ejecutado. Verifica que tu entrega no tenga errores.\n",
    "\n",
    "\n",
    "## Referencias   \n",
    "\n",
    "Vídeos: \n",
    "\n",
    "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
    "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
    "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4wYf0vgnbTv"
   },
   "source": [
    "## Preguntas Teóricas\n",
    "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5hUG6-8ngoK"
   },
   "source": [
    "### Parte 1: Modelos Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yRvZbhsoi8f"
   },
   "source": [
    "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irsqBVmCnx3M"
   },
   "source": [
    "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. (3 puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "\n",
    "Representación escogida del documento de entrada: La representación puede ser a partir de un modelo de words embedding, quedando los documentos representados por un vector no sparse de largo N.\n",
    "\n",
    "Parámetros del modelo: Para un modelo lineal $f(\\vec{x}) = \\vec{x} \\cdot \\vec{W} + \\vec{b}$ los parámetros serían una matriz $W$ de dimensiones (Nx4) y un vector bias $\\vec{b}$ de largo 4, tal que la salida sea de dimensión 4.\n",
    "\n",
    "Transformaciones necesarias: Para obtener el resultado como probabilidades de pertenecia a cada clase, a la salida se aplica una tranformación softmax quedando $\\vec{\\hat{y}} = softmax(f(\\vec{x}))$\n",
    "\n",
    "Función de pérdida escogida: Para el caso multiclase con softmax se usa entropía cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5FaWqBVvL90"
   },
   "source": [
    "**Pregunta 2**: Explique el proceso de entrenamiento y evaluación del modelo. (3 puntos)\n",
    "\n",
    "**Respuesta**: Primero se separan los datos en 3 sets, entrenamiento, validación y test. Para el entrenamiento se utiliza gradiente descendiente. En este se inicia con pesos aleatorios del modelo, luego en forma iterativa se calcula la función de pérdida con los ejemplos, se calculan gradientes y se actualizan los pesos en la dirección contraria a los gradientes. Esto se repite por una cantidad fija de iteraciones o hasta que se cumpla algun criterio. Además se puede agregar un parámetro de regularización a la función de perdida de ser necesario para que el modelo se más robusto a outliers. El modelo entrenado se puedde evaluar con el set de evaluación para ajustar hiperparámetros como la regularización o el optimizador y finalmente se evalá en el set de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkK7pc54njZq"
   },
   "source": [
    "### Parte 2: Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUbJjlj_9AFC"
   },
   "source": [
    "Supongamos que tenemos la siguiente red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obUfuOYB_TOC"
   },
   "source": [
    "![Red](https://drive.google.com/uc?id=1Yd0s9g5SlB1-XuVokGQO2J-yDudQe2Kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2z-8zKW0_6q"
   },
   "source": [
    "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, represéntela matemáticamente, entregando las dimensiones de las matrices y vectores. (3 Puntos)\n",
    "\n",
    "**Respuesta**: \n",
    "Suponiendo que hay bias en cada capa\n",
    "\n",
    "Formula:\n",
    "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = h(f(g(\\vec{x} \\cdot W^1 + \\vec{b^1}) \\cdot W^2 + \\vec{b^2}) \\cdot W^3 + \\vec{b^3}) \\cdot W^4$ + \\vec{b^4}\n",
    "\n",
    "$\\vec{x}$ es de dimensión 3, $W^1$ es de dimension (3x2), $W^2$ es de dimension (2x3), $W^3$ es de dimension (3x1), $W^4$ es de dimension (1x4), $\\vec{b^1}$ es de dimensión 2, $\\vec{b^2}$ es de dimensión 3, $\\vec{b^3}$ es de dimensión 1, $\\vec{b^4}$ es de dimensión 4 e $\\vec{\\hat{y}}$ es de dimension 4.\n",
    "\n",
    "**Pregunta 2**: Qué es backpropagation? Cuales serían los parámetros a evaluar en la red neuronal anterior? (1 punto)\n",
    "\n",
    "**Respuesta**: Backpropagation se refiere a propagar los gradientes de la red del final hacia el principio, siguiendo la regla de la cadena y así obtener los gradientes que se deben usar para el gradiente descendiente. Los parámetros de la red son los pesos $W^i$ en cada capa.\n",
    "\n",
    "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? (2 puntos)\n",
    "\n",
    "**Respuesta**: Para realizar backpropagation primero se pasa una entrada por la red para obtener todos los valores de activación en cada capa. Luego se calculan las dereviadas de la últma capa $\\vec{\\delta^m_{[j]}} = \\frac{\\partial L}{\\partial \\vec{h^m_{[j]}}}$. Despues se propaga hacia atŕas para obtener las derivadas $\\vec{\\delta^l_{[j]}} = {g}'(\\vec{h^l_{[j]}})\\times \\sum_{k}(\\vec{h^{l+1}_{[j]}} \\times W^{l+1}_{[j,k]})$. finalmente se usa la ecuación $\\frac{\\partial L}{\\partial W^{l}_{[i,j]}} = \\vec{\\delta^l_{[j]}} \\times \\vec{z^{l-1}_{[i]}}$ para obtener las derivadas de los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocS_vQhR1gcU"
   },
   "source": [
    "## Pregunta Práctica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ol82nJ0FnmcP"
   },
   "source": [
    "### Parte 3: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgmeSFqKLpFL"
   },
   "source": [
    "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de diálogos de los Simpson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecCvnryeQiG7"
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZgN06q4QPi3"
   },
   "source": [
    "Utilizando el dataset adjunto con la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY3kmg4onnsu"
   },
   "outputs": [],
   "source": [
    "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
    "df = pd.read_csv(data_file)\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAg5a5bmWk3T"
   },
   "source": [
    "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. (4 puntos) (Hint, le puede servir explorar un poco los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWw2fXFRXe5Y"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YILUICGtYJo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131848</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131849</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131850</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131851</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131852</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131853 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "131848              Miss Hoover   \n",
       "131849              Miss Hoover   \n",
       "131850              Miss Hoover   \n",
       "131851             Ralph Wiggum   \n",
       "131852                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "131848                                          I'm back.  \n",
       "131849  You see, class, my Lyme disease turned out to ...  \n",
       "131850                                 Psy-cho-so-ma-tic.  \n",
       "131851                     Does that mean you were crazy?  \n",
       "131852                  No, that means she was faking it.  \n",
       "\n",
       "[131853 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"'tis\": 1,\n",
       "         \"'twas\": 1,\n",
       "         \"'ve\": 1,\n",
       "         '10': 1,\n",
       "         '39': 1,\n",
       "         'a': 1,\n",
       "         \"a's\": 1,\n",
       "         'able': 1,\n",
       "         'ableabout': 1,\n",
       "         'about': 1,\n",
       "         'above': 1,\n",
       "         'abroad': 1,\n",
       "         'abst': 1,\n",
       "         'accordance': 1,\n",
       "         'according': 1,\n",
       "         'accordingly': 1,\n",
       "         'across': 1,\n",
       "         'act': 1,\n",
       "         'actually': 1,\n",
       "         'ad': 1,\n",
       "         'added': 1,\n",
       "         'adj': 1,\n",
       "         'adopted': 1,\n",
       "         'ae': 1,\n",
       "         'af': 1,\n",
       "         'affected': 1,\n",
       "         'affecting': 1,\n",
       "         'affects': 1,\n",
       "         'after': 1,\n",
       "         'afterwards': 1,\n",
       "         'ag': 1,\n",
       "         'again': 1,\n",
       "         'against': 1,\n",
       "         'ago': 1,\n",
       "         'ah': 1,\n",
       "         'ahead': 1,\n",
       "         'ai': 1,\n",
       "         \"ain't\": 1,\n",
       "         'aint': 1,\n",
       "         'al': 1,\n",
       "         'all': 1,\n",
       "         'allow': 1,\n",
       "         'allows': 1,\n",
       "         'almost': 1,\n",
       "         'alone': 1,\n",
       "         'along': 1,\n",
       "         'alongside': 1,\n",
       "         'already': 1,\n",
       "         'also': 1,\n",
       "         'although': 1,\n",
       "         'always': 1,\n",
       "         'am': 1,\n",
       "         'amid': 1,\n",
       "         'amidst': 1,\n",
       "         'among': 1,\n",
       "         'amongst': 1,\n",
       "         'amoungst': 1,\n",
       "         'amount': 1,\n",
       "         'an': 1,\n",
       "         'and': 1,\n",
       "         'announce': 1,\n",
       "         'another': 1,\n",
       "         'any': 1,\n",
       "         'anybody': 1,\n",
       "         'anyhow': 1,\n",
       "         'anymore': 1,\n",
       "         'anyone': 1,\n",
       "         'anything': 1,\n",
       "         'anyway': 1,\n",
       "         'anyways': 1,\n",
       "         'anywhere': 1,\n",
       "         'ao': 1,\n",
       "         'apart': 1,\n",
       "         'apparently': 1,\n",
       "         'appear': 1,\n",
       "         'appreciate': 1,\n",
       "         'appropriate': 1,\n",
       "         'approximately': 1,\n",
       "         'aq': 1,\n",
       "         'ar': 1,\n",
       "         'are': 1,\n",
       "         'area': 1,\n",
       "         'areas': 1,\n",
       "         'aren': 1,\n",
       "         \"aren't\": 1,\n",
       "         'arent': 1,\n",
       "         'arise': 1,\n",
       "         'around': 1,\n",
       "         'arpa': 1,\n",
       "         'as': 1,\n",
       "         'aside': 1,\n",
       "         'ask': 1,\n",
       "         'asked': 1,\n",
       "         'asking': 1,\n",
       "         'asks': 1,\n",
       "         'associated': 1,\n",
       "         'at': 1,\n",
       "         'au': 1,\n",
       "         'auth': 1,\n",
       "         'available': 1,\n",
       "         'aw': 1,\n",
       "         'away': 1,\n",
       "         'awfully': 1,\n",
       "         'az': 1,\n",
       "         'b': 1,\n",
       "         'ba': 1,\n",
       "         'back': 1,\n",
       "         'backed': 1,\n",
       "         'backing': 1,\n",
       "         'backs': 1,\n",
       "         'backward': 1,\n",
       "         'backwards': 1,\n",
       "         'bb': 1,\n",
       "         'bd': 1,\n",
       "         'be': 1,\n",
       "         'became': 1,\n",
       "         'because': 1,\n",
       "         'become': 1,\n",
       "         'becomes': 1,\n",
       "         'becoming': 1,\n",
       "         'been': 1,\n",
       "         'before': 1,\n",
       "         'beforehand': 1,\n",
       "         'began': 1,\n",
       "         'begin': 1,\n",
       "         'beginning': 1,\n",
       "         'beginnings': 1,\n",
       "         'begins': 1,\n",
       "         'behind': 1,\n",
       "         'being': 1,\n",
       "         'beings': 1,\n",
       "         'believe': 1,\n",
       "         'below': 1,\n",
       "         'beside': 1,\n",
       "         'besides': 1,\n",
       "         'best': 1,\n",
       "         'better': 1,\n",
       "         'between': 1,\n",
       "         'beyond': 1,\n",
       "         'bf': 1,\n",
       "         'bg': 1,\n",
       "         'bh': 1,\n",
       "         'bi': 1,\n",
       "         'big': 1,\n",
       "         'bill': 1,\n",
       "         'billion': 1,\n",
       "         'biol': 1,\n",
       "         'bj': 1,\n",
       "         'bm': 1,\n",
       "         'bn': 1,\n",
       "         'bo': 1,\n",
       "         'both': 1,\n",
       "         'bottom': 1,\n",
       "         'br': 1,\n",
       "         'brief': 1,\n",
       "         'briefly': 1,\n",
       "         'bs': 1,\n",
       "         'bt': 1,\n",
       "         'but': 1,\n",
       "         'buy': 1,\n",
       "         'bv': 1,\n",
       "         'bw': 1,\n",
       "         'by': 1,\n",
       "         'bz': 1,\n",
       "         'c': 1,\n",
       "         \"c'mon\": 1,\n",
       "         \"c's\": 1,\n",
       "         'ca': 1,\n",
       "         'call': 1,\n",
       "         'came': 1,\n",
       "         'can': 1,\n",
       "         \"can't\": 1,\n",
       "         'cannot': 1,\n",
       "         'cant': 1,\n",
       "         'caption': 1,\n",
       "         'case': 1,\n",
       "         'cases': 1,\n",
       "         'cause': 1,\n",
       "         'causes': 1,\n",
       "         'cc': 1,\n",
       "         'cd': 1,\n",
       "         'certain': 1,\n",
       "         'certainly': 1,\n",
       "         'cf': 1,\n",
       "         'cg': 1,\n",
       "         'ch': 1,\n",
       "         'changes': 1,\n",
       "         'ci': 1,\n",
       "         'ck': 1,\n",
       "         'cl': 1,\n",
       "         'clear': 1,\n",
       "         'clearly': 1,\n",
       "         'click': 1,\n",
       "         'cm': 1,\n",
       "         'cmon': 1,\n",
       "         'cn': 1,\n",
       "         'co': 1,\n",
       "         'co.': 1,\n",
       "         'com': 1,\n",
       "         'come': 1,\n",
       "         'comes': 1,\n",
       "         'computer': 1,\n",
       "         'con': 1,\n",
       "         'concerning': 1,\n",
       "         'consequently': 1,\n",
       "         'consider': 1,\n",
       "         'considering': 1,\n",
       "         'contain': 1,\n",
       "         'containing': 1,\n",
       "         'contains': 1,\n",
       "         'copy': 1,\n",
       "         'corresponding': 1,\n",
       "         'could': 1,\n",
       "         \"could've\": 1,\n",
       "         'couldn': 1,\n",
       "         \"couldn't\": 1,\n",
       "         'couldnt': 1,\n",
       "         'course': 1,\n",
       "         'cr': 1,\n",
       "         'cry': 1,\n",
       "         'cs': 1,\n",
       "         'cu': 1,\n",
       "         'currently': 1,\n",
       "         'cv': 1,\n",
       "         'cx': 1,\n",
       "         'cy': 1,\n",
       "         'cz': 1,\n",
       "         'd': 1,\n",
       "         'dare': 1,\n",
       "         \"daren't\": 1,\n",
       "         'darent': 1,\n",
       "         'date': 1,\n",
       "         'de': 1,\n",
       "         'dear': 1,\n",
       "         'definitely': 1,\n",
       "         'describe': 1,\n",
       "         'described': 1,\n",
       "         'despite': 1,\n",
       "         'detail': 1,\n",
       "         'did': 1,\n",
       "         'didn': 1,\n",
       "         \"didn't\": 1,\n",
       "         'didnt': 1,\n",
       "         'differ': 1,\n",
       "         'different': 1,\n",
       "         'differently': 1,\n",
       "         'directly': 1,\n",
       "         'dj': 1,\n",
       "         'dk': 1,\n",
       "         'dm': 1,\n",
       "         'do': 1,\n",
       "         'does': 1,\n",
       "         'doesn': 1,\n",
       "         \"doesn't\": 1,\n",
       "         'doesnt': 1,\n",
       "         'doing': 1,\n",
       "         'don': 1,\n",
       "         \"don't\": 1,\n",
       "         'done': 1,\n",
       "         'dont': 1,\n",
       "         'doubtful': 1,\n",
       "         'down': 1,\n",
       "         'downed': 1,\n",
       "         'downing': 1,\n",
       "         'downs': 1,\n",
       "         'downwards': 1,\n",
       "         'due': 1,\n",
       "         'during': 1,\n",
       "         'dz': 1,\n",
       "         'e': 1,\n",
       "         'each': 1,\n",
       "         'early': 1,\n",
       "         'ec': 1,\n",
       "         'ed': 1,\n",
       "         'edu': 1,\n",
       "         'ee': 1,\n",
       "         'effect': 1,\n",
       "         'eg': 1,\n",
       "         'eh': 1,\n",
       "         'eight': 1,\n",
       "         'eighty': 1,\n",
       "         'either': 1,\n",
       "         'eleven': 1,\n",
       "         'else': 1,\n",
       "         'elsewhere': 1,\n",
       "         'empty': 1,\n",
       "         'end': 1,\n",
       "         'ended': 1,\n",
       "         'ending': 1,\n",
       "         'ends': 1,\n",
       "         'enough': 1,\n",
       "         'entirely': 1,\n",
       "         'er': 1,\n",
       "         'es': 1,\n",
       "         'especially': 1,\n",
       "         'et': 1,\n",
       "         'et-al': 1,\n",
       "         'etc': 1,\n",
       "         'even': 1,\n",
       "         'evenly': 1,\n",
       "         'ever': 1,\n",
       "         'evermore': 1,\n",
       "         'every': 1,\n",
       "         'everybody': 1,\n",
       "         'everyone': 1,\n",
       "         'everything': 1,\n",
       "         'everywhere': 1,\n",
       "         'ex': 1,\n",
       "         'exactly': 1,\n",
       "         'example': 1,\n",
       "         'except': 1,\n",
       "         'f': 1,\n",
       "         'face': 1,\n",
       "         'faces': 1,\n",
       "         'fact': 1,\n",
       "         'facts': 1,\n",
       "         'fairly': 1,\n",
       "         'far': 1,\n",
       "         'farther': 1,\n",
       "         'felt': 1,\n",
       "         'few': 1,\n",
       "         'fewer': 1,\n",
       "         'ff': 1,\n",
       "         'fi': 1,\n",
       "         'fifteen': 1,\n",
       "         'fifth': 1,\n",
       "         'fifty': 1,\n",
       "         'fify': 1,\n",
       "         'fill': 1,\n",
       "         'find': 1,\n",
       "         'finds': 1,\n",
       "         'fire': 1,\n",
       "         'first': 1,\n",
       "         'five': 1,\n",
       "         'fix': 1,\n",
       "         'fj': 1,\n",
       "         'fk': 1,\n",
       "         'fm': 1,\n",
       "         'fo': 1,\n",
       "         'followed': 1,\n",
       "         'following': 1,\n",
       "         'follows': 1,\n",
       "         'for': 1,\n",
       "         'forever': 1,\n",
       "         'former': 1,\n",
       "         'formerly': 1,\n",
       "         'forth': 1,\n",
       "         'forty': 1,\n",
       "         'forward': 1,\n",
       "         'found': 1,\n",
       "         'four': 1,\n",
       "         'fr': 1,\n",
       "         'free': 1,\n",
       "         'from': 1,\n",
       "         'front': 1,\n",
       "         'full': 1,\n",
       "         'fully': 1,\n",
       "         'further': 1,\n",
       "         'furthered': 1,\n",
       "         'furthering': 1,\n",
       "         'furthermore': 1,\n",
       "         'furthers': 1,\n",
       "         'fx': 1,\n",
       "         'g': 1,\n",
       "         'ga': 1,\n",
       "         'gave': 1,\n",
       "         'gb': 1,\n",
       "         'gd': 1,\n",
       "         'ge': 1,\n",
       "         'general': 1,\n",
       "         'generally': 1,\n",
       "         'get': 1,\n",
       "         'gets': 1,\n",
       "         'getting': 1,\n",
       "         'gf': 1,\n",
       "         'gg': 1,\n",
       "         'gh': 1,\n",
       "         'gi': 1,\n",
       "         'give': 1,\n",
       "         'given': 1,\n",
       "         'gives': 1,\n",
       "         'giving': 1,\n",
       "         'gl': 1,\n",
       "         'gm': 1,\n",
       "         'gmt': 1,\n",
       "         'gn': 1,\n",
       "         'go': 1,\n",
       "         'goes': 1,\n",
       "         'going': 1,\n",
       "         'gone': 1,\n",
       "         'good': 1,\n",
       "         'goods': 1,\n",
       "         'got': 1,\n",
       "         'gotten': 1,\n",
       "         'gov': 1,\n",
       "         'gp': 1,\n",
       "         'gq': 1,\n",
       "         'gr': 1,\n",
       "         'great': 1,\n",
       "         'greater': 1,\n",
       "         'greatest': 1,\n",
       "         'greetings': 1,\n",
       "         'group': 1,\n",
       "         'grouped': 1,\n",
       "         'grouping': 1,\n",
       "         'groups': 1,\n",
       "         'gs': 1,\n",
       "         'gt': 1,\n",
       "         'gu': 1,\n",
       "         'gw': 1,\n",
       "         'gy': 1,\n",
       "         'h': 1,\n",
       "         'had': 1,\n",
       "         \"hadn't\": 1,\n",
       "         'hadnt': 1,\n",
       "         'half': 1,\n",
       "         'happens': 1,\n",
       "         'hardly': 1,\n",
       "         'has': 1,\n",
       "         'hasn': 1,\n",
       "         \"hasn't\": 1,\n",
       "         'hasnt': 1,\n",
       "         'have': 1,\n",
       "         'haven': 1,\n",
       "         \"haven't\": 1,\n",
       "         'havent': 1,\n",
       "         'having': 1,\n",
       "         'he': 1,\n",
       "         \"he'd\": 1,\n",
       "         \"he'll\": 1,\n",
       "         \"he's\": 1,\n",
       "         'hed': 1,\n",
       "         'hell': 1,\n",
       "         'hello': 1,\n",
       "         'help': 1,\n",
       "         'hence': 1,\n",
       "         'her': 1,\n",
       "         'here': 1,\n",
       "         \"here's\": 1,\n",
       "         'hereafter': 1,\n",
       "         'hereby': 1,\n",
       "         'herein': 1,\n",
       "         'heres': 1,\n",
       "         'hereupon': 1,\n",
       "         'hers': 1,\n",
       "         'herself': 1,\n",
       "         'herse”': 1,\n",
       "         'hes': 1,\n",
       "         'hi': 1,\n",
       "         'hid': 1,\n",
       "         'high': 1,\n",
       "         'higher': 1,\n",
       "         'highest': 1,\n",
       "         'him': 1,\n",
       "         'himself': 1,\n",
       "         'himse”': 1,\n",
       "         'his': 1,\n",
       "         'hither': 1,\n",
       "         'hk': 1,\n",
       "         'hm': 1,\n",
       "         'hn': 1,\n",
       "         'home': 1,\n",
       "         'homepage': 1,\n",
       "         'hopefully': 1,\n",
       "         'how': 1,\n",
       "         \"how'd\": 1,\n",
       "         \"how'll\": 1,\n",
       "         \"how's\": 1,\n",
       "         'howbeit': 1,\n",
       "         'however': 1,\n",
       "         'hr': 1,\n",
       "         'ht': 1,\n",
       "         'htm': 1,\n",
       "         'html': 1,\n",
       "         'http': 1,\n",
       "         'hu': 1,\n",
       "         'hundred': 1,\n",
       "         'i': 1,\n",
       "         \"i'd\": 1,\n",
       "         \"i'll\": 1,\n",
       "         \"i'm\": 1,\n",
       "         \"i've\": 1,\n",
       "         'i.e.': 1,\n",
       "         'id': 1,\n",
       "         'ie': 1,\n",
       "         'if': 1,\n",
       "         'ignored': 1,\n",
       "         'ii': 1,\n",
       "         'il': 1,\n",
       "         'ill': 1,\n",
       "         'im': 1,\n",
       "         'immediate': 1,\n",
       "         'immediately': 1,\n",
       "         'importance': 1,\n",
       "         'important': 1,\n",
       "         'in': 1,\n",
       "         'inasmuch': 1,\n",
       "         'inc': 1,\n",
       "         'inc.': 1,\n",
       "         'indeed': 1,\n",
       "         'index': 1,\n",
       "         'indicate': 1,\n",
       "         'indicated': 1,\n",
       "         'indicates': 1,\n",
       "         'information': 1,\n",
       "         'inner': 1,\n",
       "         'inside': 1,\n",
       "         'insofar': 1,\n",
       "         'instead': 1,\n",
       "         'int': 1,\n",
       "         'interest': 1,\n",
       "         'interested': 1,\n",
       "         'interesting': 1,\n",
       "         'interests': 1,\n",
       "         'into': 1,\n",
       "         'invention': 1,\n",
       "         'inward': 1,\n",
       "         'io': 1,\n",
       "         'iq': 1,\n",
       "         'ir': 1,\n",
       "         'is': 1,\n",
       "         'isn': 1,\n",
       "         \"isn't\": 1,\n",
       "         'isnt': 1,\n",
       "         'it': 1,\n",
       "         \"it'd\": 1,\n",
       "         \"it'll\": 1,\n",
       "         \"it's\": 1,\n",
       "         'itd': 1,\n",
       "         'itll': 1,\n",
       "         'its': 1,\n",
       "         'itself': 1,\n",
       "         'itse”': 1,\n",
       "         'ive': 1,\n",
       "         'j': 1,\n",
       "         'je': 1,\n",
       "         'jm': 1,\n",
       "         'jo': 1,\n",
       "         'join': 1,\n",
       "         'jp': 1,\n",
       "         'just': 1,\n",
       "         'k': 1,\n",
       "         'ke': 1,\n",
       "         'keep': 1,\n",
       "         'keeps': 1,\n",
       "         'kept': 1,\n",
       "         'keys': 1,\n",
       "         'kg': 1,\n",
       "         'kh': 1,\n",
       "         'ki': 1,\n",
       "         'kind': 1,\n",
       "         'km': 1,\n",
       "         'kn': 1,\n",
       "         'knew': 1,\n",
       "         'know': 1,\n",
       "         'known': 1,\n",
       "         'knows': 1,\n",
       "         'kp': 1,\n",
       "         'kr': 1,\n",
       "         'kw': 1,\n",
       "         'ky': 1,\n",
       "         'kz': 1,\n",
       "         'l': 1,\n",
       "         'la': 1,\n",
       "         'large': 1,\n",
       "         'largely': 1,\n",
       "         'last': 1,\n",
       "         'lately': 1,\n",
       "         'later': 1,\n",
       "         'latest': 1,\n",
       "         'latter': 1,\n",
       "         'latterly': 1,\n",
       "         'lb': 1,\n",
       "         'lc': 1,\n",
       "         'least': 1,\n",
       "         'length': 1,\n",
       "         'less': 1,\n",
       "         'lest': 1,\n",
       "         'let': 1,\n",
       "         \"let's\": 1,\n",
       "         'lets': 1,\n",
       "         'li': 1,\n",
       "         'like': 1,\n",
       "         'liked': 1,\n",
       "         'likely': 1,\n",
       "         'likewise': 1,\n",
       "         'line': 1,\n",
       "         'little': 1,\n",
       "         'lk': 1,\n",
       "         'll': 1,\n",
       "         'long': 1,\n",
       "         'longer': 1,\n",
       "         'longest': 1,\n",
       "         'look': 1,\n",
       "         'looking': 1,\n",
       "         'looks': 1,\n",
       "         'low': 1,\n",
       "         'lower': 1,\n",
       "         'lr': 1,\n",
       "         'ls': 1,\n",
       "         'lt': 1,\n",
       "         'ltd': 1,\n",
       "         'lu': 1,\n",
       "         'lv': 1,\n",
       "         'ly': 1,\n",
       "         'm': 1,\n",
       "         'ma': 1,\n",
       "         'made': 1,\n",
       "         'mainly': 1,\n",
       "         'make': 1,\n",
       "         'makes': 1,\n",
       "         'making': 1,\n",
       "         'man': 1,\n",
       "         'many': 1,\n",
       "         'may': 1,\n",
       "         'maybe': 1,\n",
       "         \"mayn't\": 1,\n",
       "         'maynt': 1,\n",
       "         'mc': 1,\n",
       "         'md': 1,\n",
       "         'me': 1,\n",
       "         'mean': 1,\n",
       "         'means': 1,\n",
       "         'meantime': 1,\n",
       "         'meanwhile': 1,\n",
       "         'member': 1,\n",
       "         'members': 1,\n",
       "         'men': 1,\n",
       "         'merely': 1,\n",
       "         'mg': 1,\n",
       "         'mh': 1,\n",
       "         'microsoft': 1,\n",
       "         'might': 1,\n",
       "         \"might've\": 1,\n",
       "         \"mightn't\": 1,\n",
       "         'mightnt': 1,\n",
       "         'mil': 1,\n",
       "         'mill': 1,\n",
       "         'million': 1,\n",
       "         'mine': 1,\n",
       "         'minus': 1,\n",
       "         'miss': 1,\n",
       "         'mk': 1,\n",
       "         'ml': 1,\n",
       "         'mm': 1,\n",
       "         'mn': 1,\n",
       "         'mo': 1,\n",
       "         'more': 1,\n",
       "         'moreover': 1,\n",
       "         'most': 1,\n",
       "         'mostly': 1,\n",
       "         'move': 1,\n",
       "         'mp': 1,\n",
       "         'mq': 1,\n",
       "         'mr': 1,\n",
       "         'mrs': 1,\n",
       "         'ms': 1,\n",
       "         'msie': 1,\n",
       "         'mt': 1,\n",
       "         'mu': 1,\n",
       "         'much': 1,\n",
       "         'mug': 1,\n",
       "         'must': 1,\n",
       "         \"must've\": 1,\n",
       "         \"mustn't\": 1,\n",
       "         'mustnt': 1,\n",
       "         'mv': 1,\n",
       "         'mw': 1,\n",
       "         'mx': 1,\n",
       "         'my': 1,\n",
       "         'myself': 1,\n",
       "         'myse”': 1,\n",
       "         'mz': 1,\n",
       "         'n': 1,\n",
       "         'na': 1,\n",
       "         'name': 1,\n",
       "         'namely': 1,\n",
       "         'nay': 1,\n",
       "         'nc': 1,\n",
       "         'nd': 1,\n",
       "         'ne': 1,\n",
       "         'near': 1,\n",
       "         'nearly': 1,\n",
       "         'necessarily': 1,\n",
       "         'necessary': 1,\n",
       "         'need': 1,\n",
       "         'needed': 1,\n",
       "         'needing': 1,\n",
       "         \"needn't\": 1,\n",
       "         'neednt': 1,\n",
       "         'needs': 1,\n",
       "         'neither': 1,\n",
       "         'net': 1,\n",
       "         'netscape': 1,\n",
       "         'never': 1,\n",
       "         'neverf': 1,\n",
       "         'neverless': 1,\n",
       "         'nevertheless': 1,\n",
       "         'new': 1,\n",
       "         'newer': 1,\n",
       "         'newest': 1,\n",
       "         'next': 1,\n",
       "         'nf': 1,\n",
       "         'ng': 1,\n",
       "         'ni': 1,\n",
       "         'nine': 1,\n",
       "         'ninety': 1,\n",
       "         'nl': 1,\n",
       "         'no': 1,\n",
       "         'no-one': 1,\n",
       "         'nobody': 1,\n",
       "         'non': 1,\n",
       "         'none': 1,\n",
       "         'nonetheless': 1,\n",
       "         'noone': 1,\n",
       "         'nor': 1,\n",
       "         'normally': 1,\n",
       "         'nos': 1,\n",
       "         'not': 1,\n",
       "         'noted': 1,\n",
       "         'nothing': 1,\n",
       "         'notwithstanding': 1,\n",
       "         'novel': 1,\n",
       "         'now': 1,\n",
       "         'nowhere': 1,\n",
       "         'np': 1,\n",
       "         'nr': 1,\n",
       "         'nu': 1,\n",
       "         nan: 1,\n",
       "         'number': 1,\n",
       "         'numbers': 1,\n",
       "         'nz': 1,\n",
       "         'o': 1,\n",
       "         'obtain': 1,\n",
       "         'obtained': 1,\n",
       "         'obviously': 1,\n",
       "         'of': 1,\n",
       "         'off': 1,\n",
       "         'often': 1,\n",
       "         'oh': 1,\n",
       "         'ok': 1,\n",
       "         'okay': 1,\n",
       "         'old': 1,\n",
       "         'older': 1,\n",
       "         'oldest': 1,\n",
       "         'om': 1,\n",
       "         'omitted': 1,\n",
       "         'on': 1,\n",
       "         'once': 1,\n",
       "         'one': 1,\n",
       "         \"one's\": 1,\n",
       "         'ones': 1,\n",
       "         'only': 1,\n",
       "         'onto': 1,\n",
       "         'open': 1,\n",
       "         'opened': 1,\n",
       "         'opening': 1,\n",
       "         'opens': 1,\n",
       "         'opposite': 1,\n",
       "         'or': 1,\n",
       "         'ord': 1,\n",
       "         'order': 1,\n",
       "         'ordered': 1,\n",
       "         'ordering': 1,\n",
       "         'orders': 1,\n",
       "         'org': 1,\n",
       "         'other': 1,\n",
       "         'others': 1,\n",
       "         'otherwise': 1,\n",
       "         'ought': 1,\n",
       "         \"oughtn't\": 1,\n",
       "         'oughtnt': 1,\n",
       "         'our': 1,\n",
       "         'ours': 1,\n",
       "         'ourselves': 1,\n",
       "         'out': 1,\n",
       "         'outside': 1,\n",
       "         'over': 1,\n",
       "         'overall': 1,\n",
       "         'owing': 1,\n",
       "         'own': 1,\n",
       "         'p': 1,\n",
       "         'pa': 1,\n",
       "         'page': 1,\n",
       "         'pages': 1,\n",
       "         'part': 1,\n",
       "         'parted': 1,\n",
       "         'particular': 1,\n",
       "         'particularly': 1,\n",
       "         'parting': 1,\n",
       "         'parts': 1,\n",
       "         'past': 1,\n",
       "         'pe': 1,\n",
       "         'per': 1,\n",
       "         'perhaps': 1,\n",
       "         'pf': 1,\n",
       "         'pg': 1,\n",
       "         'ph': 1,\n",
       "         'pk': 1,\n",
       "         'pl': 1,\n",
       "         'place': 1,\n",
       "         'placed': 1,\n",
       "         'places': 1,\n",
       "         'please': 1,\n",
       "         'plus': 1,\n",
       "         'pm': 1,\n",
       "         'pmid': 1,\n",
       "         'pn': 1,\n",
       "         'point': 1,\n",
       "         'pointed': 1,\n",
       "         'pointing': 1,\n",
       "         'points': 1,\n",
       "         'poorly': 1,\n",
       "         'possible': 1,\n",
       "         'possibly': 1,\n",
       "         'potentially': 1,\n",
       "         'pp': 1,\n",
       "         'pr': 1,\n",
       "         'predominantly': 1,\n",
       "         'present': 1,\n",
       "         'presented': 1,\n",
       "         'presenting': 1,\n",
       "         'presents': 1,\n",
       "         'presumably': 1,\n",
       "         'previously': 1,\n",
       "         'primarily': 1,\n",
       "         'probably': 1,\n",
       "         'problem': 1,\n",
       "         'problems': 1,\n",
       "         'promptly': 1,\n",
       "         'proud': 1,\n",
       "         'provided': 1,\n",
       "         'provides': 1,\n",
       "         'pt': 1,\n",
       "         'put': 1,\n",
       "         'puts': 1,\n",
       "         'pw': 1,\n",
       "         'py': 1,\n",
       "         'q': 1,\n",
       "         'qa': 1,\n",
       "         'que': 1,\n",
       "         'quickly': 1,\n",
       "         'quite': 1,\n",
       "         'qv': 1,\n",
       "         'r': 1,\n",
       "         'ran': 1,\n",
       "         'rather': 1,\n",
       "         'rd': 1,\n",
       "         're': 1,\n",
       "         'readily': 1,\n",
       "         'really': 1,\n",
       "         'reasonably': 1,\n",
       "         'recent': 1,\n",
       "         'recently': 1,\n",
       "         'ref': 1,\n",
       "         'refs': 1,\n",
       "         'regarding': 1,\n",
       "         'regardless': 1,\n",
       "         'regards': 1,\n",
       "         'related': 1,\n",
       "         'relatively': 1,\n",
       "         'research': 1,\n",
       "         'reserved': 1,\n",
       "         'respectively': 1,\n",
       "         'resulted': 1,\n",
       "         'resulting': 1,\n",
       "         'results': 1,\n",
       "         'right': 1,\n",
       "         'ring': 1,\n",
       "         'ro': 1,\n",
       "         'room': 1,\n",
       "         'rooms': 1,\n",
       "         'round': 1,\n",
       "         'ru': 1,\n",
       "         'run': 1,\n",
       "         'rw': 1,\n",
       "         's': 1,\n",
       "         'sa': 1,\n",
       "         'said': 1,\n",
       "         'same': 1,\n",
       "         'saw': 1,\n",
       "         'say': 1,\n",
       "         'saying': 1,\n",
       "         'says': 1,\n",
       "         'sb': 1,\n",
       "         'sc': 1,\n",
       "         'sd': 1,\n",
       "         'se': 1,\n",
       "         'sec': 1,\n",
       "         'second': 1,\n",
       "         'secondly': 1,\n",
       "         'seconds': 1,\n",
       "         'section': 1,\n",
       "         'see': 1,\n",
       "         'seeing': 1,\n",
       "         'seem': 1,\n",
       "         'seemed': 1,\n",
       "         'seeming': 1,\n",
       "         'seems': 1,\n",
       "         'seen': 1,\n",
       "         'sees': 1,\n",
       "         'self': 1,\n",
       "         'selves': 1,\n",
       "         'sensible': 1,\n",
       "         'sent': 1,\n",
       "         'serious': 1,\n",
       "         'seriously': 1,\n",
       "         'seven': 1,\n",
       "         'seventy': 1,\n",
       "         'several': 1,\n",
       "         'sg': 1,\n",
       "         'sh': 1,\n",
       "         'shall': 1,\n",
       "         \"shan't\": 1,\n",
       "         'shant': 1,\n",
       "         'she': 1,\n",
       "         \"she'd\": 1,\n",
       "         \"she'll\": 1,\n",
       "         \"she's\": 1,\n",
       "         'shed': 1,\n",
       "         'shell': 1,\n",
       "         'shes': 1,\n",
       "         'should': 1,\n",
       "         \"should've\": 1,\n",
       "         'shouldn': 1,\n",
       "         \"shouldn't\": 1,\n",
       "         'shouldnt': 1,\n",
       "         'show': 1,\n",
       "         'showed': 1,\n",
       "         'showing': 1,\n",
       "         'shown': 1,\n",
       "         'showns': 1,\n",
       "         'shows': 1,\n",
       "         'si': 1,\n",
       "         'side': 1,\n",
       "         'sides': 1,\n",
       "         'significant': 1,\n",
       "         'significantly': 1,\n",
       "         'similar': 1,\n",
       "         'similarly': 1,\n",
       "         'since': 1,\n",
       "         'sincere': 1,\n",
       "         'site': 1,\n",
       "         'six': 1,\n",
       "         'sixty': 1,\n",
       "         'sj': 1,\n",
       "         'sk': 1,\n",
       "         'sl': 1,\n",
       "         'slightly': 1,\n",
       "         'sm': 1,\n",
       "         'small': 1,\n",
       "         'smaller': 1,\n",
       "         'smallest': 1,\n",
       "         'sn': 1,\n",
       "         'so': 1,\n",
       "         'some': 1,\n",
       "         'somebody': 1,\n",
       "         'someday': 1,\n",
       "         'somehow': 1,\n",
       "         'someone': 1,\n",
       "         'somethan': 1,\n",
       "         'something': 1,\n",
       "         'sometime': 1,\n",
       "         'sometimes': 1,\n",
       "         'somewhat': 1,\n",
       "         'somewhere': 1,\n",
       "         'soon': 1,\n",
       "         'sorry': 1,\n",
       "         'specifically': 1,\n",
       "         'specified': 1,\n",
       "         'specify': 1,\n",
       "         'specifying': 1,\n",
       "         'sr': 1,\n",
       "         'st': 1,\n",
       "         'state': 1,\n",
       "         'states': 1,\n",
       "         'still': 1,\n",
       "         'stop': 1,\n",
       "         'strongly': 1,\n",
       "         'su': 1,\n",
       "         'sub': 1,\n",
       "         'substantially': 1,\n",
       "         'successfully': 1,\n",
       "         'such': 1,\n",
       "         'sufficiently': 1,\n",
       "         'suggest': 1,\n",
       "         'sup': 1,\n",
       "         'sure': 1,\n",
       "         'sv': 1,\n",
       "         'sy': 1,\n",
       "         'system': 1,\n",
       "         'sz': 1,\n",
       "         't': 1,\n",
       "         \"t's\": 1,\n",
       "         'take': 1,\n",
       "         'taken': 1,\n",
       "         'taking': 1,\n",
       "         'tc': 1,\n",
       "         'td': 1,\n",
       "         'tell': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~«»“”‘’…—\n"
     ]
    }
   ],
   "source": [
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "    else:\n",
    "        tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "\n",
    "    tokenized_doc = [\n",
    "        token for token in tokenized_doc if token.lower() not in stopwords\n",
    "    ]\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = [simple_tokenizer(doc, lower=True) for doc in df.spoken_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de dialogo: ['hey', 'flanders', 'time', 'alcohol']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo de dialogo: {}\".format(cleaned_content[123]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:18:38,145 : INFO : collecting all words and their counts\n",
      "2020-06-09 11:18:38,147 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-06-09 11:18:38,206 : INFO : PROGRESS: at sentence #10000, processed 33320 words and 30933 word types\n",
      "2020-06-09 11:18:38,258 : INFO : PROGRESS: at sentence #20000, processed 67371 words and 57553 word types\n",
      "2020-06-09 11:18:38,297 : INFO : PROGRESS: at sentence #30000, processed 104569 words and 86217 word types\n",
      "2020-06-09 11:18:38,349 : INFO : PROGRESS: at sentence #40000, processed 138266 words and 109953 word types\n",
      "2020-06-09 11:18:38,390 : INFO : PROGRESS: at sentence #50000, processed 170239 words and 132021 word types\n",
      "2020-06-09 11:18:38,428 : INFO : PROGRESS: at sentence #60000, processed 200105 words and 151621 word types\n",
      "2020-06-09 11:18:38,473 : INFO : PROGRESS: at sentence #70000, processed 233512 words and 173834 word types\n",
      "2020-06-09 11:18:38,524 : INFO : PROGRESS: at sentence #80000, processed 268907 words and 197412 word types\n",
      "2020-06-09 11:18:38,574 : INFO : PROGRESS: at sentence #90000, processed 303348 words and 219942 word types\n",
      "2020-06-09 11:18:38,611 : INFO : PROGRESS: at sentence #100000, processed 337260 words and 241953 word types\n",
      "2020-06-09 11:18:38,658 : INFO : PROGRESS: at sentence #110000, processed 371777 words and 264454 word types\n",
      "2020-06-09 11:18:38,705 : INFO : PROGRESS: at sentence #120000, processed 405287 words and 285577 word types\n",
      "2020-06-09 11:18:38,752 : INFO : PROGRESS: at sentence #130000, processed 438262 words and 304567 word types\n",
      "2020-06-09 11:18:38,761 : INFO : collected 308025 word types from a corpus of 444264 words (unigram + bigrams) and 131853 sentences\n",
      "2020-06-09 11:18:38,761 : INFO : using 308025 counts as vocab in Phrases<0 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(cleaned_content, min_count=100, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {b'disease': 38,\n",
       "             b'magazines': 31,\n",
       "             b'disease_magazines': 1,\n",
       "             b'news': 301,\n",
       "             b'magazines_news': 1,\n",
       "             b'natural': 75,\n",
       "             b'news_natural': 1,\n",
       "             b'bergstrom': 19,\n",
       "             b'talk': 644,\n",
       "             b'touch': 150,\n",
       "             b'talk_touch': 1,\n",
       "             b'lesson': 129,\n",
       "             b'touch_lesson': 1,\n",
       "             b'plan': 223,\n",
       "             b'lesson_plan': 2,\n",
       "             b'teach': 196,\n",
       "             b'plan_teach': 1,\n",
       "             b'life': 1149,\n",
       "             b'worth': 156,\n",
       "             b'life_worth': 3,\n",
       "             b'living': 197,\n",
       "             b'worth_living': 5,\n",
       "             b'polls': 13,\n",
       "             b'recess': 13,\n",
       "             b'polls_recess': 1,\n",
       "             b'decided': 88,\n",
       "             b'recess_decided': 1,\n",
       "             b'final': 102,\n",
       "             b'decided_final': 2,\n",
       "             b'statements': 7,\n",
       "             b'final_statements': 1,\n",
       "             b'martin': 115,\n",
       "             b'statements_martin': 1,\n",
       "             b'left': 555,\n",
       "             b'bart': 3289,\n",
       "             b'victory': 34,\n",
       "             b'party': 408,\n",
       "             b'victory_party': 1,\n",
       "             b'slide': 37,\n",
       "             b'party_slide': 1,\n",
       "             b'bergstrom_bergstrom': 1,\n",
       "             b'hey': 4276,\n",
       "             b'hey_hey': 289,\n",
       "             b'moved': 66,\n",
       "             b'hey_moved': 1,\n",
       "             b'morning': 322,\n",
       "             b'moved_morning': 1,\n",
       "             b'job': 580,\n",
       "             b'morning_job': 1,\n",
       "             b'copernicus': 5,\n",
       "             b'job_copernicus': 1,\n",
       "             b'costume': 53,\n",
       "             b'copernicus_costume': 1,\n",
       "             b'train': 94,\n",
       "             b'capital': 68,\n",
       "             b'train_capital': 1,\n",
       "             b'city': 284,\n",
       "             b'capital_city': 40,\n",
       "             b'traditional': 18,\n",
       "             b'train_traditional': 1,\n",
       "             b'environmentally': 2,\n",
       "             b'traditional_environmentally': 1,\n",
       "             b'sound': 231,\n",
       "             b'environmentally_sound': 1,\n",
       "             b'backbone': 7,\n",
       "             b'country': 226,\n",
       "             b'backbone_country': 1,\n",
       "             b'leland': 1,\n",
       "             b'country_leland': 1,\n",
       "             b'stanford': 6,\n",
       "             b'leland_stanford': 1,\n",
       "             b'drove': 44,\n",
       "             b'stanford_drove': 1,\n",
       "             b'golden': 52,\n",
       "             b'drove_golden': 1,\n",
       "             b'spike': 7,\n",
       "             b'golden_spike': 2,\n",
       "             b'promontory': 1,\n",
       "             b'spike_promontory': 1,\n",
       "             b'touched': 39,\n",
       "             b'vote': 131,\n",
       "             b'hey_vote': 4,\n",
       "             b'votings': 1,\n",
       "             b'vote_votings': 1,\n",
       "             b'geeks': 10,\n",
       "             b'votings_geeks': 1,\n",
       "             b'girls': 274,\n",
       "             b'vote_girls': 1,\n",
       "             b'forgot': 147,\n",
       "             b'sweat': 30,\n",
       "             b'couple': 182,\n",
       "             b'sweat_couple': 1,\n",
       "             b'people': 1527,\n",
       "             b'couple_people': 2,\n",
       "             b'milhouse': 567,\n",
       "             b'people_milhouse': 1,\n",
       "             b'uh': 2492,\n",
       "             b'lewis': 25,\n",
       "             b'voted': 30,\n",
       "             b'bart_vote': 2,\n",
       "             b'yayyyyyyyyyyyyyy': 1,\n",
       "             b'demand': 35,\n",
       "             b'recount': 3,\n",
       "             b'demand_recount': 1,\n",
       "             b'martin_martin': 6,\n",
       "             b'martin_recount': 1,\n",
       "             b'mister': 126,\n",
       "             b'president': 239,\n",
       "             b'mister_president': 1,\n",
       "             b'boarding': 13,\n",
       "             b'track': 56,\n",
       "             b'boarding_track': 1,\n",
       "             b'5': 20,\n",
       "             b'track_5': 1,\n",
       "             b'afternoon': 93,\n",
       "             b'5_afternoon': 1,\n",
       "             b'delight': 9,\n",
       "             b'afternoon_delight': 2,\n",
       "             b'coming': 485,\n",
       "             b'delight_coming': 1,\n",
       "             b'shelbyville': 94,\n",
       "             b'coming_shelbyville': 1,\n",
       "             b'parkville': 1,\n",
       "             b'shelbyville_parkville': 1,\n",
       "             b'and\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6': 1,\n",
       "             b'parkville_and\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6': 1,\n",
       "             b'bergstrom_hey': 1,\n",
       "             b'hey_bergstrom': 1,\n",
       "             b'lisa': 2055,\n",
       "             b'hey_lisa': 28,\n",
       "             b'leave': 533,\n",
       "             b'lisa_life': 2,\n",
       "             b'substitute': 23,\n",
       "             b'life_substitute': 1,\n",
       "             b'teacher': 151,\n",
       "             b'substitute_teacher': 5,\n",
       "             b'fraud': 32,\n",
       "             b'teacher_fraud': 1,\n",
       "             b'wearing': 163,\n",
       "             b'fraud_wearing': 1,\n",
       "             b'gym': 59,\n",
       "             b'wearing_gym': 1,\n",
       "             b'shorts': 49,\n",
       "             b'gym_shorts': 2,\n",
       "             b'tomorrow': 374,\n",
       "             b'shorts_tomorrow': 1,\n",
       "             b'speaking': 63,\n",
       "             b'tomorrow_speaking': 1,\n",
       "             b'french': 91,\n",
       "             b'speaking_french': 2,\n",
       "             b'pretending': 24,\n",
       "             b'french_pretending': 1,\n",
       "             b'band': 128,\n",
       "             b'pretending_band': 1,\n",
       "             b'god': 1236,\n",
       "             b'band_god': 1,\n",
       "             b'true': 397,\n",
       "             b'teachers': 91,\n",
       "             b'true_teachers': 1,\n",
       "             b'lie': 189,\n",
       "             b'projects': 15,\n",
       "             b'lie_projects': 1,\n",
       "             b'projects_capital': 2,\n",
       "             b'middle': 93,\n",
       "             b'class': 316,\n",
       "             b'middle_class': 4,\n",
       "             b'cares': 66,\n",
       "             b'class_cares': 1,\n",
       "             b'abandon': 14,\n",
       "             b'cares_abandon': 1,\n",
       "             b'understand': 297,\n",
       "             b'understand_bergstrom': 2,\n",
       "             b'feel': 820,\n",
       "             b'rely': 5,\n",
       "             b'feel_rely': 1,\n",
       "             b'aboard': 32,\n",
       "             b'guess': 842,\n",
       "             b'mind': 398,\n",
       "             b'guess_mind': 3,\n",
       "             b'mind_train': 1,\n",
       "             b'speeds': 1,\n",
       "             b'train_speeds': 1,\n",
       "             b'speeds_life': 1,\n",
       "             b'goodbye': 246,\n",
       "             b'goodbye_lisa': 10,\n",
       "             b'honey': 604,\n",
       "             b'lisa_honey': 24,\n",
       "             b'read': 413,\n",
       "             b'honey_read': 1,\n",
       "             b'note': 84,\n",
       "             b'read_note': 1,\n",
       "             b'thrown': 24,\n",
       "             b'thrown_party': 2,\n",
       "             b'bash': 15,\n",
       "             b'party_bash': 1,\n",
       "             b'champagne': 35,\n",
       "             b'bash_champagne': 1,\n",
       "             b'musicians': 9,\n",
       "             b'champagne_musicians': 1,\n",
       "             b'holy': 99,\n",
       "             b'musicians_holy': 1,\n",
       "             b'worst': 179,\n",
       "             b'vote_worst': 1,\n",
       "             b'happened': 397,\n",
       "             b'worst_happened': 4,\n",
       "             b'alright': 60,\n",
       "             b'happened_alright': 1,\n",
       "             b'allright': 2,\n",
       "             b'alright_allright': 1,\n",
       "             b'spilled': 11,\n",
       "             b'allright_spilled': 1,\n",
       "             b'milk': 132,\n",
       "             b'spilled_milk': 4,\n",
       "             b'milk_spilled': 2,\n",
       "             b'mopey': 1,\n",
       "             b'milk_mopey': 1,\n",
       "             b'father': 602,\n",
       "             b'lisa_father': 7,\n",
       "             b'bergstrom_left': 1,\n",
       "             b'care': 513,\n",
       "             b'hey_care': 3,\n",
       "             b'care_understand': 1,\n",
       "             b'glad': 243,\n",
       "             b'crying': 66,\n",
       "             b'glad_crying': 1,\n",
       "             b'hate': 391,\n",
       "             b'crying_hate': 1,\n",
       "             b'based': 45,\n",
       "             b'hate_based': 1,\n",
       "             b'emotion': 8,\n",
       "             b'based_emotion': 1,\n",
       "             b'sir': 1191,\n",
       "             b'emotion_sir': 1,\n",
       "             b'baboon': 13,\n",
       "             b'sir_baboon': 1,\n",
       "             b'baboon_baboon': 3,\n",
       "             b'realize': 109,\n",
       "             b'whoa': 577,\n",
       "             b'bound': 22,\n",
       "             b'whoa_bound': 1,\n",
       "             b'day': 1492,\n",
       "             b'bound_day': 1,\n",
       "             b'hear': 664,\n",
       "             b'marge': 2650,\n",
       "             b'hear_marge': 4,\n",
       "             b'called': 394,\n",
       "             b'marge_called': 5,\n",
       "             b'called_baboon': 2,\n",
       "             b'stupidest': 18,\n",
       "             b'baboon_stupidest': 1,\n",
       "             b'ugliest': 7,\n",
       "             b'stupidest_ugliest': 1,\n",
       "             b'smelliest': 2,\n",
       "             b'ugliest_smelliest': 1,\n",
       "             b'ape': 31,\n",
       "             b'smelliest_ape': 1,\n",
       "             b'homer': 4111,\n",
       "             b'allowed': 76,\n",
       "             b'homer_allowed': 1,\n",
       "             b'hurt': 286,\n",
       "             b'allowed_hurt': 1,\n",
       "             b'feelings': 113,\n",
       "             b'hurt_feelings': 15,\n",
       "             b'girl': 602,\n",
       "             b'feelings_girl': 1,\n",
       "             b'upstairs': 35,\n",
       "             b'girl_upstairs': 1,\n",
       "             b'confidence': 20,\n",
       "             b'upstairs_confidence': 1,\n",
       "             b'confidence_father': 1,\n",
       "             b'shaken': 5,\n",
       "             b'father_shaken': 1,\n",
       "             b'shaken_girl': 1,\n",
       "             b'happy': 652,\n",
       "             b'girl_happy': 1,\n",
       "             b'faith': 69,\n",
       "             b'happy_faith': 1,\n",
       "             b'daddy': 296,\n",
       "             b'faith_daddy': 1,\n",
       "             b'hold': 367,\n",
       "             b'lisa_hold': 4,\n",
       "             b'hold_crying': 1,\n",
       "             b'crying_called': 1,\n",
       "             b'called_daddy': 1,\n",
       "             b'daddy_baboon': 1,\n",
       "             b'nuts': 133,\n",
       "             b'forgive': 114,\n",
       "             b'lost': 427,\n",
       "             b'special': 343,\n",
       "             b'lost_special': 2,\n",
       "             b'hurts': 64,\n",
       "             b'special_hurts': 1,\n",
       "             b'lucky': 171,\n",
       "             b'hurts_lucky': 1,\n",
       "             b'lucky_lost': 1,\n",
       "             b'special_special': 6,\n",
       "             b'roof': 65,\n",
       "             b'special_roof': 1,\n",
       "             b'wedding': 185,\n",
       "             b'children': 589,\n",
       "             b'time': 2409,\n",
       "             b'children_time': 8,\n",
       "             b'bed': 291,\n",
       "             b'time_bed': 10,\n",
       "             b'lots': 112,\n",
       "             b'true_lots': 1,\n",
       "             b'lots_special': 1,\n",
       "             b'special_people': 2,\n",
       "             b'people_life': 5,\n",
       "             b'life_lisa': 3,\n",
       "             b'food': 381,\n",
       "             b'lisa_food': 1,\n",
       "             b'real': 712,\n",
       "             b'food_real': 1,\n",
       "             b'guys': 873,\n",
       "             b'real_guys': 1,\n",
       "             b'serving': 17,\n",
       "             b'guys_serving': 1,\n",
       "             b'drinks': 39,\n",
       "             b'serving_drinks': 1,\n",
       "             b'explain': 118,\n",
       "             b'drinks_explain': 1,\n",
       "             b'doll': 68,\n",
       "             b'explain_doll': 1,\n",
       "             b'house': 857,\n",
       "             b'doll_house': 1,\n",
       "             b'monkey': 166,\n",
       "             b'house_monkey': 1,\n",
       "             b'monkey_monkey': 7,\n",
       "             b'yeah': 2979,\n",
       "             b'nails': 19,\n",
       "             b'hold_nails': 1,\n",
       "             b'tail': 38,\n",
       "             b'nails_tail': 1,\n",
       "             b'silly': 79,\n",
       "             b'gimme': 192,\n",
       "             b'banana': 55,\n",
       "             b'gimme_banana': 1,\n",
       "             b'holding': 68,\n",
       "             b'dad': 2498,\n",
       "             b'baboon_dad': 1,\n",
       "             b'matter': 239,\n",
       "             b'hey_matter': 2,\n",
       "             b'son': 993,\n",
       "             b'matter_son': 5,\n",
       "             b'dad_milhouse': 1,\n",
       "             b'milhouse_lewis': 2,\n",
       "             b'lewis_voted': 1,\n",
       "             b'hey_son': 4,\n",
       "             b'money': 1008,\n",
       "             b'son_money': 3,\n",
       "             b'money_class': 1,\n",
       "             b'class_president': 13,\n",
       "             b'extra': 152,\n",
       "             b'guy': 849,\n",
       "             b'martin_guy': 1,\n",
       "             b'neat': 22,\n",
       "             b'guy_neat': 1,\n",
       "             b'throw': 234,\n",
       "             b'neat_throw': 1,\n",
       "             b'ball': 231,\n",
       "             b'throw_ball': 2,\n",
       "             b'series': 69,\n",
       "             b'ball_series': 1,\n",
       "             b'huh': 844,\n",
       "             b'series_huh': 1,\n",
       "             b'baby': 820,\n",
       "             b'bottle': 108,\n",
       "             b'baby_bottle': 2,\n",
       "             b'bottle_huh': 1,\n",
       "             b'motto': 9,\n",
       "             b'huh_motto': 1,\n",
       "             b'monkeyman': 2,\n",
       "             b'hey_monkeyman': 1,\n",
       "             b'holymoly': 1,\n",
       "             b'holymoly_talk': 1,\n",
       "             b'parenting': 18,\n",
       "             b'talk_parenting': 1,\n",
       "             b'sleep': 289,\n",
       "             b'maggie': 515,\n",
       "             b'sleep_maggie': 1,\n",
       "             b'homie': 558,\n",
       "             b'straighten': 20,\n",
       "             b'homie_straighten': 1,\n",
       "             b'marge_bed': 3,\n",
       "             b'biggest': 92,\n",
       "             b'bed_biggest': 1,\n",
       "             b'roll': 170,\n",
       "             b'biggest_roll': 1,\n",
       "             b'roll_life': 2,\n",
       "             b'mmm': 187,\n",
       "             b'hors': 8,\n",
       "             b'mmm_hors': 1,\n",
       "             b'doovers': 2,\n",
       "             b'hors_doovers': 1,\n",
       "             b'promised': 108,\n",
       "             b'homer_promised': 7,\n",
       "             b'eat': 644,\n",
       "             b'promised_eat': 1,\n",
       "             b'eat_lie': 1,\n",
       "             b'homer_thrown': 1,\n",
       "             b'gonna': 2630,\n",
       "             b'party_gonna': 4,\n",
       "             b'pay': 429,\n",
       "             b'gonna_pay': 14,\n",
       "             b'friends': 554,\n",
       "             b'pay_friends': 1,\n",
       "             b'whove': 10,\n",
       "             b'friends_whove': 1,\n",
       "             b'invited': 61,\n",
       "             b'whove_invited': 1,\n",
       "             b'homes': 25,\n",
       "             b'invited_homes': 1,\n",
       "             b'mom': 1395,\n",
       "             b'mom_hear': 1,\n",
       "             b'witty': 14,\n",
       "             b'hear_witty': 2,\n",
       "             b'banter': 1,\n",
       "             b'witty_banter': 1,\n",
       "             b'sophisticated': 17,\n",
       "             b'banter_sophisticated': 1,\n",
       "             b'adults': 46,\n",
       "             b'sophisticated_adults': 1,\n",
       "             b'fun': 626,\n",
       "             b'yeah_fun': 7,\n",
       "             b'fun_bed': 1,\n",
       "             b'hmmm': 248,\n",
       "             b'hmmm_baby': 1,\n",
       "             b'mmmm': 87,\n",
       "             b'baby_mmmm': 1,\n",
       "             b'mmmm_yeah': 1,\n",
       "             b'glasses': 78,\n",
       "             b'gag': 27,\n",
       "             b'ice': 240,\n",
       "             b'gag_ice': 1,\n",
       "             b'cubs': 5,\n",
       "             b'ice_cubs': 1,\n",
       "             b'homer_homer': 92,\n",
       "             b'record': 104,\n",
       "             b'homer_record': 1,\n",
       "             b'names': 153,\n",
       "             b'friends_names': 1,\n",
       "             b'hey_mind': 2,\n",
       "             b'serve': 57,\n",
       "             b'mind_serve': 1,\n",
       "             b'bartender': 27,\n",
       "             b'serve_bartender': 1,\n",
       "             b'phd': 9,\n",
       "             b'bartender_phd': 1,\n",
       "             b'mixology': 1,\n",
       "             b'phd_mixology': 1,\n",
       "             b'college': 146,\n",
       "             b'boy': 1543,\n",
       "             b'college_boy': 6,\n",
       "             b'hey_homer': 165,\n",
       "             b'homer_care': 4,\n",
       "             b'flanders': 525,\n",
       "             b'care_flanders': 1,\n",
       "             b'planters': 1,\n",
       "             b'flanders_planters': 1,\n",
       "             b'punch': 100,\n",
       "             b'planters_punch': 1,\n",
       "             b'paid': 166,\n",
       "             b'hey_flanders': 26,\n",
       "             b'flanders_time': 1,\n",
       "             b'alcohol': 61,\n",
       "             b'time_alcohol': 2,\n",
       "             b'contraire': 3,\n",
       "             b'simpson': 2144,\n",
       "             b'contraire_simpson': 1,\n",
       "             b'shots': 23,\n",
       "             b'simpson_shots': 1,\n",
       "             b'rum': 11,\n",
       "             b'shots_rum': 1,\n",
       "             b'jigger': 2,\n",
       "             b'rum_jigger': 1,\n",
       "             b'bourbon': 12,\n",
       "             b'jigger_bourbon': 1,\n",
       "             b'dabaroo': 1,\n",
       "             b'bourbon_dabaroo': 1,\n",
       "             b'creme': 8,\n",
       "             b'dabaroo_creme': 1,\n",
       "             b'cassis': 1,\n",
       "             b'creme_cassis': 1,\n",
       "             b'flavor': 41,\n",
       "             b'cassis_flavor': 1,\n",
       "             b'warm': 75,\n",
       "             b'sense': 125,\n",
       "             b'warm_sense': 1,\n",
       "             b'wellbeing': 4,\n",
       "             b'sense_wellbeing': 1,\n",
       "             b'ssslurring': 1,\n",
       "             b'wellbeing_ssslurring': 1,\n",
       "             b'shpeech': 1,\n",
       "             b'ssslurring_shpeech': 1,\n",
       "             b'easy': 305,\n",
       "             b'homer_easy': 7,\n",
       "             b'alkyhol': 1,\n",
       "             b'easy_alkyhol': 1,\n",
       "             b'remember': 706,\n",
       "             b'alkyhol_remember': 1,\n",
       "             b'winfields': 4,\n",
       "             b'remember_winfields': 1,\n",
       "             b'winfields_party': 1,\n",
       "             b'threw': 72,\n",
       "             b'party_threw': 2,\n",
       "             b'laundry': 44,\n",
       "             b'threw_laundry': 1,\n",
       "             b'hamper': 7,\n",
       "             b'laundry_hamper': 1,\n",
       "             b'homers': 153,\n",
       "             b'sisterinlaw': 4,\n",
       "             b'homers_sisterinlaw': 1,\n",
       "             b'sisterinlaw_remember': 1,\n",
       "             b'remember_remember': 4,\n",
       "             b'beau': 3,\n",
       "             b'remember_beau': 1,\n",
       "             b'tiful': 1,\n",
       "             b'beau_tiful': 1,\n",
       "             b'ow': 597,\n",
       "             b'ow_hey': 11,\n",
       "             b'jeez': 36,\n",
       "             b'hey_jeez': 1,\n",
       "             b'mace': 7,\n",
       "             b'painful': 24,\n",
       "             b'mace_painful': 1,\n",
       "             b'dr': 212,\n",
       "             b'hibbert': 61,\n",
       "             b'dr_hibbert': 43,\n",
       "             b'enjoying': 46,\n",
       "             b'hibbert_enjoying': 1,\n",
       "             b'enjoying_party': 1,\n",
       "             b'slipped': 25,\n",
       "             b'uh_slipped': 1,\n",
       "             b'novelty': 25,\n",
       "             b'slipped_novelty': 1,\n",
       "             b'novelty_ice': 2,\n",
       "             b'cubes': 16,\n",
       "             b'ice_cubes': 4,\n",
       "             b'fake': 102,\n",
       "             b'cubes_fake': 1,\n",
       "             b'fly': 149,\n",
       "             b'fake_fly': 1,\n",
       "             b'drink': 295,\n",
       "             b'fly_drink': 2,\n",
       "             b'fell': 108,\n",
       "             b'homer_novelty': 1,\n",
       "             b'highly': 41,\n",
       "             b'cubes_highly': 1,\n",
       "             b'toxic': 17,\n",
       "             b'highly_toxic': 2,\n",
       "             b'chemicals': 13,\n",
       "             b'toxic_chemicals': 2,\n",
       "             b'ironically': 11,\n",
       "             b'chemicals_ironically': 1,\n",
       "             b'ironically_real': 1,\n",
       "             b'real_fly': 1,\n",
       "             b'sanitary': 4,\n",
       "             b'fly_sanitary': 1,\n",
       "             b'priceless': 23,\n",
       "             b'cute': 143,\n",
       "             b'bart_cute': 4,\n",
       "             b'funniest': 15,\n",
       "             b'hey_funniest': 1,\n",
       "             b'funniest_guy': 1,\n",
       "             b'king': 197,\n",
       "             b'homer_king': 1,\n",
       "             b'wantin': 5,\n",
       "             b'nerve': 15,\n",
       "             b'wantin_nerve': 1,\n",
       "             b'wife': 456,\n",
       "             b'homer_wife': 6,\n",
       "             b'wife_wife': 2,\n",
       "             b'wife_friends': 1,\n",
       "             b'met': 162,\n",
       "             b'friends_met': 2,\n",
       "             b'hours': 253,\n",
       "             b'met_hours': 1,\n",
       "             b'stink': 70,\n",
       "             b'lousy': 153,\n",
       "             b'stink_lousy': 1,\n",
       "             b'operation': 59,\n",
       "             b'lousy_operation': 1,\n",
       "             b'stinks': 43,\n",
       "             b'operation_stinks': 1,\n",
       "             b'quit': 241,\n",
       "             b'stinks_quit': 1,\n",
       "             b'gee': 130,\n",
       "             b'uh_gee': 4,\n",
       "             b'gee_homer': 13,\n",
       "             b'homer_quit': 5,\n",
       "             b'handful': 12,\n",
       "             b'peanuts': 32,\n",
       "             b'handful_peanuts': 1,\n",
       "             b'maude': 77,\n",
       "             b'peanuts_maude': 1,\n",
       "             b'invitin': 2,\n",
       "             b'hey_invitin': 1,\n",
       "             b'wonderful': 247,\n",
       "             b'invitin_wonderful': 1,\n",
       "             b'wonderful_time': 9,\n",
       "             b'apologize': 72,\n",
       "             b'husband': 292,\n",
       "             b'apologize_husband': 1,\n",
       "             b'live': 525,\n",
       "             b'night': 803,\n",
       "             b'live_night': 3,\n",
       "             b'night_roll': 1,\n",
       "             b'hmmm_dr': 1,\n",
       "             b'hibbert_coming': 1,\n",
       "             b'embarrassed': 26,\n",
       "             b'embarrassed_life': 1,\n",
       "             b'whatd': 81,\n",
       "             b'wherewhere': 1,\n",
       "             b'shhh': 31,\n",
       "             b'kids': 1444,\n",
       "             b'kids_hear': 7,\n",
       "             b'hated': 52,\n",
       "             b'hear_hated': 1,\n",
       "             b'knowing': 37,\n",
       "             b'hated_knowing': 1,\n",
       "             b'parents': 232,\n",
       "             b'knowing_parents': 1,\n",
       "             b'fighting': 92,\n",
       "             b'parents_fighting': 1,\n",
       "             b'car': 669,\n",
       "             b'fighting_car': 1,\n",
       "             b'music': 277,\n",
       "             b'sends': 13,\n",
       "             b'music_sends': 1,\n",
       "             b'chill': 23,\n",
       "             b'sends_chill': 1,\n",
       "             b'spine': 21,\n",
       "             b'chill_spine': 1,\n",
       "             b'homer_remember': 14,\n",
       "             b'acted': 14,\n",
       "             b'remember_acted': 1,\n",
       "             b'acted_party': 1,\n",
       "             b'party_night': 2,\n",
       "             b'wet': 68,\n",
       "             b'clothes': 119,\n",
       "             b'wet_clothes': 1,\n",
       "             b'dry': 64,\n",
       "             b'clothes_dry': 1,\n",
       "             b'martini': 5,\n",
       "             b'dry_martini': 1,\n",
       "             b'lord': 339,\n",
       "             b'lord_fly': 1,\n",
       "             b'glass': 100,\n",
       "             b'slipped_glass': 1,\n",
       "             b'glass_gag': 1,\n",
       "             b'pure': 47,\n",
       "             b'hilarity': 4,\n",
       "             b'pure_hilarity': 1,\n",
       "             b'pure_homer': 1,\n",
       "             b'pronounce': 22,\n",
       "             b'whimsical': 3,\n",
       "             b'pronounce_whimsical': 1,\n",
       "             b'jape': 2,\n",
       "             b'whimsical_jape': 1,\n",
       "             b'season': 72,\n",
       "             b'jape_season': 1,\n",
       "             b'patient': 25,\n",
       "             b'homer_patient': 1,\n",
       "             b'tolerant': 4,\n",
       "             b'patient_tolerant': 1,\n",
       "             b'woman': 366,\n",
       "             b'tolerant_woman': 1,\n",
       "             b'cross': 43,\n",
       "             b'woman_cross': 1,\n",
       "             b'loving': 65,\n",
       "             b'cross_loving': 1,\n",
       "             b'loving_night': 1,\n",
       "             b'night_cross': 1,\n",
       "             b'cross_threw': 1,\n",
       "             b'forget': 421,\n",
       "             b'honey_forget': 1,\n",
       "             b'church': 218,\n",
       "             b'forget_church': 1,\n",
       "             b'stay': 468,\n",
       "             b'church_stay': 1,\n",
       "             b'stay_explain': 1,\n",
       "             b'explain_bart': 1,\n",
       "             b'scarred': 2,\n",
       "             b'bart_scarred': 1,\n",
       "             b'scarred_life': 1,\n",
       "             b'noticed': 56,\n",
       "             b'night_noticed': 1,\n",
       "             b'noticed_daddy': 1,\n",
       "             b'acting': 66,\n",
       "             b'daddy_acting': 1,\n",
       "             b'strange': 55,\n",
       "             b'acting_strange': 2,\n",
       "             b'strange_understand': 1,\n",
       "             b'wasted': 49,\n",
       "             b'understand_wasted': 1,\n",
       "             b'admit': 111,\n",
       "             b'admit_happened': 1,\n",
       "             b'hope': 474,\n",
       "             b'happened_hope': 1,\n",
       "             b'lose': 201,\n",
       "             b'hope_lose': 1,\n",
       "             b'lot': 729,\n",
       "             b'lose_lot': 1,\n",
       "             b'respect': 118,\n",
       "             b'lot_respect': 1,\n",
       "             b'dad_respect': 1,\n",
       "             b'sneak': 41,\n",
       "             b'preview': 7,\n",
       "             b'sneak_preview': 3,\n",
       "             b'weeks': 153,\n",
       "             b'preview_weeks': 1,\n",
       "             b'sermon': 19,\n",
       "             b'weeks_sermon': 1,\n",
       "             b'announcements': 9,\n",
       "             b'sermon_announcements': 1,\n",
       "             b'pamphlets': 6,\n",
       "             b'announcements_pamphlets': 1,\n",
       "             b'pamphlets_church': 1,\n",
       "             b'newsrack': 1,\n",
       "             b'church_newsrack': 1,\n",
       "             b'including': 46,\n",
       "             b'newsrack_including': 1,\n",
       "             b'bible': 98,\n",
       "             b'including_bible': 1,\n",
       "             b'bafflers': 1,\n",
       "             b'bible_bafflers': 1,\n",
       "             b'satans': 18,\n",
       "             b'bafflers_satans': 1,\n",
       "             b'boners': 5,\n",
       "             b'satans_boners': 2,\n",
       "             b'grief': 15,\n",
       "             b'boners_grief': 1,\n",
       "             b'grief_satans': 1,\n",
       "             b'teens': 19,\n",
       "             b'boners_teens': 1,\n",
       "             b'cool': 542,\n",
       "             b'teens_cool': 1,\n",
       "             b'fry': 20,\n",
       "             b'cool_fry': 1,\n",
       "             b'marge_marge': 51,\n",
       "             b'compete': 20,\n",
       "             b'lord_compete': 1,\n",
       "             b'squeaking': 5,\n",
       "             b'compete_squeaking': 1,\n",
       "             b'squeaking_homer': 1,\n",
       "             b'simpsons': 281,\n",
       "             b'homer_simpsons': 27,\n",
       "             b'shoes': 117,\n",
       "             b'simpsons_shoes': 1,\n",
       "             b'shoes_hold': 1,\n",
       "             b'takes': 154,\n",
       "             b'hold_takes': 1,\n",
       "             b'seat': 139,\n",
       "             b'takes_seat': 1,\n",
       "             b'reverend': 103,\n",
       "             b'sit': 250,\n",
       "             b'sit_homer': 4,\n",
       "             b'lovejoy': 61,\n",
       "             b'lovejoy_holding': 1,\n",
       "             b'annual': 42,\n",
       "             b'holding_annual': 1,\n",
       "             b'marriage': 230,\n",
       "             b'annual_marriage': 1,\n",
       "             b'retreat': 12,\n",
       "             b'marriage_retreat': 2,\n",
       "             b'weekend': 93,\n",
       "             b'retreat_weekend': 1,\n",
       "             b'catfish': 12,\n",
       "             b'weekend_catfish': 1,\n",
       "             b'lake': 42,\n",
       "             b'catfish_lake': 4,\n",
       "             b'psychological': 9,\n",
       "             b'lake_psychological': 1,\n",
       "             b'counseling': 15,\n",
       "             b'psychological_counseling': 1,\n",
       "             b'couples': 26,\n",
       "             b'counseling_couples': 1,\n",
       "             b'marriages': 9,\n",
       "             b'couples_marriages': 1,\n",
       "             b'hanging': 51,\n",
       "             b'marriages_hanging': 1,\n",
       "             b'thread': 9,\n",
       "             b'hanging_thread': 4,\n",
       "             b'tuneup': 2,\n",
       "             b'hmmmm': 17,\n",
       "             b'participate': 8,\n",
       "             b'sign': 259,\n",
       "             b'participate_sign': 1,\n",
       "             b'service': 122,\n",
       "             b'sign_service': 1,\n",
       "             b'attending': 8,\n",
       "             b'simpsons_attending': 1,\n",
       "             b'attending_retreat': 1,\n",
       "             b'tempting': 12,\n",
       "             b'tempting_wonderful': 1,\n",
       "             b'idea': 403,\n",
       "             b'wonderful_idea': 5,\n",
       "             b'insane': 61,\n",
       "             b'marge_insane': 2,\n",
       "             b'encounter': 4,\n",
       "             b'marriage_encounter': 1,\n",
       "             b'encounter_weekend': 1,\n",
       "             b'wanna': 615,\n",
       "             b'holding_catfish': 1,\n",
       "             b'lake_catfish': 2,\n",
       "             b'retreat_marriage': 2,\n",
       "             b'marriage_tuneup': 1,\n",
       "             b'fishing': 30,\n",
       "             b'tuneup_fishing': 1,\n",
       "             b'babysitter': 41,\n",
       "             b'simpson_simpson': 8,\n",
       "             b'supposed': 292,\n",
       "             b'sitter': 9,\n",
       "             b'supposed_sitter': 1,\n",
       "             b'short': 116,\n",
       "             b'sitter_short': 1,\n",
       "             b'notice': 71,\n",
       "             b'short_notice': 4,\n",
       "             b'kick': 107,\n",
       "             b'babysitter_kick': 1,\n",
       "             b'teeth': 113,\n",
       "             b'kick_teeth': 2,\n",
       "             b'tone': 18,\n",
       "             b'lady': 365,\n",
       "             b'tone_lady': 1,\n",
       "             b'taste': 110,\n",
       "             b'lady_taste': 1,\n",
       "             b'hand': 298,\n",
       "             b'taste_hand': 1,\n",
       "             b'grampa': 309,\n",
       "             b'dress': 114,\n",
       "             b'wondering': 74,\n",
       "             b'babysit': 14,\n",
       "             b'wondering_babysit': 2,\n",
       "             b'babysit_kids': 2,\n",
       "             b'kids_weekend': 3,\n",
       "             b'desperate': 33,\n",
       "             b'weekend_desperate': 1,\n",
       "             b'resort': 16,\n",
       "             b'resort_grampa': 1,\n",
       "             b'feeb': 2,\n",
       "             b'grampa_feeb': 1,\n",
       "             b'feeb_guy': 1,\n",
       "             b'counted': 3,\n",
       "             b'guy_counted': 1,\n",
       "             b'dagnabit': 5,\n",
       "             b'counted_dagnabit': 1,\n",
       "             b'everyones': 99,\n",
       "             b'dagnabit_everyones': 1,\n",
       "             b'agin': 2,\n",
       "             b'everyones_agin': 1,\n",
       "             b'puttin': 17,\n",
       "             b'marge_puttin': 1,\n",
       "             b'trunk': 25,\n",
       "             b'puttin_trunk': 1,\n",
       "             b'runs': 35,\n",
       "             b'maggie_runs': 1,\n",
       "             b'fever': 39,\n",
       "             b'runs_fever': 1,\n",
       "             b'sticks': 51,\n",
       "             b'fever_sticks': 1,\n",
       "             b'finger': 65,\n",
       "             b'sticks_finger': 1,\n",
       "             b'electrical': 15,\n",
       "             b'finger_electrical': 1,\n",
       "             b'socket': 2,\n",
       "             b'electrical_socket': 1,\n",
       "             b'socket_drinks': 1,\n",
       "             b'pine': 21,\n",
       "             b'drinks_pine': 1,\n",
       "             b'cleanser': 5,\n",
       "             b'pine_cleanser': 1,\n",
       "             b'list': 116,\n",
       "             b'list_read': 1,\n",
       "             b'goodbye_children': 1,\n",
       "             b'behave': 18,\n",
       "             b'children_behave': 2,\n",
       "             b'falls': 24,\n",
       "             b'grampa_falls': 1,\n",
       "             b'bathtub': 16,\n",
       "             b'falls_bathtub': 1,\n",
       "             b'heard': 434,\n",
       "             b'grampa_mom': 2,\n",
       "             b'hurry': 140,\n",
       "             b'mom_hurry': 2,\n",
       "             b'hurry_forgot': 1,\n",
       "             b'forgot_list': 1,\n",
       "             b'list_lisa': 1,\n",
       "             b'uh_huh': 52,\n",
       "             b'uhhuh': 159,\n",
       "             b'huh_uhhuh': 1,\n",
       "             b'uhhuh_allowed': 1,\n",
       "             b'smoke': 109,\n",
       "             b'allowed_smoke': 1,\n",
       "             b'cigars': 14,\n",
       "             b'smoke_cigars': 2,\n",
       "             b'bait': 17,\n",
       "             b'hmmm_bait': 1,\n",
       "             b'gas': 158,\n",
       "             b'whoa_gas': 1,\n",
       "             b'stretch': 24,\n",
       "             b'guess_stretch': 1,\n",
       "             b'legs': 101,\n",
       "             b'stretch_legs': 2,\n",
       "             b'ya': 946,\n",
       "             b'headed': 32,\n",
       "             b'ya_headed': 1,\n",
       "             b'sherman': 18,\n",
       "             b'sherman_ya': 1,\n",
       "             b'wait': 1347,\n",
       "             b'wait_wait': 160,\n",
       "             b'minute': 554,\n",
       "             b'wait_minute': 358,\n",
       "             b'minute_wait': 15,\n",
       "             b'minute_sherman': 1,\n",
       "             b'biggest_catfish': 1,\n",
       "             b'weighs': 7,\n",
       "             b'pounds': 64,\n",
       "             b'weighs_pounds': 1,\n",
       "             b'picture': 223,\n",
       "             b'freakishly': 2,\n",
       "             b'picture_freakishly': 1,\n",
       "             b'gentlemen': 225,\n",
       "             b'hmmmm_gentlemen': 1,\n",
       "             b'catch': 191,\n",
       "             b'gentlemen_catch': 1,\n",
       "             b'catch_sherman': 2,\n",
       "             b'supermarket': 16,\n",
       "             b'video': 104,\n",
       "             b'supermarket_video': 1,\n",
       "             b'store': 280,\n",
       "             b'video_store': 4,\n",
       "             b'grab': 72,\n",
       "             b'store_grab': 1,\n",
       "             b'krusty': 681,\n",
       "             b'grab_krusty': 1,\n",
       "             b'burger': 39,\n",
       "             b'krusty_burger': 10,\n",
       "             b'head': 490,\n",
       "             b'burger_head': 1,\n",
       "             b'arcade': 8,\n",
       "             b'head_arcade': 1,\n",
       "             b'grampas': 37,\n",
       "             b'bart_grampas': 2,\n",
       "             b'kindly': 20,\n",
       "             b'grampas_kindly': 1,\n",
       "             b'trusts': 2,\n",
       "             b'kindly_trusts': 1,\n",
       "             b'advantage': 32,\n",
       "             b'trusts_advantage': 1,\n",
       "             b'lis': 224,\n",
       "             b'crazy': 409,\n",
       "             b'lis_crazy': 1,\n",
       "             b'topsyturvy': 2,\n",
       "             b'crazy_topsyturvy': 1,\n",
       "             b'times': 302,\n",
       "             b'topsyturvy_times': 1,\n",
       "             b'wrong': 628,\n",
       "             b'times_wrong': 1,\n",
       "             b'guts': 40,\n",
       "             b'wrong_guts': 1,\n",
       "             b'telling': 186,\n",
       "             b'guts_telling': 1,\n",
       "             b'bleed': 8,\n",
       "             b'telling_bleed': 1,\n",
       "             b'gramps': 3,\n",
       "             b'bleed_gramps': 1,\n",
       "             b'gramps_dry': 1,\n",
       "             b'reverend_glad': 1,\n",
       "             b'helen': 56,\n",
       "             b'glad_helen': 1,\n",
       "             b'homer_simpson': 465,\n",
       "             b'simpson_marriage': 1,\n",
       "             b'spit': 60,\n",
       "             b'marriage_spit': 1,\n",
       "             b'shine': 39,\n",
       "             b'spit_shine': 1,\n",
       "             b'business': 334,\n",
       "             b'business_flanders': 2,\n",
       "             b'uh_uh': 101,\n",
       "             b'uh_reverend': 1,\n",
       "             b'reverend_gonna': 1,\n",
       "             b'chance': 242,\n",
       "             b'gonna_chance': 2,\n",
       "             b'chance_fishing': 1,\n",
       "             b'afraid': 402,\n",
       "             b'afraid_marriage': 2,\n",
       "             b'reconciled': 2,\n",
       "             b'marriage_reconciled': 1,\n",
       "             b'reconciled_hours': 1,\n",
       "             b'hours_homer': 2,\n",
       "             b'homer_takes': 1,\n",
       "             b'takes_weekend': 1,\n",
       "             b'hooks': 9,\n",
       "             b'bait_hooks': 2,\n",
       "             b'honesty': 14,\n",
       "             b'hooks_honesty': 1,\n",
       "             b'honesty_happy': 1,\n",
       "             b'happy_marriage': 2,\n",
       "             ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:18:43,346 : INFO : source_vocab length 308025\n",
      "2020-06-09 11:18:45,280 : INFO : Phraser built with 14 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:18:49,905 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v = Word2Vec(min_count=10,\n",
    "                      window=4,\n",
    "                      size=200,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count())\n",
    "simpsons_fast = FastText(min_count=10, window=4, size=200, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:18:55,140 : INFO : collecting all words and their counts\n",
      "2020-06-09 11:18:55,142 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-09 11:18:55,248 : INFO : PROGRESS: at sentence #10000, processed 33107 words, keeping 9133 word types\n",
      "2020-06-09 11:18:55,343 : INFO : PROGRESS: at sentence #20000, processed 66850 words, keeping 14341 word types\n",
      "2020-06-09 11:18:55,450 : INFO : PROGRESS: at sentence #30000, processed 103831 words, keeping 19108 word types\n",
      "2020-06-09 11:18:55,546 : INFO : PROGRESS: at sentence #40000, processed 137318 words, keeping 22332 word types\n",
      "2020-06-09 11:18:55,638 : INFO : PROGRESS: at sentence #50000, processed 169139 words, keeping 25361 word types\n",
      "2020-06-09 11:18:55,728 : INFO : PROGRESS: at sentence #60000, processed 198854 words, keeping 27947 word types\n",
      "2020-06-09 11:18:55,824 : INFO : PROGRESS: at sentence #70000, processed 232101 words, keeping 30616 word types\n",
      "2020-06-09 11:18:55,931 : INFO : PROGRESS: at sentence #80000, processed 267347 words, keeping 33331 word types\n",
      "2020-06-09 11:18:56,051 : INFO : PROGRESS: at sentence #90000, processed 301637 words, keeping 35817 word types\n",
      "2020-06-09 11:18:56,170 : INFO : PROGRESS: at sentence #100000, processed 335421 words, keeping 38114 word types\n",
      "2020-06-09 11:18:56,301 : INFO : PROGRESS: at sentence #110000, processed 369802 words, keeping 40606 word types\n",
      "2020-06-09 11:18:56,423 : INFO : PROGRESS: at sentence #120000, processed 403174 words, keeping 42727 word types\n",
      "2020-06-09 11:18:56,541 : INFO : PROGRESS: at sentence #130000, processed 435929 words, keeping 44266 word types\n",
      "2020-06-09 11:18:56,562 : INFO : collected 44538 word types from a corpus of 441894 raw words and 131853 sentences\n",
      "2020-06-09 11:18:56,562 : INFO : Loading a fresh vocabulary\n",
      "2020-06-09 11:18:56,591 : INFO : effective_min_count=10 retains 6210 unique words (13% of original 44538, drops 38328)\n",
      "2020-06-09 11:18:56,591 : INFO : effective_min_count=10 leaves 357945 word corpus (81% of original 441894, drops 83949)\n",
      "2020-06-09 11:18:56,607 : INFO : deleting the raw counts dictionary of 44538 items\n",
      "2020-06-09 11:18:56,608 : INFO : sample=6e-05 downsamples 1254 most-common words\n",
      "2020-06-09 11:18:56,608 : INFO : downsampling leaves estimated 208514 word corpus (58.3% of prior 357945)\n",
      "2020-06-09 11:18:56,618 : INFO : estimated required memory for 6210 words and 200 dimensions: 13041000 bytes\n",
      "2020-06-09 11:18:56,619 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_w2v.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:19:16,645 : INFO : collecting all words and their counts\n",
      "2020-06-09 11:19:16,646 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-09 11:19:16,752 : INFO : PROGRESS: at sentence #10000, processed 33107 words, keeping 9133 word types\n",
      "2020-06-09 11:19:16,850 : INFO : PROGRESS: at sentence #20000, processed 66850 words, keeping 14341 word types\n",
      "2020-06-09 11:19:16,952 : INFO : PROGRESS: at sentence #30000, processed 103831 words, keeping 19108 word types\n",
      "2020-06-09 11:19:17,050 : INFO : PROGRESS: at sentence #40000, processed 137318 words, keeping 22332 word types\n",
      "2020-06-09 11:19:17,149 : INFO : PROGRESS: at sentence #50000, processed 169139 words, keeping 25361 word types\n",
      "2020-06-09 11:19:17,237 : INFO : PROGRESS: at sentence #60000, processed 198854 words, keeping 27947 word types\n",
      "2020-06-09 11:19:17,332 : INFO : PROGRESS: at sentence #70000, processed 232101 words, keeping 30616 word types\n",
      "2020-06-09 11:19:17,434 : INFO : PROGRESS: at sentence #80000, processed 267347 words, keeping 33331 word types\n",
      "2020-06-09 11:19:17,531 : INFO : PROGRESS: at sentence #90000, processed 301637 words, keeping 35817 word types\n",
      "2020-06-09 11:19:17,628 : INFO : PROGRESS: at sentence #100000, processed 335421 words, keeping 38114 word types\n",
      "2020-06-09 11:19:17,725 : INFO : PROGRESS: at sentence #110000, processed 369802 words, keeping 40606 word types\n",
      "2020-06-09 11:19:17,819 : INFO : PROGRESS: at sentence #120000, processed 403174 words, keeping 42727 word types\n",
      "2020-06-09 11:19:17,914 : INFO : PROGRESS: at sentence #130000, processed 435929 words, keeping 44266 word types\n",
      "2020-06-09 11:19:17,931 : INFO : collected 44538 word types from a corpus of 441894 raw words and 131853 sentences\n",
      "2020-06-09 11:19:17,931 : INFO : Loading a fresh vocabulary\n",
      "2020-06-09 11:19:17,946 : INFO : effective_min_count=10 retains 6210 unique words (13% of original 44538, drops 38328)\n",
      "2020-06-09 11:19:17,947 : INFO : effective_min_count=10 leaves 357945 word corpus (81% of original 441894, drops 83949)\n",
      "2020-06-09 11:19:17,961 : INFO : deleting the raw counts dictionary of 44538 items\n",
      "2020-06-09 11:19:17,962 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2020-06-09 11:19:17,963 : INFO : downsampling leaves estimated 338842 word corpus (94.7% of prior 357945)\n",
      "2020-06-09 11:19:18,012 : INFO : estimated required memory for 6210 words, 45558 buckets and 200 dimensions: 50712952 bytes\n",
      "2020-06-09 11:19:18,014 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "simpsons_fast.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:19:28,642 : INFO : training model with 12 workers on 6210 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
      "2020-06-09 11:19:29,666 : INFO : EPOCH 1 - PROGRESS: at 63.35% examples, 129722 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:30,193 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:30,194 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:30,194 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:30,195 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:30,195 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:30,196 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:30,196 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:30,197 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:30,198 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:30,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:30,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:30,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:30,216 : INFO : EPOCH - 1 : training on 441894 raw words (208548 effective words) took 1.6s, 133304 effective words/s\n",
      "2020-06-09 11:19:31,240 : INFO : EPOCH 2 - PROGRESS: at 65.60% examples, 134869 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:31,869 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:31,870 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:31,871 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:31,872 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:31,872 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:31,872 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:31,873 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:31,873 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:31,874 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:31,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:31,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:31,893 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:31,894 : INFO : EPOCH - 2 : training on 441894 raw words (208203 effective words) took 1.7s, 125176 effective words/s\n",
      "2020-06-09 11:19:32,908 : INFO : EPOCH 3 - PROGRESS: at 59.06% examples, 121453 words/s, in_qsize 1, out_qsize 0\n",
      "2020-06-09 11:19:33,593 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:33,594 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:33,595 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:33,595 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:33,596 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:33,597 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:33,597 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:33,597 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:33,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:33,598 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:33,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:33,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:33,612 : INFO : EPOCH - 3 : training on 441894 raw words (208337 effective words) took 1.7s, 121966 effective words/s\n",
      "2020-06-09 11:19:34,642 : INFO : EPOCH 4 - PROGRESS: at 67.88% examples, 138923 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:35,090 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:35,091 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:35,092 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:35,092 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:35,093 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:35,094 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:35,094 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:35,095 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:35,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:35,096 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:35,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:35,112 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:35,112 : INFO : EPOCH - 4 : training on 441894 raw words (208526 effective words) took 1.5s, 140051 effective words/s\n",
      "2020-06-09 11:19:36,152 : INFO : EPOCH 5 - PROGRESS: at 54.87% examples, 109580 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:37,026 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:37,027 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:37,028 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:37,029 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:37,029 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:37,030 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:37,030 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:37,031 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:37,032 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:37,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:37,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:37,053 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:37,054 : INFO : EPOCH - 5 : training on 441894 raw words (208937 effective words) took 1.9s, 108141 effective words/s\n",
      "2020-06-09 11:19:38,096 : INFO : EPOCH 6 - PROGRESS: at 48.24% examples, 95963 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:39,008 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:39,010 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:39,011 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:39,011 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:39,013 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:39,013 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:39,014 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:39,014 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:39,014 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:39,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:39,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:39,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:39,035 : INFO : EPOCH - 6 : training on 441894 raw words (208578 effective words) took 2.0s, 106054 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:19:40,065 : INFO : EPOCH 7 - PROGRESS: at 54.87% examples, 110674 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:40,796 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:40,798 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:40,799 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:40,800 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:40,800 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:40,800 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:40,801 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:40,802 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:40,805 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:40,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:40,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:40,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:40,824 : INFO : EPOCH - 7 : training on 441894 raw words (208606 effective words) took 1.8s, 117329 effective words/s\n",
      "2020-06-09 11:19:41,845 : INFO : EPOCH 8 - PROGRESS: at 54.87% examples, 111683 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:42,660 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:42,663 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:42,664 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:42,665 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:42,665 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:42,666 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:42,667 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:42,668 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:42,668 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:42,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:42,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:42,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:42,679 : INFO : EPOCH - 8 : training on 441894 raw words (208684 effective words) took 1.8s, 113047 effective words/s\n",
      "2020-06-09 11:19:43,699 : INFO : EPOCH 9 - PROGRESS: at 56.93% examples, 116398 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:44,453 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:44,454 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:44,454 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:44,455 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:44,456 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:44,458 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:44,459 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:44,459 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:44,460 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:44,460 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:44,461 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:44,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:44,477 : INFO : EPOCH - 9 : training on 441894 raw words (208346 effective words) took 1.8s, 116600 effective words/s\n",
      "2020-06-09 11:19:45,502 : INFO : EPOCH 10 - PROGRESS: at 59.06% examples, 120290 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:46,200 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:46,201 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:46,202 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:46,202 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:46,203 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:46,203 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:46,203 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:46,204 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:46,204 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:46,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:46,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:46,225 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:46,226 : INFO : EPOCH - 10 : training on 441894 raw words (208352 effective words) took 1.7s, 119781 effective words/s\n",
      "2020-06-09 11:19:47,249 : INFO : EPOCH 11 - PROGRESS: at 59.06% examples, 120516 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:47,874 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:47,875 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:47,875 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:47,876 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:47,877 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:47,877 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:47,878 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:47,878 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:47,879 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:47,880 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:47,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:47,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:47,899 : INFO : EPOCH - 11 : training on 441894 raw words (208597 effective words) took 1.7s, 125331 effective words/s\n",
      "2020-06-09 11:19:48,915 : INFO : EPOCH 12 - PROGRESS: at 54.87% examples, 111837 words/s, in_qsize 1, out_qsize 0\n",
      "2020-06-09 11:19:49,731 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:49,733 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:49,734 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:49,734 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:49,735 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:49,735 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:49,736 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:49,736 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:49,737 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:49,737 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:49,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:49,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:49,758 : INFO : EPOCH - 12 : training on 441894 raw words (208367 effective words) took 1.9s, 112620 effective words/s\n",
      "2020-06-09 11:19:50,790 : INFO : EPOCH 13 - PROGRESS: at 59.06% examples, 120083 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:51,505 : INFO : worker thread finished; awaiting finish of 11 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:19:51,506 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:51,507 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:51,508 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:51,508 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:51,509 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:51,510 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:51,512 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:51,512 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:51,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:51,514 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:51,533 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:51,534 : INFO : EPOCH - 13 : training on 441894 raw words (208621 effective words) took 1.8s, 118411 effective words/s\n",
      "2020-06-09 11:19:52,557 : INFO : EPOCH 14 - PROGRESS: at 61.21% examples, 125144 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:53,185 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:53,186 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:53,187 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:53,188 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:53,188 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:53,188 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:53,189 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:53,189 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:53,190 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:53,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:53,191 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:53,211 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:53,211 : INFO : EPOCH - 14 : training on 441894 raw words (208304 effective words) took 1.7s, 124864 effective words/s\n",
      "2020-06-09 11:19:54,236 : INFO : EPOCH 15 - PROGRESS: at 56.93% examples, 115912 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:19:54,903 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:19:54,904 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:19:54,905 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:19:54,905 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:19:54,906 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:19:54,907 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:19:54,908 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:19:54,908 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:19:54,909 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:19:54,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:19:54,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:19:54,922 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:19:54,923 : INFO : EPOCH - 15 : training on 441894 raw words (208754 effective words) took 1.7s, 122629 effective words/s\n",
      "2020-06-09 11:19:54,923 : INFO : training on a 6628410 raw words (3127760 effective words) took 26.3s, 119015 effective words/s\n",
      "2020-06-09 11:19:54,924 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.44 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "simpsons_w2v.train(sentences, total_examples=simpsons_w2v.corpus_count, epochs=15, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "simpsons_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:20:07,723 : INFO : training model with 12 workers on 6210 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2020-06-09 11:20:08,746 : INFO : EPOCH 1 - PROGRESS: at 67.88% examples, 226955 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:09,237 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:09,237 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:09,238 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:09,238 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:09,238 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:09,239 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:09,239 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:09,239 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:09,239 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:09,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:09,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:09,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:09,277 : INFO : EPOCH - 1 : training on 441894 raw words (338776 effective words) took 1.5s, 219376 effective words/s\n",
      "2020-06-09 11:20:10,303 : INFO : EPOCH 2 - PROGRESS: at 65.60% examples, 218766 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:10,766 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:10,767 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:10,767 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:10,768 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:10,768 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:10,768 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:10,769 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:10,769 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:10,770 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:10,770 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:10,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:10,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:10,805 : INFO : EPOCH - 2 : training on 441894 raw words (338819 effective words) took 1.5s, 223227 effective words/s\n",
      "2020-06-09 11:20:11,812 : INFO : EPOCH 3 - PROGRESS: at 63.35% examples, 214554 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:12,270 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:12,271 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:12,271 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:12,272 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:12,272 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:12,272 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:12,273 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:12,274 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:12,274 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:12,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:12,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:12,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:12,304 : INFO : EPOCH - 3 : training on 441894 raw words (338926 effective words) took 1.5s, 227211 effective words/s\n",
      "2020-06-09 11:20:13,342 : INFO : EPOCH 4 - PROGRESS: at 70.13% examples, 230719 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:13,741 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:13,744 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:13,744 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:13,744 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:13,745 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:13,745 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:13,745 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:13,746 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:13,747 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:13,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:13,750 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:13,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:13,779 : INFO : EPOCH - 4 : training on 441894 raw words (338970 effective words) took 1.5s, 231011 effective words/s\n",
      "2020-06-09 11:20:14,810 : INFO : EPOCH 5 - PROGRESS: at 70.13% examples, 232778 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:15,223 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:15,225 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:15,225 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:15,225 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:15,226 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:15,226 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:15,227 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:15,227 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:15,228 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:15,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:15,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:15,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:15,261 : INFO : EPOCH - 5 : training on 441894 raw words (338834 effective words) took 1.5s, 230084 effective words/s\n",
      "2020-06-09 11:20:16,297 : INFO : EPOCH 6 - PROGRESS: at 63.35% examples, 208773 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:16,756 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:16,757 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:16,757 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:16,758 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:16,758 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:16,759 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:16,761 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:16,761 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:16,762 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:16,762 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:16,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:16,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:16,795 : INFO : EPOCH - 6 : training on 441894 raw words (338714 effective words) took 1.5s, 222039 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:20:17,826 : INFO : EPOCH 7 - PROGRESS: at 65.60% examples, 217570 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:18,250 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:18,253 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:18,254 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:18,254 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:18,255 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:18,255 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:18,255 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:18,256 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:18,256 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:18,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:18,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:18,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:18,286 : INFO : EPOCH - 7 : training on 441894 raw words (338944 effective words) took 1.5s, 228675 effective words/s\n",
      "2020-06-09 11:20:19,318 : INFO : EPOCH 8 - PROGRESS: at 67.88% examples, 224607 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:19,785 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:19,788 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:19,789 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:19,791 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:19,791 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:19,792 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:19,792 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:19,792 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:19,793 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:19,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:19,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:19,825 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:19,825 : INFO : EPOCH - 8 : training on 441894 raw words (339000 effective words) took 1.5s, 221323 effective words/s\n",
      "2020-06-09 11:20:20,839 : INFO : EPOCH 9 - PROGRESS: at 65.60% examples, 221267 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:21,256 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:21,258 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:21,261 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:21,263 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:21,263 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:21,264 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:21,264 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:21,265 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:21,265 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:21,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:21,269 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:21,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:21,297 : INFO : EPOCH - 9 : training on 441894 raw words (339087 effective words) took 1.5s, 231786 effective words/s\n",
      "2020-06-09 11:20:22,335 : INFO : EPOCH 10 - PROGRESS: at 67.88% examples, 223826 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:22,730 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:22,732 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:22,732 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:22,733 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:22,733 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:22,733 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:22,733 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:22,734 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:22,734 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:22,742 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:22,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:22,768 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:22,769 : INFO : EPOCH - 10 : training on 441894 raw words (338860 effective words) took 1.5s, 231861 effective words/s\n",
      "2020-06-09 11:20:23,797 : INFO : EPOCH 11 - PROGRESS: at 67.88% examples, 225505 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:24,250 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:24,253 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:24,254 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:24,255 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:24,256 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:24,256 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:24,256 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:24,257 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:24,257 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:24,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:24,267 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:24,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:24,289 : INFO : EPOCH - 11 : training on 441894 raw words (338762 effective words) took 1.5s, 224158 effective words/s\n",
      "2020-06-09 11:20:25,325 : INFO : EPOCH 12 - PROGRESS: at 65.60% examples, 216101 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:25,739 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:25,743 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:25,744 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:25,745 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:25,745 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:25,746 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:25,747 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:25,748 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:25,748 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:25,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:25,753 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:25,777 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:25,777 : INFO : EPOCH - 12 : training on 441894 raw words (338733 effective words) took 1.5s, 228827 effective words/s\n",
      "2020-06-09 11:20:26,826 : INFO : EPOCH 13 - PROGRESS: at 65.60% examples, 213592 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:27,261 : INFO : worker thread finished; awaiting finish of 11 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 11:20:27,265 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:27,265 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:27,266 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:27,267 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:27,267 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:27,267 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:27,268 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:27,268 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:27,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:27,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:27,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:27,301 : INFO : EPOCH - 13 : training on 441894 raw words (338942 effective words) took 1.5s, 223582 effective words/s\n",
      "2020-06-09 11:20:28,312 : INFO : EPOCH 14 - PROGRESS: at 63.35% examples, 213795 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:28,834 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:28,836 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:28,836 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:28,836 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:28,837 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:28,837 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:28,837 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:28,838 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:28,838 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:28,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:28,846 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:28,872 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:28,872 : INFO : EPOCH - 14 : training on 441894 raw words (338682 effective words) took 1.6s, 216620 effective words/s\n",
      "2020-06-09 11:20:29,908 : INFO : EPOCH 15 - PROGRESS: at 70.13% examples, 231137 words/s, in_qsize 0, out_qsize 0\n",
      "2020-06-09 11:20:30,295 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-06-09 11:20:30,296 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-06-09 11:20:30,297 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-06-09 11:20:30,297 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-06-09 11:20:30,297 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-06-09 11:20:30,298 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-06-09 11:20:30,299 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-06-09 11:20:30,299 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-06-09 11:20:30,300 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-06-09 11:20:30,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-09 11:20:30,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-09 11:20:30,326 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-09 11:20:30,327 : INFO : EPOCH - 15 : training on 441894 raw words (338727 effective words) took 1.4s, 234167 effective words/s\n",
      "2020-06-09 11:20:30,327 : INFO : training on a 6628410 raw words (5082776 effective words) took 22.6s, 224867 effective words/s\n",
      "2020-06-09 11:20:30,484 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-06-09 11:20:30,486 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.38 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "simpsons_fast.train(sentences, total_examples=simpsons_fast.corpus_count, epochs=15, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "simpsons_fast.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oos6kUF5tYJr"
   },
   "source": [
    "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. Cúal es la diferencia entre ambos resultados? Por qué ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escogería uno vs el otro? (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wqX03jStYJr"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3ovjW6WtYJr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bart', 0.8497897386550903),\n",
       " ('barts', 0.7809084057807922),\n",
       " ('valentine', 0.7791230082511902),\n",
       " ('daughter', 0.7786688804626465),\n",
       " ('recital', 0.7718645334243774),\n",
       " ('eightyearold', 0.7676244378089905),\n",
       " ('father', 0.7663049697875977),\n",
       " ('isaac', 0.7614347338676453),\n",
       " ('surprised', 0.7569807767868042),\n",
       " ('hoover', 0.7524691820144653)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"lisa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisas', 0.9042256474494934),\n",
       " ('bart', 0.8413902521133423),\n",
       " ('lis', 0.7958658933639526),\n",
       " ('bartholomew', 0.7610986232757568),\n",
       " ('barts', 0.7133684158325195),\n",
       " ('bartdude', 0.7113784551620483),\n",
       " ('abe', 0.710239052772522),\n",
       " ('fart', 0.7087079882621765),\n",
       " ('teacher', 0.7049365043640137),\n",
       " ('maggie', 0.6828056573867798)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast.wv.most_similar(positive=[\"lisa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barts', 0.854486882686615),\n",
       " ('lisa', 0.8497897386550903),\n",
       " ('guess', 0.8319638967514038),\n",
       " ('mom', 0.8307259678840637),\n",
       " ('hoover', 0.8234147429466248),\n",
       " ('parents', 0.8219690322875977),\n",
       " ('moms', 0.8202052712440491),\n",
       " ('eightyearold', 0.8174091577529907),\n",
       " ('talk', 0.8147856593132019),\n",
       " ('concentrate', 0.8126441836357117)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.8413901925086975),\n",
       " ('bartdude', 0.832033634185791),\n",
       " ('bartholomew', 0.824783444404602),\n",
       " ('barts', 0.8202764391899109),\n",
       " ('lisas', 0.7563924789428711),\n",
       " ('fart', 0.7027862668037415),\n",
       " ('mozart', 0.6964912414550781),\n",
       " ('barely', 0.6910310983657837),\n",
       " ('bartman', 0.6803971529006958),\n",
       " ('maggie', 0.6550633907318115)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sweetheart', 0.8435172438621521),\n",
       " ('witty', 0.8428977727890015),\n",
       " ('callin', 0.8417885303497314),\n",
       " ('jessica', 0.8383934497833252),\n",
       " ('apus', 0.8243468999862671),\n",
       " ('marge', 0.824333906173706),\n",
       " ('depressed', 0.8224407434463501),\n",
       " ('midge', 0.820472002029419),\n",
       " ('gee', 0.8198744058609009),\n",
       " ('sweetie', 0.8197618126869202)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homey', 0.8661185503005981),\n",
       " ('homers', 0.827944278717041),\n",
       " ('homemade', 0.75493323802948),\n",
       " ('homes', 0.6863672137260437),\n",
       " ('howdy', 0.6813557147979736),\n",
       " ('howd', 0.6812344789505005),\n",
       " ('hows', 0.6313207149505615),\n",
       " ('homework', 0.6242327094078064),\n",
       " ('homie', 0.6138030886650085),\n",
       " ('hohi', 0.6122370958328247)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homie', 0.8821967840194702),\n",
       " ('husband', 0.8758811950683594),\n",
       " ('glad', 0.8540130853652954),\n",
       " ('sham', 0.8494093418121338),\n",
       " ('sweetie', 0.8487114906311035),\n",
       " ('manjula', 0.8447753190994263),\n",
       " ('saving', 0.8315244913101196),\n",
       " ('arranged', 0.8311783075332642),\n",
       " ('wonderful', 0.8252994418144226),\n",
       " ('homer', 0.824333906173706)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marges', 0.889202356338501),\n",
       " ('margie', 0.819656252861023),\n",
       " ('marco', 0.7654863595962524),\n",
       " ('husband', 0.7553811073303223),\n",
       " ('marvin', 0.7169325947761536),\n",
       " ('abe', 0.7127425670623779),\n",
       " ('husbands', 0.7122534513473511),\n",
       " ('marjorie', 0.7066235542297363),\n",
       " ('marriage', 0.7009074687957764),\n",
       " ('mardi', 0.6831939220428467)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'liisa' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-14532a463531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimpsons_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"liisa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CC6205-NLP/.venv/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CC6205-NLP/.venv/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'liisa' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "simpsons_w2v.wv.most_similar(positive=[\"liisa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.703479528427124),\n",
       " ('fart', 0.7011858224868774),\n",
       " ('lisas', 0.6662884950637817),\n",
       " ('grade', 0.6516871452331543),\n",
       " ('grades', 0.636194109916687),\n",
       " ('bartholomew', 0.6284066438674927),\n",
       " ('grammar', 0.6272149085998535),\n",
       " ('mozart', 0.6252517700195312),\n",
       " ('art', 0.6215193271636963),\n",
       " ('graders', 0.6212480068206787)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_fast.wv.most_similar(positive=[\"liisa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que algunas palabras se asocian en ambos modelos, como \"bart\" con \"lisa\" pero otras no. Por ejemplo en w2v \"homer\" queda asociado con \"marge\" como se podría esperar mientras que en fasttext no aparecen. También se ve que en fasttext \"homer\" queda asociado con otras palabras que empiezan con \"ho\" y parecen menos significativas que las palabras que asocia el modelo w2v. \n",
    "\n",
    "Al usar una palabra fuera de vocabulario como \"liisa\" se ve que w2v falla como es de esperar mientras que fasttext logra asociar la palabra con \"lisa\" pues este modelo si soporta crear vectores para palabras fuera de vocabulario.\n",
    "\n",
    "W2V pareciera hacer asociaciones más significativas por lo que sería más util si el corpus en el que se entrenó es similar al que se quiere trabajar. En cambio, si son distintos o exiten palabras mal escritas puede ser mejor fasttext por su capacidad para usar palabras fuera del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAbldwNFtYJu"
   },
   "source": [
    "### Parte 4: Aplicar embeddings para clasificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5DlKSestYJu"
   },
   "source": [
    "Ahora utilizaremos estos embeddings para clasificar palabras basadas en su polaridad (positivas o negativas). Para esto ocuparemos el lexicón AFINN incluido en la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCDFYm_ytYJu"
   },
   "outputs": [],
   "source": [
    "AFINN = 'AFINN_full.csv'\n",
    "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-icASpvytYJw"
   },
   "source": [
    "Hint: Para w2v son esperables KeyErrors, para eso pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar), para luego aplicar los embeddings en toda la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXw9CGmgtYJx"
   },
   "outputs": [],
   "source": [
    "def try_apply(model,word):\n",
    "    try:\n",
    "        aux = model[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        #logger.error('Word {} not in dictionary'.format(word))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tops</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groan</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perfects</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spammer</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saluting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>mediocrity</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>bold</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>hating</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>unfavorable</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>scapegoats</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3382 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1\n",
       "0            tops  1\n",
       "1           groan -1\n",
       "2        perfects  1\n",
       "3         spammer -1\n",
       "4        saluting  1\n",
       "...           ... ..\n",
       "3377   mediocrity -1\n",
       "3378         bold  1\n",
       "3379       hating -1\n",
       "3380  unfavorable -1\n",
       "3381   scapegoats -1\n",
       "\n",
       "[3382 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8d8qtIgtYJy"
   },
   "source": [
    "**Pregunta 1**: Una vez que tengan un dataframe del estilo [embedding, sentimiento] para ambos modelos, separarlo utilizando la siguiente función, donde X es su columna de embeddings e y es la columna de los valores. (3 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_frb5aDatYJz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Dw6KBAftYJ1"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmrrkpaStYJ1"
   },
   "outputs": [],
   "source": [
    "def row_to_embedding(model, word, polarity):\n",
    "    if try_apply(model, word):\n",
    "        embedding = model[word]\n",
    "        return (word, embedding, polarity)\n",
    "    else:\n",
    "        return (word, None, polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/CC6205-NLP/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "list_w2v = [row_to_embedding(simpsons_w2v, word, pol) for word, pol in zip(df_afinn[0], df_afinn[1])]\n",
    "list_ft = [row_to_embedding(simpsons_fast, word, pol) for word, pol in zip(df_afinn[0], df_afinn[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2v = pd.DataFrame(list_w2v, columns=['word', 'embedding', 'polarity'])\n",
    "df_ft = pd.DataFrame(list_ft, columns=['word', 'embedding', 'polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word            0\n",
       "embedding    2458\n",
       "polarity        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word         0\n",
       "embedding    0\n",
       "polarity     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2v = df_w2v.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tops</td>\n",
       "      <td>[0.053487565, 0.07030328, 0.0112742055, 0.0615...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wasting</td>\n",
       "      <td>[0.0502225, 0.069726, 0.008515978, 0.06514089,...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>complaining</td>\n",
       "      <td>[0.053986385, 0.064848535, 0.013219653, 0.0657...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>super</td>\n",
       "      <td>[0.044522855, 0.078882284, 0.024905289, 0.0564...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>loving</td>\n",
       "      <td>[0.0643605, 0.05892639, 0.005540873, 0.0539736...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>fool</td>\n",
       "      <td>[0.051176768, 0.06571245, 0.01240671, 0.061800...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>attractive</td>\n",
       "      <td>[0.05422244, 0.068818256, 0.009481963, 0.06443...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>admire</td>\n",
       "      <td>[0.053098682, 0.06978595, 0.0099497, 0.0659742...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>growth</td>\n",
       "      <td>[0.05205107, 0.07010611, 0.011599099, 0.064914...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>bold</td>\n",
       "      <td>[0.05300219, 0.07070257, 0.010371038, 0.065875...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                          embedding  polarity\n",
       "0            tops  [0.053487565, 0.07030328, 0.0112742055, 0.0615...         1\n",
       "7         wasting  [0.0502225, 0.069726, 0.008515978, 0.06514089,...        -1\n",
       "9     complaining  [0.053986385, 0.064848535, 0.013219653, 0.0657...        -1\n",
       "10          super  [0.044522855, 0.078882284, 0.024905289, 0.0564...         1\n",
       "11         loving  [0.0643605, 0.05892639, 0.005540873, 0.0539736...         1\n",
       "...           ...                                                ...       ...\n",
       "3371         fool  [0.051176768, 0.06571245, 0.01240671, 0.061800...        -1\n",
       "3373   attractive  [0.05422244, 0.068818256, 0.009481963, 0.06443...         1\n",
       "3374       admire  [0.053098682, 0.06978595, 0.0099497, 0.0659742...         1\n",
       "3376       growth  [0.05205107, 0.07010611, 0.011599099, 0.064914...         1\n",
       "3378         bold  [0.05300219, 0.07070257, 0.010371038, 0.065875...         1\n",
       "\n",
       "[924 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tops</td>\n",
       "      <td>[0.010812045, 0.034981523, 0.00586286, 0.04271...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groan</td>\n",
       "      <td>[0.012154241, -0.017991234, -0.026018014, -0.0...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perfects</td>\n",
       "      <td>[0.0055362857, 0.007097093, -0.024152149, -0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spammer</td>\n",
       "      <td>[0.0008206969, 0.030699976, -0.03311503, 0.003...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saluting</td>\n",
       "      <td>[-0.006375762, 0.014084062, -0.011281406, 0.01...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>mediocrity</td>\n",
       "      <td>[0.009526177, 0.00643165, -0.018854985, -0.015...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>bold</td>\n",
       "      <td>[0.040844135, -0.0075654914, 0.000639621, 0.01...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>hating</td>\n",
       "      <td>[-0.01964631, 0.025315396, 0.028891074, -0.023...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>unfavorable</td>\n",
       "      <td>[0.01572571, -0.014215951, -0.011482378, -0.00...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>scapegoats</td>\n",
       "      <td>[-0.0039360174, -0.0054981816, -0.024219746, -...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3382 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                          embedding  polarity\n",
       "0            tops  [0.010812045, 0.034981523, 0.00586286, 0.04271...         1\n",
       "1           groan  [0.012154241, -0.017991234, -0.026018014, -0.0...        -1\n",
       "2        perfects  [0.0055362857, 0.007097093, -0.024152149, -0.0...         1\n",
       "3         spammer  [0.0008206969, 0.030699976, -0.03311503, 0.003...        -1\n",
       "4        saluting  [-0.006375762, 0.014084062, -0.011281406, 0.01...         1\n",
       "...           ...                                                ...       ...\n",
       "3377   mediocrity  [0.009526177, 0.00643165, -0.018854985, -0.015...        -1\n",
       "3378         bold  [0.040844135, -0.0075654914, 0.000639621, 0.01...         1\n",
       "3379       hating  [-0.01964631, 0.025315396, 0.028891074, -0.023...        -1\n",
       "3380  unfavorable  [0.01572571, -0.014215951, -0.011482378, -0.00...        -1\n",
       "3381   scapegoats  [-0.0039360174, -0.0054981816, -0.024219746, -...        -1\n",
       "\n",
       "[3382 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = np.array(list(df_w2v.embedding))\n",
    "X_ft  = np.array(list(df_ft.embedding))\n",
    "\n",
    "y_w2v = df_w2v.polarity.values\n",
    "y_ft = df_ft.polarity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(X_w2v, y_w2v, random_state=0, test_size=0.1, stratify=y_w2v)\n",
    "X_ft_train, X_ft_test, y_ft_train, y_ft_test = train_test_split(X_ft, y_ft, random_state=0, test_size=0.1, stratify=y_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 481, 1: 350})\n",
      "Counter({-1: 1985, 1: 1058})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count_w2v = Counter(y_w2v_train)\n",
    "count_ft = Counter(y_ft_train)\n",
    "print(count_w2v)\n",
    "print(count_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u33__LaNtYJ3"
   },
   "source": [
    "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? (3 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dP5zYH3qtYJ3"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_w2v = linear_model.LogisticRegression(class_weight='balanced')\n",
    "LR_ft = linear_model.LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_w2v.fit(X_w2v_train,y_w2v_train)\n",
    "LR_ft.fit(X_ft_train,y_ft_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reporte Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report Simpsons Word2Vec :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.76      0.67        54\n",
      "           1       0.48      0.31      0.38        39\n",
      "\n",
      "    accuracy                           0.57        93\n",
      "   macro avg       0.54      0.53      0.52        93\n",
      "weighted avg       0.55      0.57      0.55        93\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6d7ec17b70>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "labels = LR_w2v.classes_\n",
    "predicted_labels = LR_w2v.predict(X_w2v_test)\n",
    "\n",
    "print('\\nClassification Report Simpsons Word2Vec :\\n')\n",
    "print(\n",
    "    classification_report(y_w2v_test,\n",
    "                          predicted_labels,\n",
    "                          labels=labels))\n",
    "plot_confusion_matrix(LR_w2v, X_w2v_test, y_w2v_test, display_labels=labels,\n",
    "                                 normalize='true', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reporte FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report simpsons FastText :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.74      0.76       221\n",
      "           1       0.56      0.62      0.59       118\n",
      "\n",
      "    accuracy                           0.70       339\n",
      "   macro avg       0.67      0.68      0.68       339\n",
      "weighted avg       0.71      0.70      0.70       339\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6d7e822048>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = LR_ft.classes_\n",
    "predicted_labels = LR_ft.predict(X_ft_test)\n",
    "print('\\nClassification Report simpsons FastText :\\n')\n",
    "print(\n",
    "    classification_report(y_ft_test,\n",
    "                          predicted_labels,\n",
    "                          labels=labels))\n",
    "plot_confusion_matrix(LR_ft, X_ft_test, y_ft_test, display_labels=labels,\n",
    "                                 normalize='true', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en los resultados anteriores, el modelo entrenado con embeddings del modelo de tipo FastText presenta mejores resultados en accuracy, precision, recall y f1-score, solo bajando levemente el racall para la clase 1. Estos resultados se pueden deber a que el modelo w2v no puede procesar palabras fuera del vocabulario, dejando fuera mas de dos tercios del dataset (2458), teniendo así menos ejemplos para entrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos resultados se podrían mejorar usando modelos de embedding entrenados sobre corpus más grandes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOt6N4nftYJ4"
   },
   "source": [
    "# Bonus: 2 puntos en cualquier pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnsV0J6StYJ4"
   },
   "source": [
    "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim (1 punto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lYudsa_tYJ4"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd4psA7LtYJ5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 12:30:09,701 : INFO : Creating /home/javier/gensim-data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-09 12:31:16,103 : INFO : glove-wiki-gigaword-200 downloaded\n",
      "2020-06-09 12:31:16,107 : INFO : loading projection weights from /home/javier/gensim-data/glove-wiki-gigaword-200/glove-wiki-gigaword-200.gz\n",
      "2020-06-09 12:32:13,842 : INFO : loaded (400000, 200) matrix from /home/javier/gensim-data/glove-wiki-gigaword-200/glove-wiki-gigaword-200.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model_glove = api.load(\"glove-wiki-gigaword-200\")  # load glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_glove = [row_to_embedding(model_glove, word, pol) for word, pol in zip(df_afinn[0], df_afinn[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glove = pd.DataFrame(list_glove, columns=['word', 'embedding', 'polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word           0\n",
       "embedding    157\n",
       "polarity       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glove.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glove = df_glove.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tops</td>\n",
       "      <td>[0.11806, -0.66712, -0.33831, 0.36472, 0.58755...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groan</td>\n",
       "      <td>[0.69201, -0.16756, 0.2308, 0.17145, 0.0035188...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perfects</td>\n",
       "      <td>[0.54694, -0.66065, 0.02605, 0.4822, 0.21616, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spammer</td>\n",
       "      <td>[0.2717, -0.038178, 0.43993, -0.1564, 0.42552,...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saluting</td>\n",
       "      <td>[-0.062956, 0.33258, 0.081622, 0.59953, -0.513...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>mediocrity</td>\n",
       "      <td>[-0.21566, 0.023621, -0.072917, 0.82215, -0.10...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>bold</td>\n",
       "      <td>[0.10807, 0.55807, -0.44751, -0.24898, 0.61079...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>hating</td>\n",
       "      <td>[-0.15068, 0.24947, -0.034639, 0.77985, -0.507...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>unfavorable</td>\n",
       "      <td>[0.28808, 0.44418, 0.14177, 0.34296, -0.002689...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>scapegoats</td>\n",
       "      <td>[0.50561, 0.31816, -0.44091, 0.096828, -0.0136...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                          embedding  polarity\n",
       "0            tops  [0.11806, -0.66712, -0.33831, 0.36472, 0.58755...         1\n",
       "1           groan  [0.69201, -0.16756, 0.2308, 0.17145, 0.0035188...        -1\n",
       "2        perfects  [0.54694, -0.66065, 0.02605, 0.4822, 0.21616, ...         1\n",
       "3         spammer  [0.2717, -0.038178, 0.43993, -0.1564, 0.42552,...        -1\n",
       "4        saluting  [-0.062956, 0.33258, 0.081622, 0.59953, -0.513...         1\n",
       "...           ...                                                ...       ...\n",
       "3377   mediocrity  [-0.21566, 0.023621, -0.072917, 0.82215, -0.10...        -1\n",
       "3378         bold  [0.10807, 0.55807, -0.44751, -0.24898, 0.61079...         1\n",
       "3379       hating  [-0.15068, 0.24947, -0.034639, 0.77985, -0.507...        -1\n",
       "3380  unfavorable  [0.28808, 0.44418, 0.14177, 0.34296, -0.002689...        -1\n",
       "3381   scapegoats  [0.50561, 0.31816, -0.44091, 0.096828, -0.0136...        -1\n",
       "\n",
       "[3225 rows x 3 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_glove = np.array(list(df_glove.embedding))\n",
    "\n",
    "y_glove = df_glove.polarity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_glove_train, X_glove_test, y_glove_train, y_glove_test = train_test_split(X_glove, y_glove, random_state=0, test_size=0.1, stratify=y_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_glove = linear_model.LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_glove.fit(X_glove_train,y_glove_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report glove-wiki-gigaword-200:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.92      0.93       211\n",
      "           1       0.86      0.88      0.87       112\n",
      "\n",
      "    accuracy                           0.91       323\n",
      "   macro avg       0.90      0.90      0.90       323\n",
      "weighted avg       0.91      0.91      0.91       323\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6d62e8fcf8>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "labels = LR_glove.classes_\n",
    "predicted_labels = LR_glove.predict(X_glove_test)\n",
    "\n",
    "print('\\nClassification Report glove-wiki-gigaword-200:\\n')\n",
    "print(\n",
    "    classification_report(y_glove_test,\n",
    "                          predicted_labels,\n",
    "                          labels=labels))\n",
    "plot_confusion_matrix(LR_glove, X_glove_test, y_glove_test, display_labels=labels,\n",
    "                                 normalize='true', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrZxZZfutYJ7"
   },
   "source": [
    "**Pregunta 2**: Utilizar wefe para ver si el modelo w2v entrenado con los dialogos de los Simpson tienen algun bias entre los personajes hombres y la cerveza (1 punto):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Subk47EatYJ7"
   },
   "source": [
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgThotyEtYJ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Minitarea3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2obO44rRIDIm"
   },
   "source": [
    "# Minitarea 2\n",
    "\n",
    "Nombre: Javier Urrutia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JstKx3TiKcIp"
   },
   "source": [
    "---------------------------\n",
    "## Language Models (3 pts)\n",
    "Estas preguntas son teóricas y deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas. El material del curso y los videos deberian ser suficientes para que puedan responder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hwW-07MrRpt"
   },
   "source": [
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Asuma que tiene un modelo de lenguaje de trigramas (simple) entrenado sobre un corpus C. Muestre cómo el modelo le asigna una probabilidad a la oración `el perro le ladra al gato` usando los parámetros estimados a partir del corpus  $q(w_i | w_{i-2}, w_{i-1})$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VUjDx0NrYbg"
   },
   "source": [
    "**Respuesta:** La probabilidad de la oracion completa será $q(\"el\"|\"*\",\"*\")*q(\"perro\"|\"*\",\"el\")*q(\"le\"|\"el\",\"perro\")*q(\"ladra\"|\"perro\",\"le\")*q(\"al\"|\"le\",\"ladra\")*q(\"gato\"|\"ladra\",\"al\")$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwNkPIXure0C"
   },
   "source": [
    "### Pregunta 2 (1 pt)\n",
    "Muestre cómo se calcularía  $q(w_{i} | w_{i-2}, w_{i-1})$  usando un modelo que interpola un modelo de lenguajes de trigramas, bigramas, y unigrama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeLZAd0Tr9ne"
   },
   "source": [
    "**Respuesta:** En un modelo que interpole se tiene que: $q(w_{i} | w_{i-2}, w_{i-1})=\\lambda_1*q_{tri}(w_{i} | w_{i-2}, w_{i-1}) + \\lambda_2*q_{bi}(w_{i} | w_{i-1}) + \\lambda_3*q_{uni}(w_{i} )$ donde $\\lambda_1+\\lambda_2+\\lambda_3=1$ y $\\lambda_1,\\lambda_2,\\lambda_3 \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHqcRJ7Vr_8x"
   },
   "source": [
    "### Pregunta 3 (1 pt)\n",
    "¿Qué ventajas tiene el modelo interpolado sobre el modelo de trigramas simple?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F5R3Ji7sHjt"
   },
   "source": [
    "**Respuesta:** Una ventaja es que van a existir varios trigramas o bigramas que no se verán en el corpus de entrenamiento, por lo que al interpolar se evitan las indefiniciones. Además el modelo interpolado logra un mejor balance entre sesgo y varianza en comparacion al trigrama simple que es un modelo con mas variabilidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdmB-07ZKfaa"
   },
   "source": [
    "-----------------------\n",
    "## Naive Bayes (3 pts)\n",
    "En esta parte de la minitarea deberan programar Naive Bayes Multinomial usando Laplace Smothing. Las referencias las pueden encontrar en el material del curso y los videos del profesor.\n",
    "\n",
    "A continuacion se presentan un conjunto de documentos de training divididos en 2 categorias distintas. Ustedes deberan clasificar los documentos del test set usando Naive Bayes con Laplace Smothing.\n",
    "\n",
    "Por favor, documenten su codigo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algun lugar de su codigo hacen algo que creen que debe ser explicado, pongan comentarios, son bienvenidos. \n",
    "\n",
    "\n",
    "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplacenlas por sumas de logaritmos para prevenir errores de precision. Revisen la diapo 69 de las slides. \n",
    "\n",
    "NOTA: Sobre las `namedtuple`s. Es el tipo de los documentos. Son objetos inmutable que tienen dos atributos: `words` donde estan las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`. Son libres de implementar su solucion como quieran, si no les gusta este tipo de dato usen otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "HLi8PxdV2VQX",
    "outputId": "efd50e50-3317-454b-c74f-222458249c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train documents:\n",
      "(document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
      " document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
      " document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
      " document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
      " document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
      " document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
      " document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
      " document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1))\n",
      "\n",
      "Test documents:\n",
      "(document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "document = namedtuple(\n",
    "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
    ")\n",
    "\n",
    "train_set = (\n",
    "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
    "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
    "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
    "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
    "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
    "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
    "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
    "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
    ")\n",
    "print(\"Train documents:\")\n",
    "pprint(train_set)\n",
    "\n",
    "\n",
    "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),)\n",
    "print(\"\\nTest documents:\")\n",
    "pprint(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0edu0E7LA3U9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Tu respuesta\n",
    "def get_pc(train_set):\n",
    "    \"\"\"\n",
    "    Get p(c) for each class in the training set\n",
    "    input:\n",
    "        train_set: training corpus of document type\n",
    "    \"\"\"\n",
    "    countC = {}\n",
    "    # count number of documents for each class\n",
    "    for d in train_set:\n",
    "        c = d.class_\n",
    "        if c not in countC:\n",
    "            countC[c] = 1\n",
    "        else:\n",
    "            countC[c] +=1\n",
    "    Pc = {}\n",
    "    # Calculate prior for each class\n",
    "    for class_ in countC:\n",
    "        Pc[class_] = countC[class_]/len(train_set)\n",
    "    return Pc\n",
    "\n",
    "def get_vocab(train_set):\n",
    "    \"\"\"\n",
    "    Get the vocabulary in a training set corpus\n",
    "    input:\n",
    "        train_set: training corpus of document type\n",
    "    \"\"\"\n",
    "    vocab = []\n",
    "    for d in train_set:\n",
    "        for w in d.words:\n",
    "            if w not in vocab:\n",
    "                vocab.append(w)\n",
    "    return vocab\n",
    "\n",
    "def get_word_p(train_set, classes):\n",
    "    \"\"\"\n",
    "    Get P(w|c) for each word and class with Laplace smoothing\n",
    "    input:\n",
    "        train_set: training corpus of document type\n",
    "        classes: Classes in the training set as python iterable\n",
    "    \"\"\"\n",
    "    # get corpus vocabulary\n",
    "    vocab = get_vocab(train_set)\n",
    "    # Store P(w|c) as dictionary for each class\n",
    "    Pw = {}\n",
    "    for c in classes:\n",
    "        Pw[c] = {}\n",
    "        # Make single document from elements of the same class\n",
    "        doc = []\n",
    "        for d in train_set:\n",
    "            if d.class_ == c:\n",
    "                doc.extend(d.words)\n",
    "        # Calculate probability for each word\n",
    "        for word in vocab:\n",
    "            Pw[c][word] = (doc.count(word)+1)/(len(doc)+len(vocab))\n",
    "        # Probability for word outside of vocabulary\n",
    "        Pw[c][\"_?_\"] = 1/(len(doc)+len(vocab))\n",
    "    return Pw\n",
    "\n",
    "def multinomial_NB(doc ,Pc, Pw):\n",
    "    \"\"\"\n",
    "    Classify text using multinomial Naive Bayes\n",
    "    input:\n",
    "        doc: A document to classify\n",
    "        Pc: p(c) for each class from training set in dictionary form\n",
    "        Pw: P(w|c) for each word and class with Laplace smoothing in dictionary form\n",
    "    \"\"\"\n",
    "    Pd = {}\n",
    "    for class_ in Pc:\n",
    "        # Start with prior\n",
    "        log_prob = np.log2(Pc[class_])\n",
    "        for word in doc.words:\n",
    "            # Multiply by probability of each word, or the default if the word is new, given the class\n",
    "            log_prob += np.log2(Pw[class_].get(word, Pw[class_][\"_?_\"]))\n",
    "        Pd[class_] = log_prob\n",
    "    # Find most likely class\n",
    "    MLclass = max(Pd, key=Pd.get)\n",
    "    return MLclass\n",
    "\n",
    "def classify_set_NB(test_set, Pc, Pw):\n",
    "    \"\"\"\n",
    "    Classify text from a set with multinomial Naive Bayes\n",
    "    input:\n",
    "        test_set: corpus of document type\n",
    "        Pc: p(c) for each class from training set in dictionary form\n",
    "        Pw: P(w|c) for each word and class with Laplace smoothing in dictionary form\n",
    "    \"\"\"\n",
    "    result = [multinomial_NB(doc, Pc, Pw) for doc in test_set]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NB model\n",
    "Pc = get_pc(train_set)\n",
    "Pw = get_word_p(train_set, classes=Pc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Classify example\n",
    "doc_class = classify_set_NB(test_set, Pc, Pw)\n",
    "print(\"Predicted classes\")\n",
    "print(doc_class)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "minitarea2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
